<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- Generated by Apache Maven Doxia at Jan 27, 2011 -->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <title>HBase - RAT (Release Audit Tool) results</title>
    <style type="text/css" media="all">
      @import url("./css/maven-base.css");
      @import url("./css/maven-theme.css");
      @import url("./css/site.css");
    </style>
    <link rel="stylesheet" href="./css/print.css" type="text/css" media="print" />
    <meta http-equiv="Content-Language" content="en" />
          </head>
  <body class="composite">
    <div id="banner">
                  <a href="" id="bannerLeft">
                                                <img src="images/hbase_logo_med.gif" alt="HBase" />
                </a>
                        <a href="http://www.apache.org/" id="bannerRight">
                                                <img src="images/asf_logo_wide.png" alt="HBase" />
                </a>
            <div class="clear">
        <hr/>
      </div>
    </div>
    <div id="breadcrumbs">
            
                
                  <div class="xright" style="padding-left: 8px; margin-top: -4px;">
        <form method="GET" action="http://search-hadoop.com/">
          <input type="text" style="width: 192px; height: 15px; font-size: inherit; border: 1px solid darkgray" name="q" value="Search wiki, mailing lists, sources & more" onfocus="this.value=''"/>
          <input type="hidden" name="fc_project" value="HBase"/>
          <button style="height: 20px; width: 60px;">Search</button>
        </form>
      </div>
      <div class="xright">        
                
                 <span id="publishDate">Last Published: 2011-01-27</span>
              &nbsp;| <span id="projectVersion">Version: 0.89.20100924+30</span>
            </div>
      <div class="clear">
        <hr/>
      </div>
    </div>
    <div id="leftColumn">
      <div id="navcolumn">
             
                
                                <h5>HBase</h5>
                  <ul>
                  <li class="none">
                          <a href="index.html" title="Overview">Overview</a>
            </li>
                  <li class="none">
                          <a href="license.html" title="License">License</a>
            </li>
                  <li class="none">
                          <a href="http://www.apache.org/dyn/closer.cgi/hbase/" class="externalLink" title="Downloads">Downloads</a>
            </li>
                  <li class="none">
                          <a href="https://issues.apache.org/jira/browse/HBASE?report=com.atlassian.jira.plugin.system.project:changelog-panel" class="externalLink" title="Release Notes">Release Notes</a>
            </li>
                  <li class="none">
                          <a href="issue-tracking.html" title="Issue Tracking">Issue Tracking</a>
            </li>
                  <li class="none">
                          <a href="mail-lists.html" title="Mailing Lists">Mailing Lists</a>
            </li>
                  <li class="none">
                          <a href="source-repository.html" title="Source Repository">Source Repository</a>
            </li>
                  <li class="none">
                          <a href="faq.html" title="FAQ">FAQ</a>
            </li>
                  <li class="none">
                          <a href="http://wiki.apache.org/hadoop/Hbase" class="externalLink" title="Wiki">Wiki</a>
            </li>
                  <li class="none">
                          <a href="team-list.html" title="Team">Team</a>
            </li>
          </ul>
                       <h5>Documentation</h5>
                  <ul>
                  <li class="none">
                          <a href="apidocs/overview-summary.html#overview_description" title="Getting Started">Getting Started</a>
            </li>
                  <li class="none">
                          <a href="apidocs/index.html" title="API">API</a>
            </li>
                  <li class="none">
                          <a href="xref/index.html" title="X-Ref">X-Ref</a>
            </li>
                  <li class="none">
                          <a href="acid-semantics.html" title="ACID Semantics">ACID Semantics</a>
            </li>
                  <li class="none">
                          <a href="bulk-loads.html" title="Bulk Loads">Bulk Loads</a>
            </li>
                  <li class="none">
                          <a href="metrics.html" title="Metrics">Metrics</a>
            </li>
                  <li class="none">
                          <a href="cygwin.html" title="HBase on Windows">HBase on Windows</a>
            </li>
                  <li class="none">
                          <a href="replication.html" title="Cluster replication">Cluster replication</a>
            </li>
                  <li class="none">
                          <a href="pseudo-distributed.html" title="Pseudo-Distributed HBase">Pseudo-Distributed HBase</a>
            </li>
                  <li class="none">
                          <a href="book.html" title="HBase Book">HBase Book</a>
            </li>
                  <li class="none">
                          <a href="sample_article.html" title="Example Docbook Article">Example Docbook Article</a>
            </li>
          </ul>
                             <a href="http://maven.apache.org/" title="Built by Maven" class="poweredBy">
        <img class="poweredBy" alt="Built by Maven" src="./images/logos/maven-feather.png" />
      </a>
                   
                
            </div>
    </div>
    <div id="bodyColumn">
      <div id="contentBox">
        <div class="section"><h2>RAT (Release Audit Tool) results</h2>
<p>The following document contains the results of <a class="externalLink" href="http://incubator.apache.org/rat/">RAT (Release Audit Tool)</a>.</p>
<p><div class="source"><pre>
*****************************************************
Summary
-------
Notes: 5
Binaries: 8
Archives: 0
Standards: 754

Apache Licensed: 628
Generated Documents: 0

JavaDocs are generated and so license header is optional
Generated files do not required license headers

126 Unknown Licenses

*******************************

Unapproved licenses:

  bin/set_meta_block_caching.rb
  bin/local-regionservers.sh
  bin/local-master-backup.sh
  .git/config
  .git/hooks/applypatch-msg.sample
  .git/hooks/post-receive.sample
  .git/hooks/pre-commit.sample
  .git/hooks/pre-applypatch.sample
  .git/hooks/update.sample
  .git/hooks/post-update.sample
  .git/hooks/pre-rebase.sample
  .git/hooks/post-commit.sample
  .git/hooks/prepare-commit-msg.sample
  .git/hooks/commit-msg.sample
  .git/info/exclude
  .git/description
  .git/HEAD
  src/site/site.xml
  src/site/site.vm
  src/test/resources/log4j.properties
  src/test/resources/mapred-queues.xml
  src/test/java/org/apache/hadoop/hbase/filter/TestColumnPrefixFilter.java
  src/test/java/org/apache/hadoop/hbase/master/TestZKBasedCloseRegion.java
  src/test/java/org/apache/hadoop/hbase/master/TestZKBasedReopenRegion.java
  src/test/java/org/apache/hadoop/hbase/master/TestRestartCluster.java
  src/test/ruby/test_helper.rb
  src/main/resources/hbase-webapps/static/hbase.css
  src/main/resources/hbase-webapps/regionserver/index.html
  src/main/resources/hbase-webapps/regionserver/regionserver.jsp
  src/main/resources/hbase-webapps/master/table.jsp
  src/main/resources/hbase-webapps/master/zk.jsp
  src/main/resources/hbase-webapps/master/master.jsp
  src/main/resources/hbase-webapps/master/index.html
  src/main/resources/org/apache/hadoop/hbase/mapred/RowCounter_Counters.properties
  src/main/resources/org/apache/hadoop/hbase/mapreduce/RowCounter_Counters.properties
  src/main/resources/org/apache/hadoop/hbase/rest/XMLSchema.xsd
  src/main/java/org/apache/hadoop/hbase/avro/generated/ACompressionAlgorithm.java
  src/main/java/org/apache/hadoop/hbase/avro/generated/ATimeRange.java
  src/main/java/org/apache/hadoop/hbase/avro/generated/AIOError.java
  src/main/java/org/apache/hadoop/hbase/avro/generated/ARegionLoad.java
  src/main/java/org/apache/hadoop/hbase/avro/generated/AColumnFamilyDescriptor.java
  src/main/java/org/apache/hadoop/hbase/avro/generated/AClusterStatus.java
  src/main/java/org/apache/hadoop/hbase/avro/generated/ATableDescriptor.java
  src/main/java/org/apache/hadoop/hbase/avro/generated/AFamilyDescriptor.java
  src/main/java/org/apache/hadoop/hbase/avro/generated/ATableExists.java
  src/main/java/org/apache/hadoop/hbase/avro/generated/AResult.java
  src/main/java/org/apache/hadoop/hbase/avro/generated/AColumn.java
  src/main/java/org/apache/hadoop/hbase/avro/generated/APut.java
  src/main/java/org/apache/hadoop/hbase/avro/generated/AGet.java
  src/main/java/org/apache/hadoop/hbase/avro/generated/AServerAddress.java
  src/main/java/org/apache/hadoop/hbase/avro/generated/AServerLoad.java
  src/main/java/org/apache/hadoop/hbase/avro/generated/HBase.java
  src/main/java/org/apache/hadoop/hbase/avro/generated/IOError.java
  src/main/java/org/apache/hadoop/hbase/avro/generated/AServerInfo.java
  src/main/java/org/apache/hadoop/hbase/avro/generated/AColumnValue.java
  src/main/java/org/apache/hadoop/hbase/avro/generated/AIllegalArgument.java
  src/main/java/org/apache/hadoop/hbase/avro/generated/ADelete.java
  src/main/java/org/apache/hadoop/hbase/avro/generated/AMasterNotRunning.java
  src/main/java/org/apache/hadoop/hbase/avro/generated/TCell.java
  src/main/java/org/apache/hadoop/hbase/avro/generated/AAlreadyExists.java
  src/main/java/org/apache/hadoop/hbase/avro/generated/AScan.java
  src/main/java/org/apache/hadoop/hbase/avro/generated/AResultEntry.java
  src/main/java/org/apache/hadoop/hbase/avro/hbase.avpr
  src/main/java/org/apache/hadoop/hbase/filter/TimestampsFilter.java
  src/main/java/org/apache/hadoop/hbase/executor/HBaseExecutorService.java
  src/main/java/org/apache/hadoop/hbase/executor/HBaseEventHandler.java
  src/main/java/org/apache/hadoop/hbase/executor/RegionTransitionEventData.java
  src/main/java/org/apache/hadoop/hbase/regionserver/RSZookeeperUpdater.java
  src/main/java/org/apache/hadoop/hbase/regionserver/wal/LogEntryVisitor.java
  src/main/java/org/apache/hadoop/hbase/master/ZKUnassignedWatcher.java
  src/main/java/org/apache/hadoop/hbase/master/handler/MasterOpenRegionHandler.java
  src/main/java/org/apache/hadoop/hbase/master/handler/MasterCloseRegionHandler.java
  src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/ColumnSchemaMessage.java
  src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/CellSetMessage.java
  src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/VersionMessage.java
  src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/TableInfoMessage.java
  src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/ScannerMessage.java
  src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/TableListMessage.java
  src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/TableSchemaMessage.java
  src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/StorageClusterStatusMessage.java
  src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/CellMessage.java
  src/main/javadoc/org/apache/hadoop/hbase/thrift/doc-files/style.css
  src/main/javadoc/org/apache/hadoop/hbase/thrift/doc-files/Hbase.html
  src/main/javadoc/org/apache/hadoop/hbase/thrift/doc-files/index.html
  src/docbkx/sample_article.xml
  src/docbkx/book.xml
  src/assembly/bin.xml
  src/assembly/src.xml
  src/examples/mapreduce/index-builder-setup.rb
  cloudera/hbase.1
  cloudera/apply-patches
  cloudera/do-release-build
  cloudera/patches/0021-HBASE-3008-Memstore.updateColumnValue-passes-wrong-f.patch
  cloudera/patches/0016-CLOUDERA-BUILD.-HBase-running-on-secure-hadoop-tempo.patch
  cloudera/patches/0009-CLOUDERA-BUILD.-rsync-all-of-lib-into-target-directo.patch
  cloudera/patches/0005-Re-enable-log-split-test.patch
  cloudera/patches/0011-SequenceFileLogWriter-doesn-t-need-to-actually-call-.patch
  cloudera/patches/0014-HBASE-3000.-Add-hbase-classpath-command.patch
  cloudera/patches/0029-CLOUDERA-BUILD.-Publish-to-Cloudera-maven-repo.-Use-.patch
  cloudera/patches/0015-HBASE-3001.-TableMapReduceUtil-should-always-add-dep.patch
  cloudera/patches/0006-HBASE-2773.-Check-for-null-values-in-meta-in-test-ut.patch
  cloudera/patches/0010-HBASE-2467.-Concurrent-flushers-in-HLog-sync-using-H.patch
  cloudera/patches/0023-CLOUDERA-BUILD.-cloudera-directory-should-get-instal.patch
  cloudera/patches/0002-CLOUDERA-BUILD.-Add-build-infrastructure.patch
  cloudera/patches/0026-CLOUDERA-BUILD.-Update-hadoop-version.patch
  cloudera/patches/0007-CLOUDERA-BUILD.-Include-cloudera-dir-in-src-assembly.patch
  cloudera/patches/0017-HBASE-2782.-QoS-for-META-table-access.patch
  cloudera/patches/0019-CLOUDERA-BUILD.-Fix-copy-of-bin-to-be-cp-a.patch
  cloudera/patches/0028-CLOUDERA-BUILD.-Fix-versionless-jar-naming-symlinks-.patch
  cloudera/patches/0025-HBASE-3096.-TestCompaction-timing-out.patch
  cloudera/patches/0003-CLOUDERA-BUILD.-Switch-to-CDH3b3-snapshot-in-todd-s-.patch
  cloudera/patches/0008-CLOUDERA-BUILD.-hbase-config.sh-should-set-HBASE_PID.patch
  cloudera/patches/0018-CLOUDERA-BUILD.-Build-site-as-part-of-release-build.patch
  cloudera/patches/0001-Updating-the-site-for-0.89.20100924.patch
  cloudera/patches/0022-CLOUDERA-BUILD.-Change-wrapper-scripts-to-not-be-dep.patch
  cloudera/patches/0020-Fix-src-assembly-to-make-java-libs-644-and-not-inclu.patch
  cloudera/patches/0030-CLOUDERA-BUILD.-Add-man-page.patch
  cloudera/patches/0027-HBASE-3101.-bin-assembly-should-include-tests-and-so.patch
  cloudera/build.properties
  cloudera/CHANGES.cloudera.txt
  cloudera/README.cloudera
  cloudera/install_hbase.sh
  CHANGES.txt
  conf/regionservers
  conf/log4j.properties
  conf/hadoop-metrics.properties

*******************************

Archives:

*****************************************************
  Files with Apache License headers will be marked AL
  Binary files (which do not require AL headers) will be marked B
  Compressed archives will be marked A
  Notices, licenses etc will be marked N
  N     NOTICE.txt
  AL    bin/check_meta.rb
  AL    bin/rename_table.rb
  AL    bin/zookeepers.sh
  AL    bin/loadtable.rb
  AL    bin/replication/copy_tables_desc.rb
  AL    bin/replication/add_peer.rb
  AL    bin/add_table.rb
  AL    bin/hirb.rb
  AL    bin/rolling-restart.sh
  AL    bin/regionservers.sh
 !????? bin/set_meta_block_caching.rb
 !????? bin/local-regionservers.sh
  AL    bin/hbase-config.sh
  AL    bin/stop-hbase.sh
  AL    bin/master-backup.sh
  AL    bin/hbase
  AL    bin/copy_table.rb
  AL    bin/hbase-daemon.sh
  AL    bin/start-hbase.sh
  AL    bin/hbase-daemons.sh
 !????? bin/local-master-backup.sh
  AL    pom.xml
  N     LICENSE.txt
 !????? .git/config
 !????? .git/hooks/applypatch-msg.sample
 !????? .git/hooks/post-receive.sample
 !????? .git/hooks/pre-commit.sample
 !????? .git/hooks/pre-applypatch.sample
 !????? .git/hooks/update.sample
 !????? .git/hooks/post-update.sample
 !????? .git/hooks/pre-rebase.sample
 !????? .git/hooks/post-commit.sample
 !????? .git/hooks/prepare-commit-msg.sample
 !????? .git/hooks/commit-msg.sample
 !????? .git/info/exclude
 !????? .git/description
 !????? .git/HEAD
  B     src/site/resources/images/hadoop-logo.jpg
  B     src/site/resources/images/asf_logo_wide.png
  B     src/site/resources/images/favicon.ico
  B     src/site/resources/images/architecture.gif
  B     src/site/resources/images/replication_overview.png
  B     src/site/resources/images/hbase_logo_med.gif
  B     src/site/resources/images/hbase_small.gif
  AL    src/site/resources/css/site.css
  AL    src/site/fml/faq.fml
 !????? src/site/site.xml
  AL    src/site/xdoc/metrics.xml
  AL    src/site/xdoc/replication.xml
  AL    src/site/xdoc/old_news.xml
  AL    src/site/xdoc/pseudo-distributed.xml
  AL    src/site/xdoc/acid-semantics.xml
  AL    src/site/xdoc/cygwin.xml
  AL    src/site/xdoc/index.xml
  AL    src/site/xdoc/bulk-loads.xml
 !????? src/site/site.vm
 !????? src/test/resources/log4j.properties
  AL    src/test/resources/hbase-site.xml
  AL    src/test/resources/org/apache/hadoop/hbase/PerformanceEvaluation_Counter.properties
 !????? src/test/resources/mapred-queues.xml
  AL    src/test/java/org/apache/hadoop/hbase/EmptyWatcher.java
  AL    src/test/java/org/apache/hadoop/hbase/HBaseClusterTestCase.java
  AL    src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java
  AL    src/test/java/org/apache/hadoop/hbase/HBaseTestCase.java
  AL    src/test/java/org/apache/hadoop/hbase/TestInfoServers.java
  AL    src/test/java/org/apache/hadoop/hbase/TestFullLogReconstruction.java
  AL    src/test/java/org/apache/hadoop/hbase/mapred/TestTableMapReduce.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestBase64.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestIncrementingEnvironmentEdge.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestKeying.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestEnvironmentEdgeManager.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestMergeTool.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestRootPath.java
  AL    src/test/java/org/apache/hadoop/hbase/util/SoftValueSortedMapTest.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestDefaultEnvironmentEdge.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestByteBloomFilter.java
  AL    src/test/java/org/apache/hadoop/hbase/util/DisabledTestMetaUtils.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestBytes.java
  AL    src/test/java/org/apache/hadoop/hbase/util/EnvironmentEdgeManagerTestHelper.java
  AL    src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java
  AL    src/test/java/org/apache/hadoop/hbase/client/TestHCM.java
  AL    src/test/java/org/apache/hadoop/hbase/client/TestTimestampsFilter.java
  AL    src/test/java/org/apache/hadoop/hbase/client/TestTimestamp.java
  AL    src/test/java/org/apache/hadoop/hbase/client/TestHTablePool.java
  AL    src/test/java/org/apache/hadoop/hbase/client/TestMetaScanner.java
  AL    src/test/java/org/apache/hadoop/hbase/client/TestScannerTimeout.java
  AL    src/test/java/org/apache/hadoop/hbase/client/TestShell.java
  AL    src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java
  AL    src/test/java/org/apache/hadoop/hbase/client/TestMultipleTimestamps.java
  AL    src/test/java/org/apache/hadoop/hbase/client/TestGetRowVersions.java
  AL    src/test/java/org/apache/hadoop/hbase/thrift/TestThriftServer.java
  AL    src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
  AL    src/test/java/org/apache/hadoop/hbase/replication/ReplicationSourceDummy.java
  AL    src/test/java/org/apache/hadoop/hbase/replication/TestReplication.java
  AL    src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestReplicationSink.java
  AL    src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestReplicationSourceManager.java
  AL    src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSource.java
  AL    src/test/java/org/apache/hadoop/hbase/KeyValueTestUtil.java
  AL    src/test/java/org/apache/hadoop/hbase/avro/TestAvroServer.java
  AL    src/test/java/org/apache/hadoop/hbase/HFilePerformanceEvaluation.java
  AL    src/test/java/org/apache/hadoop/hbase/filter/TestFilter.java
  AL    src/test/java/org/apache/hadoop/hbase/filter/TestPageFilter.java
  AL    src/test/java/org/apache/hadoop/hbase/filter/TestFilterList.java
  AL    src/test/java/org/apache/hadoop/hbase/filter/TestSingleColumnValueFilter.java
 !????? src/test/java/org/apache/hadoop/hbase/filter/TestColumnPrefixFilter.java
  AL    src/test/java/org/apache/hadoop/hbase/filter/TestPrefixFilter.java
  AL    src/test/java/org/apache/hadoop/hbase/filter/TestColumnPaginationFilter.java
  AL    src/test/java/org/apache/hadoop/hbase/filter/TestSingleColumnValueExcludeFilter.java
  AL    src/test/java/org/apache/hadoop/hbase/filter/TestDependentColumnFilter.java
  AL    src/test/java/org/apache/hadoop/hbase/filter/TestInclusiveStopFilter.java
  AL    src/test/java/org/apache/hadoop/hbase/TestMergeTable.java
  AL    src/test/java/org/apache/hadoop/hbase/TestZooKeeper.java
  AL    src/test/java/org/apache/hadoop/hbase/PerformanceEvaluationCommons.java
  AL    src/test/java/org/apache/hadoop/hbase/zookeeper/TestHQuorumPeer.java
  AL    src/test/java/org/apache/hadoop/hbase/TestMultiParallelPut.java
  AL    src/test/java/org/apache/hadoop/hbase/TestHMsg.java
  AL    src/test/java/org/apache/hadoop/hbase/TestSerialization.java
  AL    src/test/java/org/apache/hadoop/hbase/io/TestHeapSize.java
  AL    src/test/java/org/apache/hadoop/hbase/io/TestHbaseObjectWritable.java
  AL    src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileSeek.java
  AL    src/test/java/org/apache/hadoop/hbase/io/hfile/TestCachedBlockQueue.java
  AL    src/test/java/org/apache/hadoop/hbase/io/hfile/RandomSeek.java
  AL    src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFilePerformance.java
  AL    src/test/java/org/apache/hadoop/hbase/io/hfile/NanoTimer.java
  AL    src/test/java/org/apache/hadoop/hbase/io/hfile/TestSeekTo.java
  AL    src/test/java/org/apache/hadoop/hbase/io/hfile/TestReseekTo.java
  AL    src/test/java/org/apache/hadoop/hbase/io/hfile/KVGenerator.java
  AL    src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFile.java
  AL    src/test/java/org/apache/hadoop/hbase/io/hfile/TestLruBlockCache.java
  AL    src/test/java/org/apache/hadoop/hbase/io/hfile/RandomDistribution.java
  AL    src/test/java/org/apache/hadoop/hbase/io/hfile/KeySampler.java
  AL    src/test/java/org/apache/hadoop/hbase/io/TestImmutableBytesWritable.java
  AL    src/test/java/org/apache/hadoop/hbase/MiniHBaseCluster.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestMemStore.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestWideScanner.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestCompaction.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestGetClosestAtOrBefore.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestQueryMatcher.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestScanner.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreFile.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionInfo.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestMinorCompactingStoreScanner.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/OOMERegionServer.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALReplay.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogActionsListener.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRolling.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLog.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/wal/InstrumentedSequenceFileLogWriter.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogMethods.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestKeyValueHeap.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestScanWildcardColumnTracker.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestKeyValueScanFixture.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestExplicitColumnTracker.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/DisabledTestRegionServerExit.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransaction.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/KeyValueScanFixture.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestScanDeleteTracker.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestReadWriteConsistencyControl.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestStore.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestFSErrorsExposed.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestKeyValueSkipListSet.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreScanner.java
  AL    src/test/java/org/apache/hadoop/hbase/mapreduce/TestTableInputFormatScan.java
  AL    src/test/java/org/apache/hadoop/hbase/mapreduce/TestTableMapReduce.java
  AL    src/test/java/org/apache/hadoop/hbase/mapreduce/TestSimpleTotalOrderPartitioner.java
  AL    src/test/java/org/apache/hadoop/hbase/mapreduce/TestLoadIncrementalHFiles.java
  AL    src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTsv.java
  AL    src/test/java/org/apache/hadoop/hbase/mapreduce/NMapInputFormat.java
  AL    src/test/java/org/apache/hadoop/hbase/mapreduce/TestHFileOutputFormat.java
  AL    src/test/java/org/apache/hadoop/hbase/mapreduce/TestTimeRangeMapRed.java
  AL    src/test/java/org/apache/hadoop/hbase/master/TestRegionManager.java
  AL    src/test/java/org/apache/hadoop/hbase/master/TestMinimumServerCount.java
  AL    src/test/java/org/apache/hadoop/hbase/master/TestServerManager.java
  AL    src/test/java/org/apache/hadoop/hbase/master/TestKillingServersFromMaster.java
 !????? src/test/java/org/apache/hadoop/hbase/master/TestZKBasedCloseRegion.java
  AL    src/test/java/org/apache/hadoop/hbase/master/TestMasterTransitions.java
  AL    src/test/java/org/apache/hadoop/hbase/master/OOMEHMaster.java
  AL    src/test/java/org/apache/hadoop/hbase/master/TestMaster.java
  AL    src/test/java/org/apache/hadoop/hbase/master/TestLogsCleaner.java
  AL    src/test/java/org/apache/hadoop/hbase/master/TestMasterWithDisabling.java
  AL    src/test/java/org/apache/hadoop/hbase/master/TestROOTAssignment.java
 !????? src/test/java/org/apache/hadoop/hbase/master/TestZKBasedReopenRegion.java
 !????? src/test/java/org/apache/hadoop/hbase/master/TestRestartCluster.java
  AL    src/test/java/org/apache/hadoop/hbase/master/TestRegionServerOperationQueue.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/PerformanceEvaluation.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/TestSchemaResource.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/TestStatusResource.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/TestScannersWithFilters.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/client/TestRemoteAdmin.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/client/TestRemoteTable.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/TestTableResource.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/TestRowResource.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/TestVersionResource.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/HBaseRESTClusterTestBase.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/model/TestColumnSchemaModel.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/model/TestStorageClusterVersionModel.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/model/TestStorageClusterStatusModel.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/model/TestTableSchemaModel.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/model/TestTableInfoModel.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/model/TestScannerModel.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/model/TestRowModel.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/model/TestCellModel.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/model/TestVersionModel.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/model/TestTableListModel.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/model/TestCellSetModel.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/model/TestTableRegionModel.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/TestScannerResource.java
  AL    src/test/java/org/apache/hadoop/hbase/TestMultiClusters.java
  AL    src/test/java/org/apache/hadoop/hbase/MultithreadedTestUtil.java
  AL    src/test/java/org/apache/hadoop/hbase/TestAcidGuarantees.java
  AL    src/test/java/org/apache/hadoop/hbase/TestKeyValue.java
  AL    src/test/java/org/apache/hadoop/hbase/MapFilePerformanceEvaluation.java
  AL    src/test/java/org/apache/hadoop/hbase/TimestampTestBase.java
  AL    src/test/java/org/apache/hadoop/hbase/TestCompare.java
  AL    src/test/java/org/apache/hadoop/hbase/TestEmptyMetaInfo.java
  AL    src/test/java/org/apache/hadoop/hbase/MultiRegionTable.java
  AL    src/test/java/org/apache/hadoop/hbase/AbstractMergeTestBase.java
  AL    src/test/java/org/apache/hadoop/hbase/TestMergeMeta.java
  AL    src/test/java/org/apache/hadoop/hbase/TestScanMultipleVersions.java
  AL    src/test/java/org/apache/hadoop/hbase/VerifiableEditor.java
  AL    src/test/java/org/apache/hadoop/hbase/metrics/TestMetricsMBeanBase.java
  AL    src/test/java/org/apache/hadoop/hbase/TestRegionRebalancing.java
  AL    src/test/ruby/shell/formatter_test.rb
  AL    src/test/ruby/shell/commands_test.rb
  AL    src/test/ruby/shell/shell_test.rb
 !????? src/test/ruby/test_helper.rb
  AL    src/test/ruby/tests_runner.rb
  AL    src/test/ruby/hbase/table_test.rb
  AL    src/test/ruby/hbase/hbase_test.rb
  AL    src/test/ruby/hbase/admin_test.rb
 !????? src/main/resources/hbase-webapps/static/hbase.css
  B     src/main/resources/hbase-webapps/static/hbase_logo_med.gif
 !????? src/main/resources/hbase-webapps/regionserver/index.html
 !????? src/main/resources/hbase-webapps/regionserver/regionserver.jsp
 !????? src/main/resources/hbase-webapps/master/table.jsp
 !????? src/main/resources/hbase-webapps/master/zk.jsp
 !????? src/main/resources/hbase-webapps/master/master.jsp
 !????? src/main/resources/hbase-webapps/master/index.html
  AL    src/main/resources/hbase-default.xml
 !????? src/main/resources/org/apache/hadoop/hbase/mapred/RowCounter_Counters.properties
  AL    src/main/resources/org/apache/hadoop/hbase/thrift/Hbase.thrift
 !????? src/main/resources/org/apache/hadoop/hbase/mapreduce/RowCounter_Counters.properties
  AL    src/main/resources/org/apache/hadoop/hbase/rest/protobuf/CellSetMessage.proto
  AL    src/main/resources/org/apache/hadoop/hbase/rest/protobuf/TableSchemaMessage.proto
  AL    src/main/resources/org/apache/hadoop/hbase/rest/protobuf/VersionMessage.proto
  AL    src/main/resources/org/apache/hadoop/hbase/rest/protobuf/TableListMessage.proto
  AL    src/main/resources/org/apache/hadoop/hbase/rest/protobuf/ColumnSchemaMessage.proto
  AL    src/main/resources/org/apache/hadoop/hbase/rest/protobuf/StorageClusterStatusMessage.proto
  AL    src/main/resources/org/apache/hadoop/hbase/rest/protobuf/TableInfoMessage.proto
  AL    src/main/resources/org/apache/hadoop/hbase/rest/protobuf/CellMessage.proto
  AL    src/main/resources/org/apache/hadoop/hbase/rest/protobuf/ScannerMessage.proto
 !????? src/main/resources/org/apache/hadoop/hbase/rest/XMLSchema.xsd
  AL    src/main/java/org/apache/hadoop/hbase/ValueOverMaxLengthException.java
  AL    src/main/java/org/apache/hadoop/hbase/LeaseException.java
  AL    src/main/java/org/apache/hadoop/hbase/RemoteExceptionHandler.java
  AL    src/main/java/org/apache/hadoop/hbase/ClusterStatus.java
  AL    src/main/java/org/apache/hadoop/hbase/HServerInfo.java
  AL    src/main/java/org/apache/hadoop/hbase/mapred/IdentityTableMap.java
  AL    src/main/java/org/apache/hadoop/hbase/mapred/TableOutputFormat.java
  AL    src/main/java/org/apache/hadoop/hbase/mapred/TableInputFormatBase.java
  AL    src/main/java/org/apache/hadoop/hbase/mapred/TableRecordReaderImpl.java
  AL    src/main/java/org/apache/hadoop/hbase/mapred/TableSplit.java
  AL    src/main/java/org/apache/hadoop/hbase/mapred/RowCounter.java
  AL    src/main/java/org/apache/hadoop/hbase/mapred/TableInputFormat.java
  AL    src/main/java/org/apache/hadoop/hbase/mapred/TableMap.java
  AL    src/main/java/org/apache/hadoop/hbase/mapred/HRegionPartitioner.java
  AL    src/main/java/org/apache/hadoop/hbase/mapred/Driver.java
  AL    src/main/java/org/apache/hadoop/hbase/mapred/TableMapReduceUtil.java
  AL    src/main/java/org/apache/hadoop/hbase/mapred/package-info.java
  AL    src/main/java/org/apache/hadoop/hbase/mapred/TableRecordReader.java
  AL    src/main/java/org/apache/hadoop/hbase/mapred/TableReduce.java
  AL    src/main/java/org/apache/hadoop/hbase/mapred/GroupingTableMap.java
  AL    src/main/java/org/apache/hadoop/hbase/mapred/IdentityTableReduce.java
  AL    src/main/java/org/apache/hadoop/hbase/util/InfoServer.java
  AL    src/main/java/org/apache/hadoop/hbase/util/PairOfSameType.java
  AL    src/main/java/org/apache/hadoop/hbase/util/Keying.java
  AL    src/main/java/org/apache/hadoop/hbase/util/Writables.java
  AL    src/main/java/org/apache/hadoop/hbase/util/FileSystemVersionException.java
  AL    src/main/java/org/apache/hadoop/hbase/util/Threads.java
  AL    src/main/java/org/apache/hadoop/hbase/util/MetaUtils.java
  AL    src/main/java/org/apache/hadoop/hbase/util/MD5Hash.java
  AL    src/main/java/org/apache/hadoop/hbase/util/ByteBloomFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/util/FSUtils.java
  AL    src/main/java/org/apache/hadoop/hbase/util/ClassSize.java
  AL    src/main/java/org/apache/hadoop/hbase/util/Bytes.java
  AL    src/main/java/org/apache/hadoop/hbase/util/VersionInfo.java
  AL    src/main/java/org/apache/hadoop/hbase/util/EnvironmentEdge.java
  AL    src/main/java/org/apache/hadoop/hbase/util/ManualEnvironmentEdge.java
  AL    src/main/java/org/apache/hadoop/hbase/util/DynamicByteBloomFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/util/Pair.java
  AL    src/main/java/org/apache/hadoop/hbase/util/JvmVersion.java
  AL    src/main/java/org/apache/hadoop/hbase/util/MurmurHash.java
  AL    src/main/java/org/apache/hadoop/hbase/util/BloomFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/util/JenkinsHash.java
  AL    src/main/java/org/apache/hadoop/hbase/util/DefaultEnvironmentEdge.java
  AL    src/main/java/org/apache/hadoop/hbase/util/Merge.java
  AL    src/main/java/org/apache/hadoop/hbase/util/Hash.java
  AL    src/main/java/org/apache/hadoop/hbase/util/SoftValueSortedMap.java
  AL    src/main/java/org/apache/hadoop/hbase/util/ServerCommandLine.java
  AL    src/main/java/org/apache/hadoop/hbase/util/IncrementingEnvironmentEdge.java
  AL    src/main/java/org/apache/hadoop/hbase/util/Strings.java
  AL    src/main/java/org/apache/hadoop/hbase/util/CompressionTest.java
  AL    src/main/java/org/apache/hadoop/hbase/util/Sleeper.java
  AL    src/main/java/org/apache/hadoop/hbase/util/JVMClusterUtil.java
  AL    src/main/java/org/apache/hadoop/hbase/util/Base64.java
  AL    src/main/java/org/apache/hadoop/hbase/util/EnvironmentEdgeManager.java
  AL    src/main/java/org/apache/hadoop/hbase/client/HTablePool.java
  AL    src/main/java/org/apache/hadoop/hbase/client/HTableInterfaceFactory.java
  AL    src/main/java/org/apache/hadoop/hbase/client/UnmodifyableHRegionInfo.java
  AL    src/main/java/org/apache/hadoop/hbase/client/Result.java
  AL    src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
  AL    src/main/java/org/apache/hadoop/hbase/client/Put.java
  AL    src/main/java/org/apache/hadoop/hbase/client/UnmodifyableHTableDescriptor.java
  AL    src/main/java/org/apache/hadoop/hbase/client/Delete.java
  AL    src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java
  AL    src/main/java/org/apache/hadoop/hbase/client/ServerCallable.java
  AL    src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java
  AL    src/main/java/org/apache/hadoop/hbase/client/HTableInterface.java
  AL    src/main/java/org/apache/hadoop/hbase/client/RowLock.java
  AL    src/main/java/org/apache/hadoop/hbase/client/HTableFactory.java
  AL    src/main/java/org/apache/hadoop/hbase/client/Scan.java
  AL    src/main/java/org/apache/hadoop/hbase/client/HBaseFsck.java
  AL    src/main/java/org/apache/hadoop/hbase/client/HConnection.java
  AL    src/main/java/org/apache/hadoop/hbase/client/ResultScanner.java
  AL    src/main/java/org/apache/hadoop/hbase/client/RetriesExhaustedException.java
  AL    src/main/java/org/apache/hadoop/hbase/client/ScannerCallable.java
  AL    src/main/java/org/apache/hadoop/hbase/client/UnmodifyableHColumnDescriptor.java
  AL    src/main/java/org/apache/hadoop/hbase/client/Row.java
  AL    src/main/java/org/apache/hadoop/hbase/client/RegionOfflineException.java
  AL    src/main/java/org/apache/hadoop/hbase/client/Get.java
  AL    src/main/java/org/apache/hadoop/hbase/client/package-info.java
  AL    src/main/java/org/apache/hadoop/hbase/client/NoServerForRegionException.java
  AL    src/main/java/org/apache/hadoop/hbase/client/MultiPut.java
  AL    src/main/java/org/apache/hadoop/hbase/client/ServerConnectionManager.java
  AL    src/main/java/org/apache/hadoop/hbase/client/ServerConnection.java
  AL    src/main/java/org/apache/hadoop/hbase/client/HTable.java
  AL    src/main/java/org/apache/hadoop/hbase/client/ScannerTimeoutException.java
  AL    src/main/java/org/apache/hadoop/hbase/client/MultiPutResponse.java
  AL    src/main/java/org/apache/hadoop/hbase/TableNotFoundException.java
  AL    src/main/java/org/apache/hadoop/hbase/thrift/ThriftUtilities.java
  AL    src/main/java/org/apache/hadoop/hbase/thrift/ThriftServer.java
  AL    src/main/java/org/apache/hadoop/hbase/thrift/generated/TRowResult.java
  AL    src/main/java/org/apache/hadoop/hbase/thrift/generated/AlreadyExists.java
  AL    src/main/java/org/apache/hadoop/hbase/thrift/generated/IllegalArgument.java
  AL    src/main/java/org/apache/hadoop/hbase/thrift/generated/BatchMutation.java
  AL    src/main/java/org/apache/hadoop/hbase/thrift/generated/TRegionInfo.java
  AL    src/main/java/org/apache/hadoop/hbase/thrift/generated/IOError.java
  AL    src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java
  AL    src/main/java/org/apache/hadoop/hbase/thrift/generated/ColumnDescriptor.java
  AL    src/main/java/org/apache/hadoop/hbase/thrift/generated/TCell.java
  AL    src/main/java/org/apache/hadoop/hbase/thrift/generated/Mutation.java
  AL    src/main/java/org/apache/hadoop/hbase/PleaseHoldException.java
  AL    src/main/java/org/apache/hadoop/hbase/HMerge.java
  AL    src/main/java/org/apache/hadoop/hbase/UnknownScannerException.java
  AL    src/main/java/org/apache/hadoop/hbase/HServerLoad.java
  AL    src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeperWrapper.java
  AL    src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSinkMetrics.java
  AL    src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java
  AL    src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationStatistics.java
  AL    src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java
  AL    src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java
  AL    src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.java
  AL    src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceMetrics.java
  AL    src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceInterface.java
  AL    src/main/java/org/apache/hadoop/hbase/replication/master/ReplicationLogCleaner.java
  AL    src/main/java/org/apache/hadoop/hbase/WritableComparator.java
  AL    src/main/java/org/apache/hadoop/hbase/HBaseConfTool.java
  AL    src/main/java/org/apache/hadoop/hbase/DroppedSnapshotException.java
  AL    src/main/java/org/apache/hadoop/hbase/NotServingRegionException.java
  AL    src/main/java/org/apache/hadoop/hbase/KeyValue.java
  AL    src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java
 !????? src/main/java/org/apache/hadoop/hbase/avro/generated/ACompressionAlgorithm.java
 !????? src/main/java/org/apache/hadoop/hbase/avro/generated/ATimeRange.java
 !????? src/main/java/org/apache/hadoop/hbase/avro/generated/AIOError.java
 !????? src/main/java/org/apache/hadoop/hbase/avro/generated/ARegionLoad.java
 !????? src/main/java/org/apache/hadoop/hbase/avro/generated/AColumnFamilyDescriptor.java
 !????? src/main/java/org/apache/hadoop/hbase/avro/generated/AClusterStatus.java
 !????? src/main/java/org/apache/hadoop/hbase/avro/generated/ATableDescriptor.java
 !????? src/main/java/org/apache/hadoop/hbase/avro/generated/AFamilyDescriptor.java
 !????? src/main/java/org/apache/hadoop/hbase/avro/generated/ATableExists.java
 !????? src/main/java/org/apache/hadoop/hbase/avro/generated/AResult.java
 !????? src/main/java/org/apache/hadoop/hbase/avro/generated/AColumn.java
 !????? src/main/java/org/apache/hadoop/hbase/avro/generated/APut.java
 !????? src/main/java/org/apache/hadoop/hbase/avro/generated/AGet.java
 !????? src/main/java/org/apache/hadoop/hbase/avro/generated/AServerAddress.java
 !????? src/main/java/org/apache/hadoop/hbase/avro/generated/AServerLoad.java
 !????? src/main/java/org/apache/hadoop/hbase/avro/generated/HBase.java
 !????? src/main/java/org/apache/hadoop/hbase/avro/generated/IOError.java
 !????? src/main/java/org/apache/hadoop/hbase/avro/generated/AServerInfo.java
 !????? src/main/java/org/apache/hadoop/hbase/avro/generated/AColumnValue.java
 !????? src/main/java/org/apache/hadoop/hbase/avro/generated/AIllegalArgument.java
 !????? src/main/java/org/apache/hadoop/hbase/avro/generated/ADelete.java
 !????? src/main/java/org/apache/hadoop/hbase/avro/generated/AMasterNotRunning.java
 !????? src/main/java/org/apache/hadoop/hbase/avro/generated/TCell.java
 !????? src/main/java/org/apache/hadoop/hbase/avro/generated/AAlreadyExists.java
 !????? src/main/java/org/apache/hadoop/hbase/avro/generated/AScan.java
 !????? src/main/java/org/apache/hadoop/hbase/avro/generated/AResultEntry.java
  AL    src/main/java/org/apache/hadoop/hbase/avro/AvroUtil.java
  AL    src/main/java/org/apache/hadoop/hbase/avro/AvroServer.java
 !????? src/main/java/org/apache/hadoop/hbase/avro/hbase.avpr
  AL    src/main/java/org/apache/hadoop/hbase/avro/hbase.genavro
  AL    src/main/java/org/apache/hadoop/hbase/avro/package.html
  AL    src/main/java/org/apache/hadoop/hbase/HMsg.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/RowFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/SkipFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/CompareFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/ColumnPrefixFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/RegexStringComparator.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/WhileMatchFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/PrefixFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/SingleColumnValueFilter.java
 !????? src/main/java/org/apache/hadoop/hbase/filter/TimestampsFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/InvalidRowFilterException.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/FirstKeyOnlyFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/ColumnPaginationFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/IncompatibleFilterException.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/ValueFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/QualifierFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/SingleColumnValueExcludeFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/WritableByteArrayComparable.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/BinaryComparator.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/FilterList.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/BinaryPrefixComparator.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/InclusiveStopFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/DependentColumnFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/FilterBase.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/ColumnCountGetFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/PageFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/package-info.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/Filter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/SubstringComparator.java
  AL    src/main/java/org/apache/hadoop/hbase/ColumnNameParseException.java
  AL    src/main/java/org/apache/hadoop/hbase/HRegionLocation.java
  AL    src/main/java/org/apache/hadoop/hbase/zookeeper/HQuorumPeer.java
  AL    src/main/java/org/apache/hadoop/hbase/zookeeper/ZKServerTool.java
  AL    src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWrapper.java
 !????? src/main/java/org/apache/hadoop/hbase/executor/HBaseExecutorService.java
 !????? src/main/java/org/apache/hadoop/hbase/executor/HBaseEventHandler.java
  AL    src/main/java/org/apache/hadoop/hbase/executor/NamedThreadFactory.java
 !????? src/main/java/org/apache/hadoop/hbase/executor/RegionTransitionEventData.java
  AL    src/main/java/org/apache/hadoop/hbase/TableExistsException.java
  AL    src/main/java/org/apache/hadoop/hbase/Chore.java
  AL    src/main/java/org/apache/hadoop/hbase/UnknownRowLockException.java
  AL    src/main/java/org/apache/hadoop/hbase/io/HbaseMapWritable.java
  AL    src/main/java/org/apache/hadoop/hbase/io/HeapSize.java
  AL    src/main/java/org/apache/hadoop/hbase/io/HbaseObjectWritable.java
  AL    src/main/java/org/apache/hadoop/hbase/io/TimeRange.java
  AL    src/main/java/org/apache/hadoop/hbase/io/Reference.java
  AL    src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java
  AL    src/main/java/org/apache/hadoop/hbase/io/CodeToClassAndBack.java
  AL    src/main/java/org/apache/hadoop/hbase/io/ImmutableBytesWritable.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/BoundedRangeFileInputStream.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/BlockCache.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/HFileScanner.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/CachedBlockQueue.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/SimpleBlockCache.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/CachedBlock.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/LruBlockCache.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/ScanWildcardColumnTracker.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/GetClosestRowBeforeTracker.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueHeap.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/OnlineRegions.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServerCommandLine.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/ReadWriteConsistencyControl.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/DebugPrint.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/Store.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/LruHashMap.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/DeleteTracker.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/StoreFlusher.java
 !????? src/main/java/org/apache/hadoop/hbase/regionserver/RSZookeeperUpdater.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/ColumnTracker.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/ShutdownHook.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/LogRoller.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/WrongRegionException.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/Stoppable.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueSkipListSet.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/NoSuchColumnFamilyException.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogKey.java
 !????? src/main/java/org/apache/hadoop/hbase/regionserver/wal/LogEntryVisitor.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogReader.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/wal/FailedLogCloseException.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/wal/LogRollListener.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/wal/LogActionsListener.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueScanner.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/ScanDeleteTracker.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/ChangedReadersObserver.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/MinorCompactingStoreScanner.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/TimeRangeTracker.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/InternalScanner.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/FlushRequester.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/ColumnCount.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/RegionServerRunningException.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/metrics/RegionServerMetrics.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/metrics/RegionServerStatistics.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java
  AL    src/main/java/org/apache/hadoop/hbase/LocalHBaseCluster.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/SimpleTotalOrderPartitioner.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/IdentityTableReducer.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/IdentityTableMapper.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/TableOutputFormat.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/Export.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/TableRecordReaderImpl.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/TableSplit.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/hadoopbackport/TotalOrderPartitioner.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/hadoopbackport/InputSampler.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/RowCounter.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormat.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/HRegionPartitioner.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/TableReducer.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/LoadIncrementalHFiles.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapper.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/PutSortReducer.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/Driver.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableOutputFormat.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/package-info.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/TableOutputCommitter.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/TableRecordReader.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/KeyValueSortReducer.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/GroupingTableMapper.java
  AL    src/main/java/org/apache/hadoop/hbase/master/HMasterCommandLine.java
  AL    src/main/java/org/apache/hadoop/hbase/master/ModifyTableMeta.java
  AL    src/main/java/org/apache/hadoop/hbase/master/RegionManager.java
  AL    src/main/java/org/apache/hadoop/hbase/master/RegionServerOperationQueue.java
  AL    src/main/java/org/apache/hadoop/hbase/master/HMaster.java
  AL    src/main/java/org/apache/hadoop/hbase/master/RetryableMetaOperation.java
  AL    src/main/java/org/apache/hadoop/hbase/master/NotAllMetaRegionsOnlineException.java
  AL    src/main/java/org/apache/hadoop/hbase/master/ProcessServerShutdown.java
 !????? src/main/java/org/apache/hadoop/hbase/master/ZKUnassignedWatcher.java
  AL    src/main/java/org/apache/hadoop/hbase/master/MetaRegion.java
  AL    src/main/java/org/apache/hadoop/hbase/master/ProcessRegionClose.java
  AL    src/main/java/org/apache/hadoop/hbase/master/DeleteColumn.java
  AL    src/main/java/org/apache/hadoop/hbase/master/ProcessRegionOpen.java
  AL    src/main/java/org/apache/hadoop/hbase/master/MetaScanner.java
  AL    src/main/java/org/apache/hadoop/hbase/master/LogCleanerDelegate.java
  AL    src/main/java/org/apache/hadoop/hbase/master/TableOperation.java
  AL    src/main/java/org/apache/hadoop/hbase/master/TimeToLiveLogCleaner.java
  AL    src/main/java/org/apache/hadoop/hbase/master/LogsCleaner.java
  AL    src/main/java/org/apache/hadoop/hbase/master/TableDelete.java
  AL    src/main/java/org/apache/hadoop/hbase/master/AddColumn.java
  AL    src/main/java/org/apache/hadoop/hbase/master/ZKMasterAddressWatcher.java
  AL    src/main/java/org/apache/hadoop/hbase/master/InvalidColumnNameException.java
  AL    src/main/java/org/apache/hadoop/hbase/master/ModifyColumn.java
  AL    src/main/java/org/apache/hadoop/hbase/master/ChangeTableState.java
  AL    src/main/java/org/apache/hadoop/hbase/master/RegionServerOperationListener.java
  AL    src/main/java/org/apache/hadoop/hbase/master/ServerManager.java
  AL    src/main/java/org/apache/hadoop/hbase/master/RootScanner.java
 !????? src/main/java/org/apache/hadoop/hbase/master/handler/MasterOpenRegionHandler.java
 !????? src/main/java/org/apache/hadoop/hbase/master/handler/MasterCloseRegionHandler.java
  AL    src/main/java/org/apache/hadoop/hbase/master/ColumnOperation.java
  AL    src/main/java/org/apache/hadoop/hbase/master/ProcessRegionStatusChange.java
  AL    src/main/java/org/apache/hadoop/hbase/master/RegionServerOperation.java
  AL    src/main/java/org/apache/hadoop/hbase/master/metrics/MasterMetrics.java
  AL    src/main/java/org/apache/hadoop/hbase/master/metrics/MasterStatistics.java
  AL    src/main/java/org/apache/hadoop/hbase/master/BaseScanner.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/SchemaResource.java
 !????? src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/ColumnSchemaMessage.java
 !????? src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/CellSetMessage.java
 !????? src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/VersionMessage.java
 !????? src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/TableInfoMessage.java
 !????? src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/ScannerMessage.java
 !????? src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/TableListMessage.java
 !????? src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/TableSchemaMessage.java
 !????? src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/StorageClusterStatusMessage.java
 !????? src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/CellMessage.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/provider/consumer/ProtobufMessageBodyConsumer.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/provider/JAXBContextResolver.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/provider/producer/PlainTextMessageBodyProducer.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/provider/producer/ProtobufMessageBodyProducer.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/client/RemoteAdmin.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/client/Client.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/client/Response.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/client/Cluster.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/client/RemoteHTable.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/Main.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/RowResultGenerator.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/VersionResource.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/ScannerResultGenerator.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/StorageClusterStatusResource.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/RowSpec.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/Constants.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/ScannerResource.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/TableResource.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/ResourceBase.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/RESTServlet.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/RootResource.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/ResourceConfig.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/model/TableSchemaModel.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/model/CellModel.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/model/TableInfoModel.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/model/TableModel.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/model/ScannerModel.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/model/VersionModel.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/model/StorageClusterStatusModel.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/model/StorageClusterVersionModel.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/model/CellSetModel.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/model/TableListModel.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/model/ColumnSchemaModel.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/model/TableRegionModel.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/model/RowModel.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/RegionsResource.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/StorageClusterVersionResource.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/ResultGenerator.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/ExistsResource.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/ProtobufMessageHandler.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/ScannerInstanceResource.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/metrics/RESTMetrics.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/metrics/RESTStatistics.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/RowResource.java
  AL    src/main/java/org/apache/hadoop/hbase/MiniZooKeeperCluster.java
  AL    src/main/java/org/apache/hadoop/hbase/ipc/HBaseRPCErrorHandler.java
  AL    src/main/java/org/apache/hadoop/hbase/ipc/HBaseRPCProtocolVersion.java
  AL    src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java
  AL    src/main/java/org/apache/hadoop/hbase/ipc/HRegionInterface.java
  AL    src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java
  AL    src/main/java/org/apache/hadoop/hbase/ipc/HBaseRPCStatistics.java
  AL    src/main/java/org/apache/hadoop/hbase/ipc/HMasterRegionInterface.java
  AL    src/main/java/org/apache/hadoop/hbase/ipc/HMasterInterface.java
  AL    src/main/java/org/apache/hadoop/hbase/ipc/HBaseRPC.java
  AL    src/main/java/org/apache/hadoop/hbase/ipc/HBaseRpcMetrics.java
  AL    src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java
  AL    src/main/java/org/apache/hadoop/hbase/MasterNotRunningException.java
  AL    src/main/java/org/apache/hadoop/hbase/Leases.java
  AL    src/main/java/org/apache/hadoop/hbase/HBaseConfiguration.java
  AL    src/main/java/org/apache/hadoop/hbase/HConstants.java
  AL    src/main/java/org/apache/hadoop/hbase/HRegionInfo.java
  AL    src/main/java/org/apache/hadoop/hbase/YouAreDeadException.java
  AL    src/main/java/org/apache/hadoop/hbase/RegionException.java
  AL    src/main/java/org/apache/hadoop/hbase/VersionAnnotation.java
  AL    src/main/java/org/apache/hadoop/hbase/LeaseListener.java
  AL    src/main/java/org/apache/hadoop/hbase/metrics/file/TimeStampingFileContext.java
  AL    src/main/java/org/apache/hadoop/hbase/metrics/MetricsRate.java
  AL    src/main/java/org/apache/hadoop/hbase/metrics/MetricsMBeanBase.java
  AL    src/main/java/org/apache/hadoop/hbase/TableNotDisabledException.java
  AL    src/main/java/org/apache/hadoop/hbase/HServerAddress.java
  AL    src/main/java/org/apache/hadoop/hbase/DoNotRetryIOException.java
  AL    src/main/ruby/shell.rb
  AL    src/main/ruby/shell/commands/enable_region.rb
  AL    src/main/ruby/shell/commands/get_counter.rb
  AL    src/main/ruby/shell/commands/disable.rb
  AL    src/main/ruby/shell/commands/zk.rb
  AL    src/main/ruby/shell/commands/delete.rb
  AL    src/main/ruby/shell/commands/put.rb
  AL    src/main/ruby/shell/commands/major_compact.rb
  AL    src/main/ruby/shell/commands/version.rb
  AL    src/main/ruby/shell/commands/compact.rb
  AL    src/main/ruby/shell/commands/deleteall.rb
  AL    src/main/ruby/shell/commands/disable_region.rb
  AL    src/main/ruby/shell/commands/flush.rb
  AL    src/main/ruby/shell/commands/incr.rb
  AL    src/main/ruby/shell/commands/scan.rb
  AL    src/main/ruby/shell/commands/drop.rb
  AL    src/main/ruby/shell/commands/shutdown.rb
  AL    src/main/ruby/shell/commands/exists.rb
  AL    src/main/ruby/shell/commands/count.rb
  AL    src/main/ruby/shell/commands/alter.rb
  AL    src/main/ruby/shell/commands/enable.rb
  AL    src/main/ruby/shell/commands/describe.rb
  AL    src/main/ruby/shell/commands/get.rb
  AL    src/main/ruby/shell/commands/status.rb
  AL    src/main/ruby/shell/commands/list.rb
  AL    src/main/ruby/shell/commands/split.rb
  AL    src/main/ruby/shell/commands/truncate.rb
  AL    src/main/ruby/shell/commands/close_region.rb
  AL    src/main/ruby/shell/commands/create.rb
  AL    src/main/ruby/shell/commands/zk_dump.rb
  AL    src/main/ruby/shell/commands.rb
  AL    src/main/ruby/shell/formatter.rb
  AL    src/main/ruby/hbase.rb
  AL    src/main/ruby/irb/hirb.rb
  AL    src/main/ruby/hbase/hbase.rb
  AL    src/main/ruby/hbase/table.rb
  AL    src/main/ruby/hbase/admin.rb
  AL    src/main/javadoc/overview.html
 !????? src/main/javadoc/org/apache/hadoop/hbase/thrift/doc-files/style.css
 !????? src/main/javadoc/org/apache/hadoop/hbase/thrift/doc-files/Hbase.html
 !????? src/main/javadoc/org/apache/hadoop/hbase/thrift/doc-files/index.html
  AL    src/main/javadoc/org/apache/hadoop/hbase/thrift/package.html
  AL    src/main/javadoc/org/apache/hadoop/hbase/replication/package.html
  AL    src/main/javadoc/org/apache/hadoop/hbase/io/hfile/package.html
  AL    src/main/javadoc/org/apache/hadoop/hbase/ipc/package.html
 !????? src/docbkx/sample_article.xml
 !????? src/docbkx/book.xml
 !????? src/assembly/bin.xml
 !????? src/assembly/src.xml
  AL    src/saveVersion.sh
  AL    src/examples/thrift/Makefile
  AL    src/examples/thrift/DemoClient.rb
  AL    src/examples/thrift/DemoClient.php
  AL    src/examples/thrift/DemoClient.py
  AL    src/examples/thrift/DemoClient.cpp
  N     src/examples/thrift/README.txt
  AL    src/examples/thrift/DemoClient.java
 !????? src/examples/mapreduce/index-builder-setup.rb
  AL    src/examples/mapreduce/org/apache/hadoop/hbase/mapreduce/IndexBuilder.java
  AL    src/examples/mapreduce/org/apache/hadoop/hbase/mapreduce/SampleUploader.java
  N     src/examples/README.txt
  N     README.txt
 !????? cloudera/hbase.1
 !????? cloudera/apply-patches
 !????? cloudera/do-release-build
 !????? cloudera/patches/0021-HBASE-3008-Memstore.updateColumnValue-passes-wrong-f.patch
 !????? cloudera/patches/0016-CLOUDERA-BUILD.-HBase-running-on-secure-hadoop-tempo.patch
 !????? cloudera/patches/0009-CLOUDERA-BUILD.-rsync-all-of-lib-into-target-directo.patch
 !????? cloudera/patches/0005-Re-enable-log-split-test.patch
 !????? cloudera/patches/0011-SequenceFileLogWriter-doesn-t-need-to-actually-call-.patch
 !????? cloudera/patches/0014-HBASE-3000.-Add-hbase-classpath-command.patch
 !????? cloudera/patches/0029-CLOUDERA-BUILD.-Publish-to-Cloudera-maven-repo.-Use-.patch
  AL    cloudera/patches/0024-HBASE-2799.-Append-not-enabled-warning-should-not-sh.patch
 !????? cloudera/patches/0015-HBASE-3001.-TableMapReduceUtil-should-always-add-dep.patch
 !????? cloudera/patches/0006-HBASE-2773.-Check-for-null-values-in-meta-in-test-ut.patch
 !????? cloudera/patches/0010-HBASE-2467.-Concurrent-flushers-in-HLog-sync-using-H.patch
 !????? cloudera/patches/0023-CLOUDERA-BUILD.-cloudera-directory-should-get-instal.patch
 !????? cloudera/patches/0002-CLOUDERA-BUILD.-Add-build-infrastructure.patch
  AL    cloudera/patches/0013-HBASE-2980.-Refactor-region-server-command-line-to-a.patch
 !????? cloudera/patches/0026-CLOUDERA-BUILD.-Update-hadoop-version.patch
 !????? cloudera/patches/0007-CLOUDERA-BUILD.-Include-cloudera-dir-in-src-assembly.patch
 !????? cloudera/patches/0017-HBASE-2782.-QoS-for-META-table-access.patch
 !????? cloudera/patches/0019-CLOUDERA-BUILD.-Fix-copy-of-bin-to-be-cp-a.patch
 !????? cloudera/patches/0028-CLOUDERA-BUILD.-Fix-versionless-jar-naming-symlinks-.patch
 !????? cloudera/patches/0025-HBASE-3096.-TestCompaction-timing-out.patch
 !????? cloudera/patches/0003-CLOUDERA-BUILD.-Switch-to-CDH3b3-snapshot-in-todd-s-.patch
 !????? cloudera/patches/0008-CLOUDERA-BUILD.-hbase-config.sh-should-set-HBASE_PID.patch
 !????? cloudera/patches/0018-CLOUDERA-BUILD.-Build-site-as-part-of-release-build.patch
 !????? cloudera/patches/0001-Updating-the-site-for-0.89.20100924.patch
  AL    cloudera/patches/0004-Add-VerifiableEditor.patch
  AL    cloudera/patches/0012-HBASE-2977.-Refactor-master-command-line-to-a-new-cl.patch
 !????? cloudera/patches/0022-CLOUDERA-BUILD.-Change-wrapper-scripts-to-not-be-dep.patch
 !????? cloudera/patches/0020-Fix-src-assembly-to-make-java-libs-644-and-not-inclu.patch
 !????? cloudera/patches/0030-CLOUDERA-BUILD.-Add-man-page.patch
 !????? cloudera/patches/0027-HBASE-3101.-bin-assembly-should-include-tests-and-so.patch
 !????? cloudera/build.properties
 !????? cloudera/CHANGES.cloudera.txt
 !????? cloudera/README.cloudera
 !????? cloudera/install_hbase.sh
 !????? CHANGES.txt
  AL    conf/hbase-site.xml.psuedo-distributed.template
 !????? conf/regionservers
  AL    conf/tohtml.xsl
 !????? conf/log4j.properties
  AL    conf/hbase-site.xml
  AL    conf/hbase-env.sh
 !????? conf/hadoop-metrics.properties
 
 *****************************************************
 Printing headers for files without AL header...
 
 
 =======================================================================
 ==bin/set_meta_block_caching.rb
 =======================================================================
# Set in_memory=true and blockcache=true on catalog tables.
# The .META. and -ROOT- tables can be created with caching and
# in_memory set to false.  You want them set to true so that
# these hot tables make it into cache.  To see if the
# .META. table has BLOCKCACHE set, in the shell do the following:
#
#   hbase&gt; scan '-ROOT-'
#
# Look for the 'info' column family.  See if BLOCKCACHE =&gt; 'true'? 
# If not, run this script and it will set the value to true.
# Setting cache to 'true' will only take effect on region restart
# of if you close the .META. region -- *disruptive* -- and have
# it deploy elsewhere.  This script runs against an up and running
# hbase instance.
# 
# To see usage for this script, run: 
#
#  ${HBASE_HOME}/bin/hbase org.jruby.Main set_meta_block_caching.rb
#
include Java
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.HConstants
import org.apache.hadoop.hbase.HRegionInfo
import org.apache.hadoop.hbase.client.HTable
import org.apache.hadoop.hbase.client.Delete
import org.apache.hadoop.hbase.client.Put
import org.apache.hadoop.hbase.client.Scan
import org.apache.hadoop.hbase.HTableDescriptor
import org.apache.hadoop.hbase.HBaseConfiguration
import org.apache.hadoop.hbase.util.FSUtils
import org.apache.hadoop.hbase.util.Writables
import org.apache.hadoop.fs.Path
import org.apache.hadoop.fs.FileSystem
import org.apache.commons.logging.LogFactory

# Name of this script
NAME = &quot;set_meta_block_caching.rb&quot;


# Print usage for this script
def usage
  puts 'Usage: %s.rb]' % NAME
  exit!
end

# Get configuration to use.
c = HBaseConfiguration.new()

# Set hadoop filesystem configuration using the hbase.rootdir.
# Otherwise, we'll always use localhost though the hbase.rootdir

 =======================================================================
 ==bin/local-regionservers.sh
 =======================================================================
#!/bin/sh
# This is used for starting multiple regionservers on the same machine.
# run it from hbase-dir/ just like 'bin/hbase'
# Supports up to 100 regionservers (limitation = overlapping ports)

bin=`dirname &quot;${BASH_SOURCE-$0}&quot;`
bin=`cd &quot;$bin&quot; &gt;/dev/null &amp;&amp; pwd`

if [ $# -lt 2 ]; then
  S=`basename &quot;${BASH_SOURCE-$0}&quot;`
  echo &quot;Usage: $S [start|stop] offset(s)&quot;
  echo &quot;&quot;
  echo &quot;    e.g. $S start 1 2&quot;
  exit
fi

# sanity check: make sure your regionserver opts don't use ports [i.e. JMX/DBG]
export HBASE_REGIONSERVER_OPTS=&quot; &quot;

run_regionserver () {
  DN=$2
  export HBASE_IDENT_STRING=&quot;$USER-$DN&quot;
  HBASE_REGIONSERVER_ARGS=&quot;\
    -D hbase.regionserver.port=`expr 60200 + $DN` \
    -D hbase.regionserver.info.port=`expr 60300 + $DN`&quot;
  &quot;$bin&quot;/hbase-daemon.sh $1 regionserver $HBASE_REGIONSERVER_ARGS
}

cmd=$1
shift;

for i in $*
do
  run_regionserver  $cmd $i
done

 =======================================================================
 ==bin/local-master-backup.sh
 =======================================================================
#!/bin/sh
# This is used for starting multiple masters on the same machine.
# run it from hbase-dir/ just like 'bin/hbase'
# Supports up to 10 masters (limitation = overlapping ports)

bin=`dirname &quot;${BASH_SOURCE-$0}&quot;`
bin=`cd &quot;$bin&quot; &gt;/dev/null &amp;&amp; pwd`

if [ $# -lt 2 ]; then
  S=`basename &quot;${BASH_SOURCE-$0}&quot;`
  echo &quot;Usage: $S [start|stop] offset(s)&quot;
  echo &quot;&quot;
  echo &quot;    e.g. $S start 1&quot;
  exit
fi

# sanity check: make sure your master opts don't use ports [i.e. JMX/DBG]
export HBASE_MASTER_OPTS=&quot; &quot;

run_master () {
  DN=$2
  export HBASE_IDENT_STRING=&quot;$USER-$DN&quot;
  HBASE_MASTER_ARGS=&quot;\
    --backup \
    -D hbase.master.port=`expr 60000 + $DN` \
    -D hbase.master.info.port=`expr 60010 + $DN`&quot;
  &quot;$bin&quot;/hbase-daemon.sh $1 master $HBASE_MASTER_ARGS
}

cmd=$1
shift;

for i in $*
do
  run_master  $cmd $i
done

 =======================================================================
 ==.git/config
 =======================================================================
[core]
	repositoryformatversion = 0
	filemode = true
	bare = false
	logallrefupdates = true

 =======================================================================
 ==.git/hooks/applypatch-msg.sample
 =======================================================================
#!/bin/sh
#
# An example hook script to check the commit log message taken by
# applypatch from an e-mail message.
#
# The hook should exit with non-zero status after issuing an
# appropriate message if it wants to stop the commit.  The hook is
# allowed to edit the commit message file.
#
# To enable this hook, rename this file to &quot;applypatch-msg&quot;.

. git-sh-setup
test -x &quot;$GIT_DIR/hooks/commit-msg&quot; &amp;&amp;
	exec &quot;$GIT_DIR/hooks/commit-msg&quot; ${1+&quot;$@&quot;}
:

 =======================================================================
 ==.git/hooks/post-receive.sample
 =======================================================================
#!/bin/sh
#
# An example hook script for the &quot;post-receive&quot; event.
#
# The &quot;post-receive&quot; script is run after receive-pack has accepted a pack
# and the repository has been updated.  It is passed arguments in through
# stdin in the form
#  &lt;oldrev&gt; &lt;newrev&gt; &lt;refname&gt;
# For example:
#  aa453216d1b3e49e7f6f98441fa56946ddcd6a20 68f7abf4e6f922807889f52bc043ecd31b79f814 refs/heads/master
#
# see contrib/hooks/ for a sample, or uncomment the next line and
# rename the file to &quot;post-receive&quot;.

#. /usr/share/doc/git-core/contrib/hooks/post-receive-email

 =======================================================================
 ==.git/hooks/pre-commit.sample
 =======================================================================
#!/bin/sh
#
# An example hook script to verify what is about to be committed.
# Called by git-commit with no arguments.  The hook should
# exit with non-zero status after issuing an appropriate message if
# it wants to stop the commit.
#
# To enable this hook, rename this file to &quot;pre-commit&quot;.

if git-rev-parse --verify HEAD &gt;/dev/null 2&gt;&amp;1
then
	against=HEAD
else
	# Initial commit: diff against an empty tree object
	against=4b825dc642cb6eb9a060e54bf8d69288fbee4904
fi

# If you want to allow non-ascii filenames set this variable to true.
allownonascii=$(git config hooks.allownonascii)

# Cross platform projects tend to avoid non-ascii filenames; prevent
# them from being added to the repository. We exploit the fact that the
# printable range starts at the space character and ends with tilde.
if [ &quot;$allownonascii&quot; != &quot;true&quot; ] &amp;&amp;
	# Note that the use of brackets around a tr range is ok here, (it's
	# even required, for portability to Solaris 10's /usr/bin/tr), since
	# the square bracket bytes happen to fall in the designated range.
	test &quot;$(git diff --cached --name-only --diff-filter=A -z $against |
	  LC_ALL=C tr -d '[ -~]\0')&quot;
then
	echo &quot;Error: Attempt to add a non-ascii file name.&quot;
	echo
	echo &quot;This can cause problems if you want to work&quot;
	echo &quot;with people on other platforms.&quot;
	echo
	echo &quot;To be portable it is advisable to rename the file ...&quot;
	echo
	echo &quot;If you know what you are doing you can disable this&quot;
	echo &quot;check using:&quot;
	echo
	echo &quot;  git config hooks.allownonascii true&quot;
	echo
	exit 1
fi

exec git diff-index --check --cached $against --

 =======================================================================
 ==.git/hooks/pre-applypatch.sample
 =======================================================================
#!/bin/sh
#
# An example hook script to verify what is about to be committed
# by applypatch from an e-mail message.
#
# The hook should exit with non-zero status after issuing an
# appropriate message if it wants to stop the commit.
#
# To enable this hook, rename this file to &quot;pre-applypatch&quot;.

. git-sh-setup
test -x &quot;$GIT_DIR/hooks/pre-commit&quot; &amp;&amp;
	exec &quot;$GIT_DIR/hooks/pre-commit&quot; ${1+&quot;$@&quot;}
:

 =======================================================================
 ==.git/hooks/update.sample
 =======================================================================
#!/bin/sh
#
# An example hook script to blocks unannotated tags from entering.
# Called by git-receive-pack with arguments: refname sha1-old sha1-new
#
# To enable this hook, rename this file to &quot;update&quot;.
#
# Config
# ------
# hooks.allowunannotated
#   This boolean sets whether unannotated tags will be allowed into the
#   repository.  By default they won't be.
# hooks.allowdeletetag
#   This boolean sets whether deleting tags will be allowed in the
#   repository.  By default they won't be.
# hooks.allowmodifytag
#   This boolean sets whether a tag may be modified after creation. By default
#   it won't be.
# hooks.allowdeletebranch
#   This boolean sets whether deleting branches will be allowed in the
#   repository.  By default they won't be.
# hooks.denycreatebranch
#   This boolean sets whether remotely creating branches will be denied
#   in the repository.  By default this is allowed.
#

# --- Command line
refname=&quot;$1&quot;
oldrev=&quot;$2&quot;
newrev=&quot;$3&quot;

# --- Safety check
if [ -z &quot;$GIT_DIR&quot; ]; then
	echo &quot;Don't run this script from the command line.&quot; &gt;&amp;2
	echo &quot; (if you want, you could supply GIT_DIR then run&quot; &gt;&amp;2
	echo &quot;  $0 &lt;ref&gt; &lt;oldrev&gt; &lt;newrev&gt;)&quot; &gt;&amp;2
	exit 1
fi

if [ -z &quot;$refname&quot; -o -z &quot;$oldrev&quot; -o -z &quot;$newrev&quot; ]; then
	echo &quot;Usage: $0 &lt;ref&gt; &lt;oldrev&gt; &lt;newrev&gt;&quot; &gt;&amp;2
	exit 1
fi

# --- Config
allowunannotated=$(git config --bool hooks.allowunannotated)
allowdeletebranch=$(git config --bool hooks.allowdeletebranch)
denycreatebranch=$(git config --bool hooks.denycreatebranch)
allowdeletetag=$(git config --bool hooks.allowdeletetag)
allowmodifytag=$(git config --bool hooks.allowmodifytag)

 =======================================================================
 ==.git/hooks/post-update.sample
 =======================================================================
#!/bin/sh
#
# An example hook script to prepare a packed repository for use over
# dumb transports.
#
# To enable this hook, rename this file to &quot;post-update&quot;.

exec git-update-server-info

 =======================================================================
 ==.git/hooks/pre-rebase.sample
 =======================================================================
#!/bin/sh
#
# Copyright (c) 2006, 2008 Junio C Hamano
#
# The &quot;pre-rebase&quot; hook is run just before &quot;git-rebase&quot; starts doing
# its job, and can prevent the command from running by exiting with
# non-zero status.
#
# The hook is called with the following parameters:
#
# $1 -- the upstream the series was forked from.
# $2 -- the branch being rebased (or empty when rebasing the current branch).
#
# This sample shows how to prevent topic branches that are already
# merged to 'next' branch from getting rebased, because allowing it
# would result in rebasing already published history.

publish=next
basebranch=&quot;$1&quot;
if test &quot;$#&quot; = 2
then
	topic=&quot;refs/heads/$2&quot;
else
	topic=`git symbolic-ref HEAD` ||
	exit 0 ;# we do not interrupt rebasing detached HEAD
fi

case &quot;$topic&quot; in
refs/heads/??/*)
	;;
*)
	exit 0 ;# we do not interrupt others.
	;;
esac

# Now we are dealing with a topic branch being rebased
# on top of master.  Is it OK to rebase it?

# Does the topic really exist?
git show-ref -q &quot;$topic&quot; || {
	echo &gt;&amp;2 &quot;No such branch $topic&quot;
	exit 1
}

# Is topic fully merged to master?
not_in_master=`git-rev-list --pretty=oneline ^master &quot;$topic&quot;`
if test -z &quot;$not_in_master&quot;
then
	echo &gt;&amp;2 &quot;$topic is fully merged to master; better remove it.&quot;
	exit 1 ;# we could allow it, but there is no point.

 =======================================================================
 ==.git/hooks/post-commit.sample
 =======================================================================
#!/bin/sh
#
# An example hook script that is called after a successful
# commit is made.
#
# To enable this hook, rename this file to &quot;post-commit&quot;.

: Nothing

 =======================================================================
 ==.git/hooks/prepare-commit-msg.sample
 =======================================================================
#!/bin/sh
#
# An example hook script to prepare the commit log message.
# Called by git-commit with the name of the file that has the
# commit message, followed by the description of the commit
# message's source.  The hook's purpose is to edit the commit
# message file.  If the hook fails with a non-zero status,
# the commit is aborted.
#
# To enable this hook, rename this file to &quot;prepare-commit-msg&quot;.

# This hook includes three examples.  The first comments out the
# &quot;Conflicts:&quot; part of a merge commit.
#
# The second includes the output of &quot;git diff --name-status -r&quot;
# into the message, just before the &quot;git status&quot; output.  It is
# commented because it doesn't cope with --amend or with squashed
# commits.
#
# The third example adds a Signed-off-by line to the message, that can
# still be edited.  This is rarely a good idea.

case &quot;$2,$3&quot; in
  merge,)
    perl -i.bak -ne 's/^/# /, s/^# #/#/ if /^Conflicts/ .. /#/; print' &quot;$1&quot; ;;

# ,|template,)
#   perl -i.bak -pe '
#      print &quot;\n&quot; . `git diff --cached --name-status -r`
#	 if /^#/ &amp;&amp; $first++ == 0' &quot;$1&quot; ;;

  *) ;;
esac

# SOB=$(git var GIT_AUTHOR_IDENT | sed -n 's/^\(.*&gt;\).*$/Signed-off-by: \1/p')
# grep -qs &quot;^$SOB&quot; &quot;$1&quot; || echo &quot;$SOB&quot; &gt;&gt; &quot;$1&quot;

 =======================================================================
 ==.git/hooks/commit-msg.sample
 =======================================================================
#!/bin/sh
#
# An example hook script to check the commit log message.
# Called by git-commit with one argument, the name of the file
# that has the commit message.  The hook should exit with non-zero
# status after issuing an appropriate message if it wants to stop the
# commit.  The hook is allowed to edit the commit message file.
#
# To enable this hook, rename this file to &quot;commit-msg&quot;.

# Uncomment the below to add a Signed-off-by line to the message.
# Doing this in a hook is a bad idea in general, but the prepare-commit-msg
# hook is more suited to it.
#
# SOB=$(git var GIT_AUTHOR_IDENT | sed -n 's/^\(.*&gt;\).*$/Signed-off-by: \1/p')
# grep -qs &quot;^$SOB&quot; &quot;$1&quot; || echo &quot;$SOB&quot; &gt;&gt; &quot;$1&quot;

# This example catches duplicate Signed-off-by lines.

test &quot;&quot; = &quot;$(grep '^Signed-off-by: ' &quot;$1&quot; |
	 sort | uniq -c | sed -e '/^[ 	]*1[ 	]/d')&quot; || {
	echo &gt;&amp;2 Duplicate Signed-off-by lines.
	exit 1
}

 =======================================================================
 ==.git/info/exclude
 =======================================================================
# git-ls-files --others --exclude-from=.git/info/exclude
# Lines that start with '#' are comments.
# For a project mostly in C, the following would be a good set of
# exclude patterns (uncomment them if you want to use them):
# *.[oa]
# *~

 =======================================================================
 ==.git/description
 =======================================================================
Unnamed repository; edit this file 'description' to name the repository.

 =======================================================================
 ==.git/HEAD
 =======================================================================
ref: refs/heads/master

 =======================================================================
 ==src/site/site.xml
 =======================================================================
&lt;?xml version=&quot;1.0&quot; encoding=&quot;ISO-8859-1&quot;?&gt;

&lt;project xmlns=&quot;http://maven.apache.org/DECORATION/1.0.0&quot;
    xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
    xsi:schemaLocation=&quot;http://maven.apache.org/DECORATION/1.0.0 http://maven.apache.org/xsd/decoration-1.0.0.xsd&quot;&gt;
  &lt;bannerLeft&gt;
    &lt;name&gt;HBase&lt;/name&gt;
    &lt;src&gt;http://hbase.apache.org/images/hbase_logo_med.gif&lt;/src&gt;
    &lt;href&gt;http://hbase.apache.org/&lt;/href&gt;
  &lt;/bannerLeft&gt;
  &lt;bannerRight&gt;
    &lt;src&gt;http://hbase.apache.org/images/asf_logo_wide.png&lt;/src&gt;
    &lt;href&gt;http://www.apache.org/&lt;/href&gt;
  &lt;/bannerRight&gt;
  &lt;version position=&quot;right&quot; /&gt;
  &lt;publishDate position=&quot;right&quot; /&gt;
  &lt;body&gt;
    &lt;menu name=&quot;HBase&quot;&gt;
      &lt;item name=&quot;Overview&quot; href=&quot;index.html&quot;/&gt;
      &lt;item name=&quot;License&quot; href=&quot;license.html&quot; /&gt;
      &lt;item name=&quot;Downloads&quot; href=&quot;http://www.apache.org/dyn/closer.cgi/hbase/&quot; /&gt;
      &lt;item name=&quot;Release Notes&quot; href=&quot;https://issues.apache.org/jira/browse/HBASE?report=com.atlassian.jira.plugin.system.project:changelog-panel&quot; /&gt;
      &lt;item name=&quot;Issue Tracking&quot; href=&quot;issue-tracking.html&quot; /&gt;
      &lt;item name=&quot;Mailing Lists&quot; href=&quot;mail-lists.html&quot; /&gt;
      &lt;item name=&quot;Source Repository&quot; href=&quot;source-repository.html&quot; /&gt;
      &lt;item name=&quot;FAQ&quot; href=&quot;faq.html&quot; /&gt;
      &lt;item name=&quot;Wiki&quot; href=&quot;http://wiki.apache.org/hadoop/Hbase&quot; /&gt;
      &lt;item name=&quot;Team&quot; href=&quot;team-list.html&quot; /&gt;
    &lt;/menu&gt;
    &lt;menu name=&quot;Documentation&quot;&gt;
        &lt;item name=&quot;Getting Started&quot; href=&quot;apidocs/overview-summary.html#overview_description&quot; /&gt;
      &lt;item name=&quot;API&quot; href=&quot;apidocs/index.html&quot; /&gt;
      &lt;item name=&quot;X-Ref&quot; href=&quot;xref/index.html&quot; /&gt;
      &lt;item name=&quot;ACID Semantics&quot; href=&quot;acid-semantics.html&quot; /&gt;
      &lt;item name=&quot;Bulk Loads&quot; href=&quot;bulk-loads.html&quot; /&gt;
      &lt;item name=&quot;Metrics&quot;      href=&quot;metrics.html&quot; /&gt;
      &lt;item name=&quot;HBase on Windows&quot;      href=&quot;cygwin.html&quot; /&gt;
      &lt;item name=&quot;Cluster replication&quot;      href=&quot;replication.html&quot; /&gt;
      &lt;item name=&quot;Pseudo-Distributed HBase&quot;      href=&quot;pseudo-distributed.html&quot; /&gt;
      &lt;item name=&quot;HBase Book&quot;      href=&quot;book.html&quot; /&gt;
      &lt;item name=&quot;Example Docbook Article&quot;      href=&quot;sample_article.html&quot; /&gt;
    &lt;/menu&gt;
  &lt;/body&gt;
    &lt;skin&gt;
        &lt;groupId&gt;org.apache.maven.skins&lt;/groupId&gt;
      &lt;artifactId&gt;maven-stylus-skin&lt;/artifactId&gt;
    &lt;/skin&gt;
&lt;/project&gt;


 =======================================================================
 ==src/site/site.vm
 =======================================================================
&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD XHTML 1.0 Transitional//EN&quot; &quot;http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd&quot;&gt;
&lt;!-- Generated by Apache Maven Doxia at $dateFormat.format( $currentDate ) --&gt;
#macro ( link $href $name $target $img $position $alt $border $width $height )
  #set ( $linkTitle = ' title=&quot;' + $name + '&quot;' )
  #if( $target )
    #set ( $linkTarget = ' target=&quot;' + $target + '&quot;' )
  #else
    #set ( $linkTarget = &quot;&quot; )
  #end
  #if ( ( $href.toLowerCase().startsWith(&quot;http&quot;) || $href.toLowerCase().startsWith(&quot;https&quot;) ) )
    #set ( $linkClass = ' class=&quot;externalLink&quot;' )
  #else
    #set ( $linkClass = &quot;&quot; )
  #end
  #if ( $img )
    #if ( $position == &quot;left&quot; )
      &lt;a href=&quot;$href&quot;$linkClass$linkTarget$linkTitle&gt;#image($img $alt $border $width $height)$name&lt;/a&gt;
    #else
      &lt;a href=&quot;$href&quot;$linkClass$linkTarget$linkTitle&gt;$name #image($img $alt $border $width $height)&lt;/a&gt;
    #end
  #else
    &lt;a href=&quot;$href&quot;$linkClass$linkTarget$linkTitle&gt;$name&lt;/a&gt;
  #end
#end
##
#macro ( image $img $alt $border $width $height )
  #if( $img )
    #if ( ! ( $img.toLowerCase().startsWith(&quot;http&quot;) || $img.toLowerCase().startsWith(&quot;https&quot;) ) )
      #set ( $imgSrc = $PathTool.calculateLink( $img, . ) )
      #set ( $imgSrc = $imgSrc.replaceAll( &quot;\\&quot;, &quot;/&quot; ) )
      #set ( $imgSrc = ' src=&quot;' + $imgSrc + '&quot;' )
    #else
      #set ( $imgSrc = ' src=&quot;' + $img + '&quot;' )
    #end
    #if( $alt )
      #set ( $imgAlt = ' alt=&quot;' + $alt + '&quot;' )
    #else
      #set ( $imgAlt = ' alt=&quot;&quot;' )
    #end
    #if( $border )
      #set ( $imgBorder = ' border=&quot;' + $border + '&quot;' )
    #else
      #set ( $imgBorder = &quot;&quot; )
    #end
    #if( $width )
      #set ( $imgWidth = ' width=&quot;' + $width + '&quot;' )
    #else
      #set ( $imgWidth = &quot;&quot; )
    #end
    #if( $height )

 =======================================================================
 ==src/test/resources/log4j.properties
 =======================================================================
# Define some default values that can be overridden by system properties
hbase.root.logger=INFO,console
hbase.log.dir=.
hbase.log.file=hbase.log

# Define the root logger to the system property &quot;hbase.root.logger&quot;.
log4j.rootLogger=${hbase.root.logger}

# Logging Threshold
log4j.threshhold=ALL

#
# Daily Rolling File Appender
#
log4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender
log4j.appender.DRFA.File=${hbase.log.dir}/${hbase.log.file}

# Rollver at midnight
log4j.appender.DRFA.DatePattern=.yyyy-MM-dd

# 30-day backup
#log4j.appender.DRFA.MaxBackupIndex=30
log4j.appender.DRFA.layout=org.apache.log4j.PatternLayout

# Pattern format: Date LogLevel LoggerName LogMessage
#log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n

# Debugging Pattern format
log4j.appender.DRFA.layout.ConversionPattern=%d %-5p [%t] %C{2}(%L): %m%n


#
# console
# Add &quot;console&quot; to rootlogger above if you want to use this
#
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d %-5p [%t] %C{2}(%L): %m%n

# Custom Logging levels

#log4j.logger.org.apache.hadoop.fs.FSNamesystem=DEBUG

log4j.logger.org.apache.hadoop=WARN
log4j.logger.org.apache.zookeeper=ERROR
log4j.logger.org.apache.hadoop.hbase=DEBUG

 =======================================================================
 ==src/test/resources/mapred-queues.xml
 =======================================================================
&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;!-- This is the template for queue configuration. The format supports nesting of
     queues within queues - a feature called hierarchical queues. All queues are
     defined within the 'queues' tag which is the top level element for this
     XML document.
     The 'aclsEnabled' attribute should be set to true, if ACLs should be checked
     on queue operations such as submitting jobs, killing jobs etc. --&gt;
&lt;queues aclsEnabled=&quot;false&quot;&gt;

  &lt;!-- Configuration for a queue is specified by defining a 'queue' element. --&gt;
  &lt;queue&gt;

    &lt;!-- Name of a queue. Queue name cannot contain a ':'  --&gt;
    &lt;name&gt;default&lt;/name&gt;

    &lt;!-- properties for a queue, typically used by schedulers,
    can be defined here --&gt;
    &lt;properties&gt;
    &lt;/properties&gt;

	&lt;!-- State of the queue. If running, the queue will accept new jobs.
         If stopped, the queue will not accept new jobs. --&gt;
    &lt;state&gt;running&lt;/state&gt;

    &lt;!-- Specifies the ACLs to check for submitting jobs to this queue.
         If set to '*', it allows all users to submit jobs to the queue.
         For specifying a list of users and groups the format to use is
         user1,user2 group1,group2 --&gt;
    &lt;acl-submit-job&gt;*&lt;/acl-submit-job&gt;

    &lt;!-- Specifies the ACLs to check for modifying jobs in this queue.
         Modifications include killing jobs, tasks of jobs or changing
         priorities.
         If set to '*', it allows all users to submit jobs to the queue.
         For specifying a list of users and groups the format to use is
         user1,user2 group1,group2 --&gt;
    &lt;acl-administer-jobs&gt;*&lt;/acl-administer-jobs&gt;
  &lt;/queue&gt;

  &lt;!-- Here is a sample of a hierarchical queue configuration
       where q2 is a child of q1. In this example, q2 is a leaf level
       queue as it has no queues configured within it. Currently, ACLs
       and state are only supported for the leaf level queues.
       Note also the usage of properties for the queue q2.
  &lt;queue&gt;
    &lt;name&gt;q1&lt;/name&gt;
    &lt;queue&gt;
      &lt;name&gt;q2&lt;/name&gt;
      &lt;properties&gt;
        &lt;property key=&quot;capacity&quot; value=&quot;20&quot;/&gt;

 =======================================================================
 ==src/test/java/org/apache/hadoop/hbase/filter/TestColumnPrefixFilter.java
 =======================================================================
package org.apache.hadoop.hbase.filter;

import static org.junit.Assert.*;

import java.io.IOException;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;

import org.apache.hadoop.hbase.HBaseTestingUtility;
import org.apache.hadoop.hbase.HColumnDescriptor;
import org.apache.hadoop.hbase.HRegionInfo;
import org.apache.hadoop.hbase.HTableDescriptor;
import org.apache.hadoop.hbase.KeyValue;
import org.apache.hadoop.hbase.KeyValueTestUtil;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.client.Scan;
import org.apache.hadoop.hbase.regionserver.HRegion;
import org.apache.hadoop.hbase.regionserver.InternalScanner;
import org.apache.hadoop.hbase.util.Bytes;
import org.junit.Test;

public class TestColumnPrefixFilter {

  private final static HBaseTestingUtility TEST_UTIL = new
      HBaseTestingUtility();

  @Test
  public void testColumnPrefixFilter() throws IOException {
    String family = &quot;Family&quot;;
    HTableDescriptor htd = new HTableDescriptor(&quot;TestColumnPrefixFilter&quot;);
    htd.addFamily(new HColumnDescriptor(family));
    HRegionInfo info = new HRegionInfo(htd, null, null, false);
    HRegion region = HRegion.createHRegion(info, HBaseTestingUtility.
        getTestDir(), TEST_UTIL.getConfiguration());

    List&lt;String&gt; rows = generateRandomWords(100, &quot;row&quot;);
    List&lt;String&gt; columns = generateRandomWords(10000, &quot;column&quot;);
    long maxTimestamp = 2;

    List&lt;KeyValue&gt; kvList = new ArrayList&lt;KeyValue&gt;();

    Map&lt;String, List&lt;KeyValue&gt;&gt; prefixMap = new HashMap&lt;String,
        List&lt;KeyValue&gt;&gt;();

    prefixMap.put(&quot;p&quot;, new ArrayList&lt;KeyValue&gt;());
    prefixMap.put(&quot;s&quot;, new ArrayList&lt;KeyValue&gt;());

 =======================================================================
 ==src/test/java/org/apache/hadoop/hbase/master/TestZKBasedCloseRegion.java
 =======================================================================

 =======================================================================
 ==src/test/java/org/apache/hadoop/hbase/master/TestZKBasedReopenRegion.java
 =======================================================================

 =======================================================================
 ==src/test/java/org/apache/hadoop/hbase/master/TestRestartCluster.java
 =======================================================================

 =======================================================================
 ==src/test/ruby/test_helper.rb
 =======================================================================
require 'test/unit'

module Testing
  module Declarative
    # define_test &quot;should do something&quot; do
    #   ...
    # end
    def define_test(name, &amp;block)
      test_name = &quot;test_#{name.gsub(/\s+/,'_')}&quot;.to_sym
      defined = instance_method(test_name) rescue false
      raise &quot;#{test_name} is already defined in #{self}&quot; if defined
      if block_given?
        define_method(test_name, &amp;block)
      else
        define_method(test_name) do
          flunk &quot;No implementation provided for #{name}&quot;
        end
      end
    end
  end
end

module Hbase
  module TestHelpers
    def setup_hbase
      @formatter = Shell::Formatter::Console.new()
      @hbase = ::Hbase::Hbase.new($TEST_CLUSTER.getConfiguration)
    end

    def table(table)
      @hbase.table(table, @formatter)
    end

    def admin
      @hbase.admin(@formatter)
    end

    def create_test_table(name)
      # Create the table if needed
      unless admin.exists?(name)
        admin.create name, [{'NAME' =&gt; 'x', 'VERSIONS' =&gt; 5}, 'y']
        return
      end

      # Enable the table if needed
      unless admin.enabled?(name)
        admin.enable(name)
      end
    end


 =======================================================================
 ==src/main/resources/hbase-webapps/static/hbase.css
 =======================================================================
h1, h2, h3 { color: DarkSlateBlue }
table { border: thin solid DodgerBlue }
tr { border: thin solid DodgerBlue }
td { border: thin solid DodgerBlue }
th { border: thin solid DodgerBlue }
#logo {float: left;}
#logo img {border: none;}
#page_title {padding-top: 27px;}

div.warning {
  border: 1px solid #666;
  background-color: #fcc;
  font-size: 110%;
  font-weight: bold;
}

 =======================================================================
 ==src/main/resources/hbase-webapps/regionserver/index.html
 =======================================================================
&lt;meta HTTP-EQUIV=&quot;REFRESH&quot; content=&quot;0;url=regionserver.jsp&quot;/&gt;

 =======================================================================
 ==src/main/resources/hbase-webapps/regionserver/regionserver.jsp
 =======================================================================
&lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot;
  import=&quot;java.util.*&quot;
  import=&quot;java.io.IOException&quot;
  import=&quot;org.apache.hadoop.io.Text&quot;
  import=&quot;org.apache.hadoop.hbase.regionserver.HRegionServer&quot;
  import=&quot;org.apache.hadoop.hbase.regionserver.HRegion&quot;
  import=&quot;org.apache.hadoop.hbase.regionserver.metrics.RegionServerMetrics&quot;
  import=&quot;org.apache.hadoop.hbase.util.Bytes&quot;
  import=&quot;org.apache.hadoop.hbase.HConstants&quot;
  import=&quot;org.apache.hadoop.hbase.HServerInfo&quot;
  import=&quot;org.apache.hadoop.hbase.HServerLoad&quot;
  import=&quot;org.apache.hadoop.hbase.HRegionInfo&quot; %&gt;&lt;%
  HRegionServer regionServer = (HRegionServer)getServletContext().getAttribute(HRegionServer.REGIONSERVER);
  HServerInfo serverInfo = null;
  try {
    serverInfo = regionServer.getHServerInfo();
  } catch (IOException e) {
    e.printStackTrace();
  }
  RegionServerMetrics metrics = regionServer.getMetrics();
  Collection&lt;HRegionInfo&gt; onlineRegions = regionServer.getSortedOnlineRegionInfos();
  int interval = regionServer.getConfiguration().getInt(&quot;hbase.regionserver.msginterval&quot;, 3000)/1000;

%&gt;&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;
&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD XHTML 1.0 Transitional//EN&quot; 
  &quot;http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd&quot;&gt; 
&lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot;&gt;
&lt;head&gt;&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html;charset=UTF-8&quot;/&gt;
&lt;title&gt;HBase Region Server: &lt;%= serverInfo.getServerAddress().getHostname() %&gt;:&lt;%= serverInfo.getServerAddress().getPort() %&gt;&lt;/title&gt;
&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;/static/hbase.css&quot; /&gt;
&lt;/head&gt;

&lt;body&gt;
&lt;a id=&quot;logo&quot; href=&quot;http://wiki.apache.org/lucene-hadoop/Hbase&quot;&gt;&lt;img src=&quot;/static/hbase_logo_med.gif&quot; alt=&quot;HBase Logo&quot; title=&quot;HBase Logo&quot; /&gt;&lt;/a&gt;
&lt;h1 id=&quot;page_title&quot;&gt;Region Server: &lt;%= serverInfo.getServerAddress().getHostname() %&gt;:&lt;%= serverInfo.getServerAddress().getPort() %&gt;&lt;/h1&gt;
&lt;p id=&quot;links_menu&quot;&gt;&lt;a href=&quot;/logs/&quot;&gt;Local logs&lt;/a&gt;, &lt;a href=&quot;/stacks&quot;&gt;Thread Dump&lt;/a&gt;, &lt;a href=&quot;/logLevel&quot;&gt;Log Level&lt;/a&gt;&lt;/p&gt;
&lt;hr id=&quot;head_rule&quot; /&gt;

&lt;h2&gt;Region Server Attributes&lt;/h2&gt;
&lt;table&gt;
&lt;tr&gt;&lt;th&gt;Attribute Name&lt;/th&gt;&lt;th&gt;Value&lt;/th&gt;&lt;th&gt;Description&lt;/th&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;HBase Version&lt;/td&gt;&lt;td&gt;&lt;%= org.apache.hadoop.hbase.util.VersionInfo.getVersion() %&gt;, r&lt;%= org.apache.hadoop.hbase.util.VersionInfo.getRevision() %&gt;&lt;/td&gt;&lt;td&gt;HBase version and svn revision&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;HBase Compiled&lt;/td&gt;&lt;td&gt;&lt;%= org.apache.hadoop.hbase.util.VersionInfo.getDate() %&gt;, &lt;%= org.apache.hadoop.hbase.util.VersionInfo.getUser() %&gt;&lt;/td&gt;&lt;td&gt;When HBase version was compiled and by whom&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Metrics&lt;/td&gt;&lt;td&gt;&lt;%= metrics.toString() %&gt;&lt;/td&gt;&lt;td&gt;RegionServer Metrics; file and heap sizes are in megabytes&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Zookeeper Quorum&lt;/td&gt;&lt;td&gt;&lt;%= regionServer.getZooKeeperWrapper().getQuorumServers() %&gt;&lt;/td&gt;&lt;td&gt;Addresses of all registered ZK servers&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;h2&gt;Online Regions&lt;/h2&gt;
&lt;% if (onlineRegions != null &amp;&amp; onlineRegions.size() &gt; 0) { %&gt;
&lt;table&gt;

 =======================================================================
 ==src/main/resources/hbase-webapps/master/table.jsp
 =======================================================================
&lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot;
  import=&quot;java.util.Map&quot;
  import=&quot;org.apache.hadoop.io.Writable&quot;
  import=&quot;org.apache.hadoop.conf.Configuration&quot;
  import=&quot;org.apache.hadoop.hbase.client.HTable&quot;
  import=&quot;org.apache.hadoop.hbase.client.HBaseAdmin&quot;
  import=&quot;org.apache.hadoop.hbase.HRegionInfo&quot;
  import=&quot;org.apache.hadoop.hbase.HServerAddress&quot;
  import=&quot;org.apache.hadoop.hbase.HServerInfo&quot;
  import=&quot;org.apache.hadoop.hbase.io.ImmutableBytesWritable&quot;
  import=&quot;org.apache.hadoop.hbase.master.HMaster&quot; 
  import=&quot;org.apache.hadoop.hbase.master.MetaRegion&quot;
  import=&quot;org.apache.hadoop.hbase.util.Bytes&quot;
  import=&quot;java.util.Map&quot;
  import=&quot;org.apache.hadoop.hbase.HConstants&quot;%&gt;&lt;%
  HMaster master = (HMaster)getServletContext().getAttribute(HMaster.MASTER);
  Configuration conf = master.getConfiguration();
  HBaseAdmin hbadmin = new HBaseAdmin(conf);
  String tableName = request.getParameter(&quot;name&quot;);
  HTable table = new HTable(conf, tableName);
  String tableHeader = &quot;&lt;h2&gt;Table Regions&lt;/h2&gt;&lt;table&gt;&lt;tr&gt;&lt;th&gt;Name&lt;/th&gt;&lt;th&gt;Region Server&lt;/th&gt;&lt;th&gt;Start Key&lt;/th&gt;&lt;th&gt;End Key&lt;/th&gt;&lt;/tr&gt;&quot;;
  HServerAddress rootLocation = master.getRegionManager().getRootRegionLocation();
  boolean showFragmentation = conf.getBoolean(&quot;hbase.master.ui.fragmentation.enabled&quot;, false);
  Map&lt;String, Integer&gt; frags = null;
  if (showFragmentation) {
      frags = master.getTableFragmentation();
  }
%&gt;

&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;
&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD XHTML 1.0 Transitional//EN&quot; 
  &quot;http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd&quot;&gt; 
&lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot;&gt;

&lt;%
  String action = request.getParameter(&quot;action&quot;);
  String key = request.getParameter(&quot;key&quot;);
  if ( action != null ) {
%&gt;
&lt;head&gt;&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html;charset=UTF-8&quot;/&gt;
&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;/static/hbase.css&quot; /&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;a id=&quot;logo&quot; href=&quot;http://wiki.apache.org/lucene-hadoop/Hbase&quot;&gt;&lt;img src=&quot;/static/hbase_logo_med.gif&quot; alt=&quot;HBase Logo&quot; title=&quot;HBase Logo&quot; /&gt;&lt;/a&gt;
&lt;h1 id=&quot;page_title&quot;&gt;Table action request accepted&lt;/h1&gt;
&lt;p&gt;&lt;hr&gt;&lt;p&gt;
&lt;%
  if (action.equals(&quot;split&quot;)) {
    if (key != null &amp;&amp; key.length() &gt; 0) {
      Writable[] arr = new Writable[1];

 =======================================================================
 ==src/main/resources/hbase-webapps/master/zk.jsp
 =======================================================================
&lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot;
  import=&quot;java.io.IOException&quot;
  import=&quot;org.apache.hadoop.conf.Configuration&quot;
  import=&quot;org.apache.hadoop.hbase.client.HBaseAdmin&quot;
  import=&quot;org.apache.hadoop.hbase.client.HConnection&quot;
  import=&quot;org.apache.hadoop.hbase.HRegionInfo&quot;
  import=&quot;org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper&quot;
  import=&quot;org.apache.hadoop.hbase.HBaseConfiguration&quot;
  import=&quot;org.apache.hadoop.hbase.master.HMaster&quot; 
  import=&quot;org.apache.hadoop.hbase.HConstants&quot;%&gt;&lt;%
  HMaster master = (HMaster)getServletContext().getAttribute(HMaster.MASTER);
  Configuration conf = master.getConfiguration();
  HBaseAdmin hbadmin = new HBaseAdmin(conf);
  HConnection connection = hbadmin.getConnection();
  ZooKeeperWrapper wrapper = connection.getZooKeeperWrapper();
%&gt;

&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;
&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD XHTML 1.0 Transitional//EN&quot; 
  &quot;http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd&quot;&gt; 
&lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot;&gt;
&lt;head&gt;&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html;charset=UTF-8&quot;/&gt;
&lt;title&gt;ZooKeeper Dump&lt;/title&gt;
&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;/static/hbase.css&quot; /&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;a id=&quot;logo&quot; href=&quot;http://hbase.org&quot;&gt;&lt;img src=&quot;/static/hbase_logo_med.gif&quot; alt=&quot;HBase Logo&quot; title=&quot;HBase Logo&quot; /&gt;&lt;/a&gt;
&lt;h1 id=&quot;page_title&quot;&gt;ZooKeeper Dump&lt;/h1&gt;
&lt;p id=&quot;links_menu&quot;&gt;&lt;a href=&quot;/master.jsp&quot;&gt;Master&lt;/a&gt;, &lt;a href=&quot;/logs/&quot;&gt;Local logs&lt;/a&gt;, &lt;a href=&quot;/stacks&quot;&gt;Thread Dump&lt;/a&gt;, &lt;a href=&quot;/logLevel&quot;&gt;Log Level&lt;/a&gt;&lt;/p&gt;
&lt;hr id=&quot;head_rule&quot; /&gt;
&lt;pre&gt;
&lt;%= wrapper.dump() %&gt;
&lt;/pre&gt;

&lt;/body&gt;
&lt;/html&gt;

 =======================================================================
 ==src/main/resources/hbase-webapps/master/master.jsp
 =======================================================================
&lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot;
  import=&quot;java.util.*&quot;
  import=&quot;org.apache.hadoop.conf.Configuration&quot;
  import=&quot;org.apache.hadoop.hbase.util.Bytes&quot;
  import=&quot;org.apache.hadoop.hbase.util.JvmVersion&quot;
  import=&quot;org.apache.hadoop.hbase.util.FSUtils&quot;
  import=&quot;org.apache.hadoop.hbase.master.HMaster&quot;
  import=&quot;org.apache.hadoop.hbase.HConstants&quot;
  import=&quot;org.apache.hadoop.hbase.master.MetaRegion&quot;
  import=&quot;org.apache.hadoop.hbase.client.HBaseAdmin&quot;
  import=&quot;org.apache.hadoop.hbase.HServerInfo&quot;
  import=&quot;org.apache.hadoop.hbase.HServerAddress&quot;
  import=&quot;org.apache.hadoop.hbase.HTableDescriptor&quot; %&gt;&lt;%
  HMaster master = (HMaster)getServletContext().getAttribute(HMaster.MASTER);
  Configuration conf = master.getConfiguration();
  HServerAddress rootLocation = master.getRegionManager().getRootRegionLocation();
  Map&lt;byte [], MetaRegion&gt; onlineRegions = master.getRegionManager().getOnlineMetaRegions();
  Map&lt;String, HServerInfo&gt; serverToServerInfos =
    master.getServerManager().getServersToServerInfo();
  int interval = conf.getInt(&quot;hbase.regionserver.msginterval&quot;, 1000)/1000;
  if (interval == 0) {
      interval = 1;
  }
  boolean showFragmentation = conf.getBoolean(&quot;hbase.master.ui.fragmentation.enabled&quot;, false);
  Map&lt;String, Integer&gt; frags = null;
  if (showFragmentation) {
      frags = master.getTableFragmentation();
  }
%&gt;&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;
&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD XHTML 1.0 Transitional//EN&quot; 
  &quot;http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd&quot;&gt; 
&lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot;&gt;
&lt;head&gt;&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html;charset=UTF-8&quot;/&gt;
&lt;title&gt;HBase Master: &lt;%= master.getMasterAddress().getHostname()%&gt;:&lt;%= master.getMasterAddress().getPort() %&gt;&lt;/title&gt;
&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;/static/hbase.css&quot; /&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;a id=&quot;logo&quot; href=&quot;http://wiki.apache.org/lucene-hadoop/Hbase&quot;&gt;&lt;img src=&quot;/static/hbase_logo_med.gif&quot; alt=&quot;HBase Logo&quot; title=&quot;HBase Logo&quot; /&gt;&lt;/a&gt;
&lt;h1 id=&quot;page_title&quot;&gt;Master: &lt;%=master.getMasterAddress().getHostname()%&gt;:&lt;%=master.getMasterAddress().getPort()%&gt;&lt;/h1&gt;
&lt;p id=&quot;links_menu&quot;&gt;&lt;a href=&quot;/logs/&quot;&gt;Local logs&lt;/a&gt;, &lt;a href=&quot;/stacks&quot;&gt;Thread Dump&lt;/a&gt;, &lt;a href=&quot;/logLevel&quot;&gt;Log Level&lt;/a&gt;&lt;/p&gt;

&lt;!-- Various warnings that cluster admins should be aware of --&gt;
&lt;% if (JvmVersion.isBadJvmVersion()) { %&gt;
  &lt;div class=&quot;warning&quot;&gt;
  Your current JVM version &lt;%= System.getProperty(&quot;java.version&quot;) %&gt; is known to be
  unstable with HBase. Please see the
  &lt;a href=&quot;http://wiki.apache.org/hadoop/Hbase/Troubleshooting#A18&quot;&gt;HBase wiki&lt;/a&gt;
  for details.
  &lt;/div&gt;
&lt;% } %&gt;

 =======================================================================
 ==src/main/resources/hbase-webapps/master/index.html
 =======================================================================
&lt;meta HTTP-EQUIV=&quot;REFRESH&quot; content=&quot;0;url=master.jsp&quot;/&gt;

 =======================================================================
 ==src/main/resources/org/apache/hadoop/hbase/mapred/RowCounter_Counters.properties
 =======================================================================

# ResourceBundle properties file for RowCounter MR job

CounterGroupName=         RowCounter

ROWS.name=                Rows

 =======================================================================
 ==src/main/resources/org/apache/hadoop/hbase/mapreduce/RowCounter_Counters.properties
 =======================================================================

# ResourceBundle properties file for RowCounter MR job

CounterGroupName=         RowCounter

ROWS.name=                Rows

 =======================================================================
 ==src/main/resources/org/apache/hadoop/hbase/rest/XMLSchema.xsd
 =======================================================================
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;schema targetNamespace=&quot;ModelSchema&quot; elementFormDefault=&quot;qualified&quot; xmlns=&quot;http://www.w3.org/2001/XMLSchema&quot; xmlns:tns=&quot;ModelSchema&quot;&gt;

    &lt;element name=&quot;Version&quot; type=&quot;tns:Version&quot;&gt;&lt;/element&gt;
    
    &lt;complexType name=&quot;Version&quot;&gt;
      &lt;attribute name=&quot;REST&quot; type=&quot;string&quot;&gt;&lt;/attribute&gt;
      &lt;attribute name=&quot;JVM&quot; type=&quot;string&quot;&gt;&lt;/attribute&gt;
      &lt;attribute name=&quot;OS&quot; type=&quot;string&quot;&gt;&lt;/attribute&gt;
      &lt;attribute name=&quot;Server&quot; type=&quot;string&quot;&gt;&lt;/attribute&gt;
      &lt;attribute name=&quot;Jersey&quot; type=&quot;string&quot;&gt;&lt;/attribute&gt;
    &lt;/complexType&gt;

    &lt;element name=&quot;TableList&quot; type=&quot;tns:TableList&quot;&gt;&lt;/element&gt;
    
    &lt;complexType name=&quot;TableList&quot;&gt;
        &lt;sequence&gt;
            &lt;element name=&quot;table&quot; type=&quot;tns:Table&quot; maxOccurs=&quot;unbounded&quot; minOccurs=&quot;1&quot;&gt;&lt;/element&gt;
        &lt;/sequence&gt;
    &lt;/complexType&gt;

    &lt;complexType name=&quot;Table&quot;&gt;
        &lt;sequence&gt;
            &lt;element name=&quot;name&quot; type=&quot;string&quot;&gt;&lt;/element&gt;
        &lt;/sequence&gt;
    &lt;/complexType&gt;

    &lt;element name=&quot;TableInfo&quot; type=&quot;tns:TableInfo&quot;&gt;&lt;/element&gt;
    
    &lt;complexType name=&quot;TableInfo&quot;&gt;
        &lt;sequence&gt;
            &lt;element name=&quot;region&quot; type=&quot;tns:TableRegion&quot; maxOccurs=&quot;unbounded&quot; minOccurs=&quot;1&quot;&gt;&lt;/element&gt;
        &lt;/sequence&gt;
        &lt;attribute name=&quot;name&quot; type=&quot;string&quot;&gt;&lt;/attribute&gt;
    &lt;/complexType&gt;

    &lt;complexType name=&quot;TableRegion&quot;&gt;
        &lt;attribute name=&quot;name&quot; type=&quot;string&quot;&gt;&lt;/attribute&gt;
        &lt;attribute name=&quot;id&quot; type=&quot;int&quot;&gt;&lt;/attribute&gt;
        &lt;attribute name=&quot;startKey&quot; type=&quot;base64Binary&quot;&gt;&lt;/attribute&gt;
        &lt;attribute name=&quot;endKey&quot; type=&quot;base64Binary&quot;&gt;&lt;/attribute&gt;
        &lt;attribute name=&quot;location&quot; type=&quot;string&quot;&gt;&lt;/attribute&gt;
    &lt;/complexType&gt;

    &lt;element name=&quot;TableSchema&quot; type=&quot;tns:TableSchema&quot;&gt;&lt;/element&gt;
    
    &lt;complexType name=&quot;TableSchema&quot;&gt;
        &lt;sequence&gt;
            &lt;element name=&quot;column&quot; type=&quot;tns:ColumnSchema&quot; maxOccurs=&quot;unbounded&quot; minOccurs=&quot;1&quot;&gt;&lt;/element&gt;
        &lt;/sequence&gt;

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/avro/generated/ACompressionAlgorithm.java
 =======================================================================
package org.apache.hadoop.hbase.avro.generated;

@SuppressWarnings(&quot;all&quot;)
public enum ACompressionAlgorithm { 
  LZO, GZ, NONE
}

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/avro/generated/ATimeRange.java
 =======================================================================
package org.apache.hadoop.hbase.avro.generated;

@SuppressWarnings(&quot;all&quot;)
public class ATimeRange extends org.apache.avro.specific.SpecificRecordBase implements org.apache.avro.specific.SpecificRecord {
  public static final org.apache.avro.Schema SCHEMA$ = org.apache.avro.Schema.parse(&quot;{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;ATimeRange\&quot;,\&quot;namespace\&quot;:\&quot;org.apache.hadoop.hbase.avro.generated\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;minStamp\&quot;,\&quot;type\&quot;:\&quot;long\&quot;},{\&quot;name\&quot;:\&quot;maxStamp\&quot;,\&quot;type\&quot;:\&quot;long\&quot;}]}&quot;);
  public long minStamp;
  public long maxStamp;
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return minStamp;
    case 1: return maxStamp;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
  @SuppressWarnings(value=&quot;unchecked&quot;)
  public void put(int field$, java.lang.Object value$) {
    switch (field$) {
    case 0: minStamp = (java.lang.Long)value$; break;
    case 1: maxStamp = (java.lang.Long)value$; break;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
}

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/avro/generated/AIOError.java
 =======================================================================
package org.apache.hadoop.hbase.avro.generated;

@SuppressWarnings(&quot;all&quot;)
public class AIOError extends org.apache.avro.specific.SpecificExceptionBase implements org.apache.avro.specific.SpecificRecord {
  public static final org.apache.avro.Schema SCHEMA$ = org.apache.avro.Schema.parse(&quot;{\&quot;type\&quot;:\&quot;error\&quot;,\&quot;name\&quot;:\&quot;AIOError\&quot;,\&quot;namespace\&quot;:\&quot;org.apache.hadoop.hbase.avro.generated\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;message\&quot;,\&quot;type\&quot;:\&quot;string\&quot;}]}&quot;);
  public org.apache.avro.util.Utf8 message;
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return message;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
  @SuppressWarnings(value=&quot;unchecked&quot;)
  public void put(int field$, java.lang.Object value$) {
    switch (field$) {
    case 0: message = (org.apache.avro.util.Utf8)value$; break;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
}

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/avro/generated/ARegionLoad.java
 =======================================================================
package org.apache.hadoop.hbase.avro.generated;

@SuppressWarnings(&quot;all&quot;)
public class ARegionLoad extends org.apache.avro.specific.SpecificRecordBase implements org.apache.avro.specific.SpecificRecord {
  public static final org.apache.avro.Schema SCHEMA$ = org.apache.avro.Schema.parse(&quot;{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;ARegionLoad\&quot;,\&quot;namespace\&quot;:\&quot;org.apache.hadoop.hbase.avro.generated\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;memStoreSizeMB\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;name\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;storefileIndexSizeMB\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;storefiles\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;storefileSizeMB\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;stores\&quot;,\&quot;type\&quot;:\&quot;int\&quot;}]}&quot;);
  public int memStoreSizeMB;
  public java.nio.ByteBuffer name;
  public int storefileIndexSizeMB;
  public int storefiles;
  public int storefileSizeMB;
  public int stores;
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return memStoreSizeMB;
    case 1: return name;
    case 2: return storefileIndexSizeMB;
    case 3: return storefiles;
    case 4: return storefileSizeMB;
    case 5: return stores;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
  @SuppressWarnings(value=&quot;unchecked&quot;)
  public void put(int field$, java.lang.Object value$) {
    switch (field$) {
    case 0: memStoreSizeMB = (java.lang.Integer)value$; break;
    case 1: name = (java.nio.ByteBuffer)value$; break;
    case 2: storefileIndexSizeMB = (java.lang.Integer)value$; break;
    case 3: storefiles = (java.lang.Integer)value$; break;
    case 4: storefileSizeMB = (java.lang.Integer)value$; break;
    case 5: stores = (java.lang.Integer)value$; break;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
}

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/avro/generated/AColumnFamilyDescriptor.java
 =======================================================================
package org.apache.hadoop.hbase.avro.generated;

@SuppressWarnings(&quot;all&quot;)
public class AColumnFamilyDescriptor extends org.apache.avro.specific.SpecificRecordBase implements org.apache.avro.specific.SpecificRecord {
  public static final org.apache.avro.Schema SCHEMA$ = org.apache.avro.Schema.parse(&quot;{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;AColumnFamilyDescriptor\&quot;,\&quot;namespace\&quot;:\&quot;org.apache.hadoop.hbase.avro.generated\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;name\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;compression\&quot;,\&quot;type\&quot;:{\&quot;type\&quot;:\&quot;enum\&quot;,\&quot;name\&quot;:\&quot;ACompressionAlgorithm\&quot;,\&quot;symbols\&quot;:[\&quot;LZO\&quot;,\&quot;GZ\&quot;,\&quot;NONE\&quot;]}},{\&quot;name\&quot;:\&quot;maxVersions\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;blocksize\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;inMemory\&quot;,\&quot;type\&quot;:\&quot;boolean\&quot;},{\&quot;name\&quot;:\&quot;timeToLive\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;blockCacheEnabled\&quot;,\&quot;type\&quot;:\&quot;boolean\&quot;},{\&quot;name\&quot;:\&quot;bloomfilterEnabled\&quot;,\&quot;type\&quot;:\&quot;boolean\&quot;}]}&quot;);
  public java.nio.ByteBuffer name;
  public org.apache.hadoop.hbase.avro.generated.ACompressionAlgorithm compression;
  public int maxVersions;
  public int blocksize;
  public boolean inMemory;
  public int timeToLive;
  public boolean blockCacheEnabled;
  public boolean bloomfilterEnabled;
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return name;
    case 1: return compression;
    case 2: return maxVersions;
    case 3: return blocksize;
    case 4: return inMemory;
    case 5: return timeToLive;
    case 6: return blockCacheEnabled;
    case 7: return bloomfilterEnabled;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
  @SuppressWarnings(value=&quot;unchecked&quot;)
  public void put(int field$, java.lang.Object value$) {
    switch (field$) {
    case 0: name = (java.nio.ByteBuffer)value$; break;
    case 1: compression = (org.apache.hadoop.hbase.avro.generated.ACompressionAlgorithm)value$; break;
    case 2: maxVersions = (java.lang.Integer)value$; break;
    case 3: blocksize = (java.lang.Integer)value$; break;
    case 4: inMemory = (java.lang.Boolean)value$; break;
    case 5: timeToLive = (java.lang.Integer)value$; break;
    case 6: blockCacheEnabled = (java.lang.Boolean)value$; break;
    case 7: bloomfilterEnabled = (java.lang.Boolean)value$; break;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
}

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/avro/generated/AClusterStatus.java
 =======================================================================
package org.apache.hadoop.hbase.avro.generated;

@SuppressWarnings(&quot;all&quot;)
public class AClusterStatus extends org.apache.avro.specific.SpecificRecordBase implements org.apache.avro.specific.SpecificRecord {
  public static final org.apache.avro.Schema SCHEMA$ = org.apache.avro.Schema.parse(&quot;{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;AClusterStatus\&quot;,\&quot;namespace\&quot;:\&quot;org.apache.hadoop.hbase.avro.generated\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;averageLoad\&quot;,\&quot;type\&quot;:\&quot;double\&quot;},{\&quot;name\&quot;:\&quot;deadServerNames\&quot;,\&quot;type\&quot;:{\&quot;type\&quot;:\&quot;array\&quot;,\&quot;items\&quot;:\&quot;string\&quot;}},{\&quot;name\&quot;:\&quot;deadServers\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;hbaseVersion\&quot;,\&quot;type\&quot;:\&quot;string\&quot;},{\&quot;name\&quot;:\&quot;regionsCount\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;requestsCount\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;serverInfos\&quot;,\&quot;type\&quot;:{\&quot;type\&quot;:\&quot;array\&quot;,\&quot;items\&quot;:{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;AServerInfo\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;infoPort\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;load\&quot;,\&quot;type\&quot;:{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;AServerLoad\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;load\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;maxHeapMB\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;memStoreSizeInMB\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;numberOfRegions\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;numberOfRequests\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;regionsLoad\&quot;,\&quot;type\&quot;:{\&quot;type\&quot;:\&quot;array\&quot;,\&quot;items\&quot;:{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;ARegionLoad\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;memStoreSizeMB\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;name\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;storefileIndexSizeMB\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;storefiles\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;storefileSizeMB\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;stores\&quot;,\&quot;type\&quot;:\&quot;int\&quot;}]}}},{\&quot;name\&quot;:\&quot;storefileIndexSizeInMB\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;storefiles\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;storefileSizeInMB\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;usedHeapMB\&quot;,\&quot;type\&quot;:\&quot;int\&quot;}]}},{\&quot;name\&quot;:\&quot;serverAddress\&quot;,\&quot;type\&quot;:{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;AServerAddress\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;hostname\&quot;,\&quot;type\&quot;:\&quot;string\&quot;},{\&quot;name\&quot;:\&quot;inetSocketAddress\&quot;,\&quot;type\&quot;:\&quot;string\&quot;},{\&quot;name\&quot;:\&quot;port\&quot;,\&quot;type\&quot;:\&quot;int\&quot;}]}},{\&quot;name\&quot;:\&quot;serverName\&quot;,\&quot;type\&quot;:\&quot;string\&quot;},{\&quot;name\&quot;:\&quot;startCode\&quot;,\&quot;type\&quot;:\&quot;long\&quot;}]}}},{\&quot;name\&quot;:\&quot;servers\&quot;,\&quot;type\&quot;:\&quot;int\&quot;}]}&quot;);
  public double averageLoad;
  public org.apache.avro.generic.GenericArray&lt;org.apache.avro.util.Utf8&gt; deadServerNames;
  public int deadServers;
  public org.apache.avro.util.Utf8 hbaseVersion;
  public int regionsCount;
  public int requestsCount;
  public org.apache.avro.generic.GenericArray&lt;org.apache.hadoop.hbase.avro.generated.AServerInfo&gt; serverInfos;
  public int servers;
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return averageLoad;
    case 1: return deadServerNames;
    case 2: return deadServers;
    case 3: return hbaseVersion;
    case 4: return regionsCount;
    case 5: return requestsCount;
    case 6: return serverInfos;
    case 7: return servers;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
  @SuppressWarnings(value=&quot;unchecked&quot;)
  public void put(int field$, java.lang.Object value$) {
    switch (field$) {
    case 0: averageLoad = (java.lang.Double)value$; break;
    case 1: deadServerNames = (org.apache.avro.generic.GenericArray&lt;org.apache.avro.util.Utf8&gt;)value$; break;
    case 2: deadServers = (java.lang.Integer)value$; break;
    case 3: hbaseVersion = (org.apache.avro.util.Utf8)value$; break;
    case 4: regionsCount = (java.lang.Integer)value$; break;
    case 5: requestsCount = (java.lang.Integer)value$; break;
    case 6: serverInfos = (org.apache.avro.generic.GenericArray&lt;org.apache.hadoop.hbase.avro.generated.AServerInfo&gt;)value$; break;
    case 7: servers = (java.lang.Integer)value$; break;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
}

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/avro/generated/ATableDescriptor.java
 =======================================================================
package org.apache.hadoop.hbase.avro.generated;

@SuppressWarnings(&quot;all&quot;)
public class ATableDescriptor extends org.apache.avro.specific.SpecificRecordBase implements org.apache.avro.specific.SpecificRecord {
  public static final org.apache.avro.Schema SCHEMA$ = org.apache.avro.Schema.parse(&quot;{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;ATableDescriptor\&quot;,\&quot;namespace\&quot;:\&quot;org.apache.hadoop.hbase.avro.generated\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;name\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;families\&quot;,\&quot;type\&quot;:[{\&quot;type\&quot;:\&quot;array\&quot;,\&quot;items\&quot;:{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;AFamilyDescriptor\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;name\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;compression\&quot;,\&quot;type\&quot;:[{\&quot;type\&quot;:\&quot;enum\&quot;,\&quot;name\&quot;:\&quot;ACompressionAlgorithm\&quot;,\&quot;symbols\&quot;:[\&quot;LZO\&quot;,\&quot;GZ\&quot;,\&quot;NONE\&quot;]},\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;maxVersions\&quot;,\&quot;type\&quot;:[\&quot;int\&quot;,\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;blocksize\&quot;,\&quot;type\&quot;:[\&quot;int\&quot;,\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;inMemory\&quot;,\&quot;type\&quot;:[\&quot;boolean\&quot;,\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;timeToLive\&quot;,\&quot;type\&quot;:[\&quot;int\&quot;,\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;blockCacheEnabled\&quot;,\&quot;type\&quot;:[\&quot;boolean\&quot;,\&quot;null\&quot;]}]}},\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;maxFileSize\&quot;,\&quot;type\&quot;:[\&quot;long\&quot;,\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;memStoreFlushSize\&quot;,\&quot;type\&quot;:[\&quot;long\&quot;,\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;rootRegion\&quot;,\&quot;type\&quot;:[\&quot;boolean\&quot;,\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;metaRegion\&quot;,\&quot;type\&quot;:[\&quot;boolean\&quot;,\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;metaTable\&quot;,\&quot;type\&quot;:[\&quot;boolean\&quot;,\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;readOnly\&quot;,\&quot;type\&quot;:[\&quot;boolean\&quot;,\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;deferredLogFlush\&quot;,\&quot;type\&quot;:[\&quot;boolean\&quot;,\&quot;null\&quot;]}]}&quot;);
  public java.nio.ByteBuffer name;
  public org.apache.avro.generic.GenericArray&lt;org.apache.hadoop.hbase.avro.generated.AFamilyDescriptor&gt; families;
  public java.lang.Long maxFileSize;
  public java.lang.Long memStoreFlushSize;
  public java.lang.Boolean rootRegion;
  public java.lang.Boolean metaRegion;
  public java.lang.Boolean metaTable;
  public java.lang.Boolean readOnly;
  public java.lang.Boolean deferredLogFlush;
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return name;
    case 1: return families;
    case 2: return maxFileSize;
    case 3: return memStoreFlushSize;
    case 4: return rootRegion;
    case 5: return metaRegion;
    case 6: return metaTable;
    case 7: return readOnly;
    case 8: return deferredLogFlush;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
  @SuppressWarnings(value=&quot;unchecked&quot;)
  public void put(int field$, java.lang.Object value$) {
    switch (field$) {
    case 0: name = (java.nio.ByteBuffer)value$; break;
    case 1: families = (org.apache.avro.generic.GenericArray&lt;org.apache.hadoop.hbase.avro.generated.AFamilyDescriptor&gt;)value$; break;
    case 2: maxFileSize = (java.lang.Long)value$; break;
    case 3: memStoreFlushSize = (java.lang.Long)value$; break;
    case 4: rootRegion = (java.lang.Boolean)value$; break;
    case 5: metaRegion = (java.lang.Boolean)value$; break;
    case 6: metaTable = (java.lang.Boolean)value$; break;
    case 7: readOnly = (java.lang.Boolean)value$; break;
    case 8: deferredLogFlush = (java.lang.Boolean)value$; break;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
}

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/avro/generated/AFamilyDescriptor.java
 =======================================================================
package org.apache.hadoop.hbase.avro.generated;

@SuppressWarnings(&quot;all&quot;)
public class AFamilyDescriptor extends org.apache.avro.specific.SpecificRecordBase implements org.apache.avro.specific.SpecificRecord {
  public static final org.apache.avro.Schema SCHEMA$ = org.apache.avro.Schema.parse(&quot;{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;AFamilyDescriptor\&quot;,\&quot;namespace\&quot;:\&quot;org.apache.hadoop.hbase.avro.generated\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;name\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;compression\&quot;,\&quot;type\&quot;:[{\&quot;type\&quot;:\&quot;enum\&quot;,\&quot;name\&quot;:\&quot;ACompressionAlgorithm\&quot;,\&quot;symbols\&quot;:[\&quot;LZO\&quot;,\&quot;GZ\&quot;,\&quot;NONE\&quot;]},\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;maxVersions\&quot;,\&quot;type\&quot;:[\&quot;int\&quot;,\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;blocksize\&quot;,\&quot;type\&quot;:[\&quot;int\&quot;,\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;inMemory\&quot;,\&quot;type\&quot;:[\&quot;boolean\&quot;,\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;timeToLive\&quot;,\&quot;type\&quot;:[\&quot;int\&quot;,\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;blockCacheEnabled\&quot;,\&quot;type\&quot;:[\&quot;boolean\&quot;,\&quot;null\&quot;]}]}&quot;);
  public java.nio.ByteBuffer name;
  public org.apache.hadoop.hbase.avro.generated.ACompressionAlgorithm compression;
  public java.lang.Integer maxVersions;
  public java.lang.Integer blocksize;
  public java.lang.Boolean inMemory;
  public java.lang.Integer timeToLive;
  public java.lang.Boolean blockCacheEnabled;
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return name;
    case 1: return compression;
    case 2: return maxVersions;
    case 3: return blocksize;
    case 4: return inMemory;
    case 5: return timeToLive;
    case 6: return blockCacheEnabled;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
  @SuppressWarnings(value=&quot;unchecked&quot;)
  public void put(int field$, java.lang.Object value$) {
    switch (field$) {
    case 0: name = (java.nio.ByteBuffer)value$; break;
    case 1: compression = (org.apache.hadoop.hbase.avro.generated.ACompressionAlgorithm)value$; break;
    case 2: maxVersions = (java.lang.Integer)value$; break;
    case 3: blocksize = (java.lang.Integer)value$; break;
    case 4: inMemory = (java.lang.Boolean)value$; break;
    case 5: timeToLive = (java.lang.Integer)value$; break;
    case 6: blockCacheEnabled = (java.lang.Boolean)value$; break;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
}

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/avro/generated/ATableExists.java
 =======================================================================
package org.apache.hadoop.hbase.avro.generated;

@SuppressWarnings(&quot;all&quot;)
public class ATableExists extends org.apache.avro.specific.SpecificExceptionBase implements org.apache.avro.specific.SpecificRecord {
  public static final org.apache.avro.Schema SCHEMA$ = org.apache.avro.Schema.parse(&quot;{\&quot;type\&quot;:\&quot;error\&quot;,\&quot;name\&quot;:\&quot;ATableExists\&quot;,\&quot;namespace\&quot;:\&quot;org.apache.hadoop.hbase.avro.generated\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;message\&quot;,\&quot;type\&quot;:\&quot;string\&quot;}]}&quot;);
  public org.apache.avro.util.Utf8 message;
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return message;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
  @SuppressWarnings(value=&quot;unchecked&quot;)
  public void put(int field$, java.lang.Object value$) {
    switch (field$) {
    case 0: message = (org.apache.avro.util.Utf8)value$; break;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
}

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/avro/generated/AResult.java
 =======================================================================
package org.apache.hadoop.hbase.avro.generated;

@SuppressWarnings(&quot;all&quot;)
public class AResult extends org.apache.avro.specific.SpecificRecordBase implements org.apache.avro.specific.SpecificRecord {
  public static final org.apache.avro.Schema SCHEMA$ = org.apache.avro.Schema.parse(&quot;{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;AResult\&quot;,\&quot;namespace\&quot;:\&quot;org.apache.hadoop.hbase.avro.generated\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;row\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;entries\&quot;,\&quot;type\&quot;:{\&quot;type\&quot;:\&quot;array\&quot;,\&quot;items\&quot;:{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;AResultEntry\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;family\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;qualifier\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;value\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;timestamp\&quot;,\&quot;type\&quot;:\&quot;long\&quot;}]}}}]}&quot;);
  public java.nio.ByteBuffer row;
  public org.apache.avro.generic.GenericArray&lt;org.apache.hadoop.hbase.avro.generated.AResultEntry&gt; entries;
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return row;
    case 1: return entries;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
  @SuppressWarnings(value=&quot;unchecked&quot;)
  public void put(int field$, java.lang.Object value$) {
    switch (field$) {
    case 0: row = (java.nio.ByteBuffer)value$; break;
    case 1: entries = (org.apache.avro.generic.GenericArray&lt;org.apache.hadoop.hbase.avro.generated.AResultEntry&gt;)value$; break;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
}

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/avro/generated/AColumn.java
 =======================================================================
package org.apache.hadoop.hbase.avro.generated;

@SuppressWarnings(&quot;all&quot;)
public class AColumn extends org.apache.avro.specific.SpecificRecordBase implements org.apache.avro.specific.SpecificRecord {
  public static final org.apache.avro.Schema SCHEMA$ = org.apache.avro.Schema.parse(&quot;{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;AColumn\&quot;,\&quot;namespace\&quot;:\&quot;org.apache.hadoop.hbase.avro.generated\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;family\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;qualifier\&quot;,\&quot;type\&quot;:[\&quot;bytes\&quot;,\&quot;null\&quot;]}]}&quot;);
  public java.nio.ByteBuffer family;
  public java.nio.ByteBuffer qualifier;
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return family;
    case 1: return qualifier;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
  @SuppressWarnings(value=&quot;unchecked&quot;)
  public void put(int field$, java.lang.Object value$) {
    switch (field$) {
    case 0: family = (java.nio.ByteBuffer)value$; break;
    case 1: qualifier = (java.nio.ByteBuffer)value$; break;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
}

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/avro/generated/APut.java
 =======================================================================
package org.apache.hadoop.hbase.avro.generated;

@SuppressWarnings(&quot;all&quot;)
public class APut extends org.apache.avro.specific.SpecificRecordBase implements org.apache.avro.specific.SpecificRecord {
  public static final org.apache.avro.Schema SCHEMA$ = org.apache.avro.Schema.parse(&quot;{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;APut\&quot;,\&quot;namespace\&quot;:\&quot;org.apache.hadoop.hbase.avro.generated\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;row\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;columnValues\&quot;,\&quot;type\&quot;:{\&quot;type\&quot;:\&quot;array\&quot;,\&quot;items\&quot;:{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;AColumnValue\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;family\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;qualifier\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;value\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;timestamp\&quot;,\&quot;type\&quot;:[\&quot;long\&quot;,\&quot;null\&quot;]}]}}}]}&quot;);
  public java.nio.ByteBuffer row;
  public org.apache.avro.generic.GenericArray&lt;org.apache.hadoop.hbase.avro.generated.AColumnValue&gt; columnValues;
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return row;
    case 1: return columnValues;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
  @SuppressWarnings(value=&quot;unchecked&quot;)
  public void put(int field$, java.lang.Object value$) {
    switch (field$) {
    case 0: row = (java.nio.ByteBuffer)value$; break;
    case 1: columnValues = (org.apache.avro.generic.GenericArray&lt;org.apache.hadoop.hbase.avro.generated.AColumnValue&gt;)value$; break;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
}

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/avro/generated/AGet.java
 =======================================================================
package org.apache.hadoop.hbase.avro.generated;

@SuppressWarnings(&quot;all&quot;)
public class AGet extends org.apache.avro.specific.SpecificRecordBase implements org.apache.avro.specific.SpecificRecord {
  public static final org.apache.avro.Schema SCHEMA$ = org.apache.avro.Schema.parse(&quot;{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;AGet\&quot;,\&quot;namespace\&quot;:\&quot;org.apache.hadoop.hbase.avro.generated\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;row\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;columns\&quot;,\&quot;type\&quot;:[{\&quot;type\&quot;:\&quot;array\&quot;,\&quot;items\&quot;:{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;AColumn\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;family\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;qualifier\&quot;,\&quot;type\&quot;:[\&quot;bytes\&quot;,\&quot;null\&quot;]}]}},\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;timestamp\&quot;,\&quot;type\&quot;:[\&quot;long\&quot;,\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;timerange\&quot;,\&quot;type\&quot;:[{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;ATimeRange\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;minStamp\&quot;,\&quot;type\&quot;:\&quot;long\&quot;},{\&quot;name\&quot;:\&quot;maxStamp\&quot;,\&quot;type\&quot;:\&quot;long\&quot;}]},\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;maxVersions\&quot;,\&quot;type\&quot;:[\&quot;int\&quot;,\&quot;null\&quot;]}]}&quot;);
  public java.nio.ByteBuffer row;
  public org.apache.avro.generic.GenericArray&lt;org.apache.hadoop.hbase.avro.generated.AColumn&gt; columns;
  public java.lang.Long timestamp;
  public org.apache.hadoop.hbase.avro.generated.ATimeRange timerange;
  public java.lang.Integer maxVersions;
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return row;
    case 1: return columns;
    case 2: return timestamp;
    case 3: return timerange;
    case 4: return maxVersions;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
  @SuppressWarnings(value=&quot;unchecked&quot;)
  public void put(int field$, java.lang.Object value$) {
    switch (field$) {
    case 0: row = (java.nio.ByteBuffer)value$; break;
    case 1: columns = (org.apache.avro.generic.GenericArray&lt;org.apache.hadoop.hbase.avro.generated.AColumn&gt;)value$; break;
    case 2: timestamp = (java.lang.Long)value$; break;
    case 3: timerange = (org.apache.hadoop.hbase.avro.generated.ATimeRange)value$; break;
    case 4: maxVersions = (java.lang.Integer)value$; break;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
}

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/avro/generated/AServerAddress.java
 =======================================================================
package org.apache.hadoop.hbase.avro.generated;

@SuppressWarnings(&quot;all&quot;)
public class AServerAddress extends org.apache.avro.specific.SpecificRecordBase implements org.apache.avro.specific.SpecificRecord {
  public static final org.apache.avro.Schema SCHEMA$ = org.apache.avro.Schema.parse(&quot;{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;AServerAddress\&quot;,\&quot;namespace\&quot;:\&quot;org.apache.hadoop.hbase.avro.generated\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;hostname\&quot;,\&quot;type\&quot;:\&quot;string\&quot;},{\&quot;name\&quot;:\&quot;inetSocketAddress\&quot;,\&quot;type\&quot;:\&quot;string\&quot;},{\&quot;name\&quot;:\&quot;port\&quot;,\&quot;type\&quot;:\&quot;int\&quot;}]}&quot;);
  public org.apache.avro.util.Utf8 hostname;
  public org.apache.avro.util.Utf8 inetSocketAddress;
  public int port;
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return hostname;
    case 1: return inetSocketAddress;
    case 2: return port;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
  @SuppressWarnings(value=&quot;unchecked&quot;)
  public void put(int field$, java.lang.Object value$) {
    switch (field$) {
    case 0: hostname = (org.apache.avro.util.Utf8)value$; break;
    case 1: inetSocketAddress = (org.apache.avro.util.Utf8)value$; break;
    case 2: port = (java.lang.Integer)value$; break;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
}

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/avro/generated/AServerLoad.java
 =======================================================================
package org.apache.hadoop.hbase.avro.generated;

@SuppressWarnings(&quot;all&quot;)
public class AServerLoad extends org.apache.avro.specific.SpecificRecordBase implements org.apache.avro.specific.SpecificRecord {
  public static final org.apache.avro.Schema SCHEMA$ = org.apache.avro.Schema.parse(&quot;{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;AServerLoad\&quot;,\&quot;namespace\&quot;:\&quot;org.apache.hadoop.hbase.avro.generated\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;load\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;maxHeapMB\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;memStoreSizeInMB\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;numberOfRegions\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;numberOfRequests\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;regionsLoad\&quot;,\&quot;type\&quot;:{\&quot;type\&quot;:\&quot;array\&quot;,\&quot;items\&quot;:{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;ARegionLoad\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;memStoreSizeMB\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;name\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;storefileIndexSizeMB\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;storefiles\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;storefileSizeMB\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;stores\&quot;,\&quot;type\&quot;:\&quot;int\&quot;}]}}},{\&quot;name\&quot;:\&quot;storefileIndexSizeInMB\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;storefiles\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;storefileSizeInMB\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;usedHeapMB\&quot;,\&quot;type\&quot;:\&quot;int\&quot;}]}&quot;);
  public int load;
  public int maxHeapMB;
  public int memStoreSizeInMB;
  public int numberOfRegions;
  public int numberOfRequests;
  public org.apache.avro.generic.GenericArray&lt;org.apache.hadoop.hbase.avro.generated.ARegionLoad&gt; regionsLoad;
  public int storefileIndexSizeInMB;
  public int storefiles;
  public int storefileSizeInMB;
  public int usedHeapMB;
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return load;
    case 1: return maxHeapMB;
    case 2: return memStoreSizeInMB;
    case 3: return numberOfRegions;
    case 4: return numberOfRequests;
    case 5: return regionsLoad;
    case 6: return storefileIndexSizeInMB;
    case 7: return storefiles;
    case 8: return storefileSizeInMB;
    case 9: return usedHeapMB;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
  @SuppressWarnings(value=&quot;unchecked&quot;)
  public void put(int field$, java.lang.Object value$) {
    switch (field$) {
    case 0: load = (java.lang.Integer)value$; break;
    case 1: maxHeapMB = (java.lang.Integer)value$; break;
    case 2: memStoreSizeInMB = (java.lang.Integer)value$; break;
    case 3: numberOfRegions = (java.lang.Integer)value$; break;
    case 4: numberOfRequests = (java.lang.Integer)value$; break;
    case 5: regionsLoad = (org.apache.avro.generic.GenericArray&lt;org.apache.hadoop.hbase.avro.generated.ARegionLoad&gt;)value$; break;
    case 6: storefileIndexSizeInMB = (java.lang.Integer)value$; break;
    case 7: storefiles = (java.lang.Integer)value$; break;
    case 8: storefileSizeInMB = (java.lang.Integer)value$; break;
    case 9: usedHeapMB = (java.lang.Integer)value$; break;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
}

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/avro/generated/HBase.java
 =======================================================================
package org.apache.hadoop.hbase.avro.generated;

@SuppressWarnings(&quot;all&quot;)
public interface HBase {
  public static final org.apache.avro.Protocol PROTOCOL = org.apache.avro.Protocol.parse(&quot;{\&quot;protocol\&quot;:\&quot;HBase\&quot;,\&quot;namespace\&quot;:\&quot;org.apache.hadoop.hbase.avro.generated\&quot;,\&quot;types\&quot;:[{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;AServerAddress\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;hostname\&quot;,\&quot;type\&quot;:\&quot;string\&quot;},{\&quot;name\&quot;:\&quot;inetSocketAddress\&quot;,\&quot;type\&quot;:\&quot;string\&quot;},{\&quot;name\&quot;:\&quot;port\&quot;,\&quot;type\&quot;:\&quot;int\&quot;}]},{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;ARegionLoad\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;memStoreSizeMB\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;name\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;storefileIndexSizeMB\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;storefiles\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;storefileSizeMB\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;stores\&quot;,\&quot;type\&quot;:\&quot;int\&quot;}]},{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;AServerLoad\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;load\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;maxHeapMB\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;memStoreSizeInMB\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;numberOfRegions\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;numberOfRequests\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;regionsLoad\&quot;,\&quot;type\&quot;:{\&quot;type\&quot;:\&quot;array\&quot;,\&quot;items\&quot;:\&quot;ARegionLoad\&quot;}},{\&quot;name\&quot;:\&quot;storefileIndexSizeInMB\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;storefiles\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;storefileSizeInMB\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;usedHeapMB\&quot;,\&quot;type\&quot;:\&quot;int\&quot;}]},{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;AServerInfo\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;infoPort\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;load\&quot;,\&quot;type\&quot;:\&quot;AServerLoad\&quot;},{\&quot;name\&quot;:\&quot;serverAddress\&quot;,\&quot;type\&quot;:\&quot;AServerAddress\&quot;},{\&quot;name\&quot;:\&quot;serverName\&quot;,\&quot;type\&quot;:\&quot;string\&quot;},{\&quot;name\&quot;:\&quot;startCode\&quot;,\&quot;type\&quot;:\&quot;long\&quot;}]},{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;AClusterStatus\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;averageLoad\&quot;,\&quot;type\&quot;:\&quot;double\&quot;},{\&quot;name\&quot;:\&quot;deadServerNames\&quot;,\&quot;type\&quot;:{\&quot;type\&quot;:\&quot;array\&quot;,\&quot;items\&quot;:\&quot;string\&quot;}},{\&quot;name\&quot;:\&quot;deadServers\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;hbaseVersion\&quot;,\&quot;type\&quot;:\&quot;string\&quot;},{\&quot;name\&quot;:\&quot;regionsCount\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;requestsCount\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;serverInfos\&quot;,\&quot;type\&quot;:{\&quot;type\&quot;:\&quot;array\&quot;,\&quot;items\&quot;:\&quot;AServerInfo\&quot;}},{\&quot;name\&quot;:\&quot;servers\&quot;,\&quot;type\&quot;:\&quot;int\&quot;}]},{\&quot;type\&quot;:\&quot;enum\&quot;,\&quot;name\&quot;:\&quot;ACompressionAlgorithm\&quot;,\&quot;symbols\&quot;:[\&quot;LZO\&quot;,\&quot;GZ\&quot;,\&quot;NONE\&quot;]},{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;AFamilyDescriptor\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;name\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;compression\&quot;,\&quot;type\&quot;:[\&quot;ACompressionAlgorithm\&quot;,\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;maxVersions\&quot;,\&quot;type\&quot;:[\&quot;int\&quot;,\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;blocksize\&quot;,\&quot;type\&quot;:[\&quot;int\&quot;,\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;inMemory\&quot;,\&quot;type\&quot;:[\&quot;boolean\&quot;,\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;timeToLive\&quot;,\&quot;type\&quot;:[\&quot;int\&quot;,\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;blockCacheEnabled\&quot;,\&quot;type\&quot;:[\&quot;boolean\&quot;,\&quot;null\&quot;]}]},{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;ATableDescriptor\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;name\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;families\&quot;,\&quot;type\&quot;:[{\&quot;type\&quot;:\&quot;array\&quot;,\&quot;items\&quot;:\&quot;AFamilyDescriptor\&quot;},\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;maxFileSize\&quot;,\&quot;type\&quot;:[\&quot;long\&quot;,\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;memStoreFlushSize\&quot;,\&quot;type\&quot;:[\&quot;long\&quot;,\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;rootRegion\&quot;,\&quot;type\&quot;:[\&quot;boolean\&quot;,\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;metaRegion\&quot;,\&quot;type\&quot;:[\&quot;boolean\&quot;,\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;metaTable\&quot;,\&quot;type\&quot;:[\&quot;boolean\&quot;,\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;readOnly\&quot;,\&quot;type\&quot;:[\&quot;boolean\&quot;,\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;deferredLogFlush\&quot;,\&quot;type\&quot;:[\&quot;boolean\&quot;,\&quot;null\&quot;]}]},{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;AColumn\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;family\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;qualifier\&quot;,\&quot;type\&quot;:[\&quot;bytes\&quot;,\&quot;null\&quot;]}]},{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;ATimeRange\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;minStamp\&quot;,\&quot;type\&quot;:\&quot;long\&quot;},{\&quot;name\&quot;:\&quot;maxStamp\&quot;,\&quot;type\&quot;:\&quot;long\&quot;}]},{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;AGet\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;row\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;columns\&quot;,\&quot;type\&quot;:[{\&quot;type\&quot;:\&quot;array\&quot;,\&quot;items\&quot;:\&quot;AColumn\&quot;},\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;timestamp\&quot;,\&quot;type\&quot;:[\&quot;long\&quot;,\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;timerange\&quot;,\&quot;type\&quot;:[\&quot;ATimeRange\&quot;,\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;maxVersions\&quot;,\&quot;type\&quot;:[\&quot;int\&quot;,\&quot;null\&quot;]}]},{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;AResultEntry\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;family\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;qualifier\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;value\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;timestamp\&quot;,\&quot;type\&quot;:\&quot;long\&quot;}]},{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;AResult\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;row\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;entries\&quot;,\&quot;type\&quot;:{\&quot;type\&quot;:\&quot;array\&quot;,\&quot;items\&quot;:\&quot;AResultEntry\&quot;}}]},{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;AColumnValue\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;family\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;qualifier\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;value\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;timestamp\&quot;,\&quot;type\&quot;:[\&quot;long\&quot;,\&quot;null\&quot;]}]},{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;APut\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;row\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;columnValues\&quot;,\&quot;type\&quot;:{\&quot;type\&quot;:\&quot;array\&quot;,\&quot;items\&quot;:\&quot;AColumnValue\&quot;}}]},{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;ADelete\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;row\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;columns\&quot;,\&quot;type\&quot;:[{\&quot;type\&quot;:\&quot;array\&quot;,\&quot;items\&quot;:\&quot;AColumn\&quot;},\&quot;null\&quot;]}]},{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;AScan\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;startRow\&quot;,\&quot;type\&quot;:[\&quot;bytes\&quot;,\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;stopRow\&quot;,\&quot;type\&quot;:[\&quot;bytes\&quot;,\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;columns\&quot;,\&quot;type\&quot;:[{\&quot;type\&quot;:\&quot;array\&quot;,\&quot;items\&quot;:\&quot;AColumn\&quot;},\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;timestamp\&quot;,\&quot;type\&quot;:[\&quot;long\&quot;,\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;timerange\&quot;,\&quot;type\&quot;:[\&quot;ATimeRange\&quot;,\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;maxVersions\&quot;,\&quot;type\&quot;:[\&quot;int\&quot;,\&quot;null\&quot;]}]},{\&quot;type\&quot;:\&quot;error\&quot;,\&quot;name\&quot;:\&quot;AIOError\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;message\&quot;,\&quot;type\&quot;:\&quot;string\&quot;}]},{\&quot;type\&quot;:\&quot;error\&quot;,\&quot;name\&quot;:\&quot;AIllegalArgument\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;message\&quot;,\&quot;type\&quot;:\&quot;string\&quot;}]},{\&quot;type\&quot;:\&quot;error\&quot;,\&quot;name\&quot;:\&quot;ATableExists\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;message\&quot;,\&quot;type\&quot;:\&quot;string\&quot;}]},{\&quot;type\&quot;:\&quot;error\&quot;,\&quot;name\&quot;:\&quot;AMasterNotRunning\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;message\&quot;,\&quot;type\&quot;:\&quot;string\&quot;}]}],\&quot;messages\&quot;:{\&quot;getHBaseVersion\&quot;:{\&quot;request\&quot;:[],\&quot;response\&quot;:\&quot;string\&quot;,\&quot;errors\&quot;:[\&quot;AIOError\&quot;]},\&quot;getClusterStatus\&quot;:{\&quot;request\&quot;:[],\&quot;response\&quot;:\&quot;AClusterStatus\&quot;,\&quot;errors\&quot;:[\&quot;AIOError\&quot;]},\&quot;listTables\&quot;:{\&quot;request\&quot;:[],\&quot;response\&quot;:{\&quot;type\&quot;:\&quot;array\&quot;,\&quot;items\&quot;:\&quot;ATableDescriptor\&quot;},\&quot;errors\&quot;:[\&quot;AIOError\&quot;]},\&quot;describeTable\&quot;:{\&quot;request\&quot;:[{\&quot;name\&quot;:\&quot;table\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;}],\&quot;response\&quot;:\&quot;ATableDescriptor\&quot;,\&quot;errors\&quot;:[\&quot;AIOError\&quot;]},\&quot;isTableEnabled\&quot;:{\&quot;request\&quot;:[{\&quot;name\&quot;:\&quot;table\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;}],\&quot;response\&quot;:\&quot;boolean\&quot;,\&quot;errors\&quot;:[\&quot;AIOError\&quot;]},\&quot;tableExists\&quot;:{\&quot;request\&quot;:[{\&quot;name\&quot;:\&quot;table\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;}],\&quot;response\&quot;:\&quot;boolean\&quot;,\&quot;errors\&quot;:[\&quot;AIOError\&quot;]},\&quot;describeFamily\&quot;:{\&quot;request\&quot;:[{\&quot;name\&quot;:\&quot;table\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;family\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;}],\&quot;response\&quot;:\&quot;AFamilyDescriptor\&quot;,\&quot;errors\&quot;:[\&quot;AIOError\&quot;]},\&quot;createTable\&quot;:{\&quot;request\&quot;:[{\&quot;name\&quot;:\&quot;table\&quot;,\&quot;type\&quot;:\&quot;ATableDescriptor\&quot;}],\&quot;response\&quot;:\&quot;null\&quot;,\&quot;errors\&quot;:[\&quot;AIOError\&quot;,\&quot;AIllegalArgument\&quot;,\&quot;ATableExists\&quot;,\&quot;AMasterNotRunning\&quot;]},\&quot;deleteTable\&quot;:{\&quot;request\&quot;:[{\&quot;name\&quot;:\&quot;table\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;}],\&quot;response\&quot;:\&quot;null\&quot;,\&quot;errors\&quot;:[\&quot;AIOError\&quot;]},\&quot;modifyTable\&quot;:{\&quot;request\&quot;:[{\&quot;name\&quot;:\&quot;table\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;tableDescriptor\&quot;,\&quot;type\&quot;:\&quot;ATableDescriptor\&quot;}],\&quot;response\&quot;:\&quot;null\&quot;,\&quot;errors\&quot;:[\&quot;AIOError\&quot;]},\&quot;enableTable\&quot;:{\&quot;request\&quot;:[{\&quot;name\&quot;:\&quot;table\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;}],\&quot;response\&quot;:\&quot;null\&quot;,\&quot;errors\&quot;:[\&quot;AIOError\&quot;]},\&quot;disableTable\&quot;:{\&quot;request\&quot;:[{\&quot;name\&quot;:\&quot;table\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;}],\&quot;response\&quot;:\&quot;null\&quot;,\&quot;errors\&quot;:[\&quot;AIOError\&quot;]},\&quot;flush\&quot;:{\&quot;request\&quot;:[{\&quot;name\&quot;:\&quot;table\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;}],\&quot;response\&quot;:\&quot;null\&quot;,\&quot;errors\&quot;:[\&quot;AIOError\&quot;]},\&quot;split\&quot;:{\&quot;request\&quot;:[{\&quot;name\&quot;:\&quot;table\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;}],\&quot;response\&quot;:\&quot;null\&quot;,\&quot;errors\&quot;:[\&quot;AIOError\&quot;]},\&quot;addFamily\&quot;:{\&quot;request\&quot;:[{\&quot;name\&quot;:\&quot;table\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;family\&quot;,\&quot;type\&quot;:\&quot;AFamilyDescriptor\&quot;}],\&quot;response\&quot;:\&quot;null\&quot;,\&quot;errors\&quot;:[\&quot;AIOError\&quot;]},\&quot;deleteFamily\&quot;:{\&quot;request\&quot;:[{\&quot;name\&quot;:\&quot;table\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;family\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;}],\&quot;response\&quot;:\&quot;null\&quot;,\&quot;errors\&quot;:[\&quot;AIOError\&quot;]},\&quot;modifyFamily\&quot;:{\&quot;request\&quot;:[{\&quot;name\&quot;:\&quot;table\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;familyName\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;familyDescriptor\&quot;,\&quot;type\&quot;:\&quot;AFamilyDescriptor\&quot;}],\&quot;response\&quot;:\&quot;null\&quot;,\&quot;errors\&quot;:[\&quot;AIOError\&quot;]},\&quot;get\&quot;:{\&quot;request\&quot;:[{\&quot;name\&quot;:\&quot;table\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;get\&quot;,\&quot;type\&quot;:\&quot;AGet\&quot;}],\&quot;response\&quot;:\&quot;AResult\&quot;,\&quot;errors\&quot;:[\&quot;AIOError\&quot;]},\&quot;exists\&quot;:{\&quot;request\&quot;:[{\&quot;name\&quot;:\&quot;table\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;get\&quot;,\&quot;type\&quot;:\&quot;AGet\&quot;}],\&quot;response\&quot;:\&quot;boolean\&quot;,\&quot;errors\&quot;:[\&quot;AIOError\&quot;]},\&quot;put\&quot;:{\&quot;request\&quot;:[{\&quot;name\&quot;:\&quot;table\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;put\&quot;,\&quot;type\&quot;:\&quot;APut\&quot;}],\&quot;response\&quot;:\&quot;null\&quot;,\&quot;errors\&quot;:[\&quot;AIOError\&quot;]},\&quot;delete\&quot;:{\&quot;request\&quot;:[{\&quot;name\&quot;:\&quot;table\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;delete\&quot;,\&quot;type\&quot;:\&quot;ADelete\&quot;}],\&quot;response\&quot;:\&quot;null\&quot;,\&quot;errors\&quot;:[\&quot;AIOError\&quot;]},\&quot;incrementColumnValue\&quot;:{\&quot;request\&quot;:[{\&quot;name\&quot;:\&quot;table\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;row\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;family\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;qualifier\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;amount\&quot;,\&quot;type\&quot;:\&quot;long\&quot;},{\&quot;name\&quot;:\&quot;writeToWAL\&quot;,\&quot;type\&quot;:\&quot;boolean\&quot;}],\&quot;response\&quot;:\&quot;long\&quot;,\&quot;errors\&quot;:[\&quot;AIOError\&quot;]},\&quot;scannerOpen\&quot;:{\&quot;request\&quot;:[{\&quot;name\&quot;:\&quot;table\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;scan\&quot;,\&quot;type\&quot;:\&quot;AScan\&quot;}],\&quot;response\&quot;:\&quot;int\&quot;,\&quot;errors\&quot;:[\&quot;AIOError\&quot;]},\&quot;scannerClose\&quot;:{\&quot;request\&quot;:[{\&quot;name\&quot;:\&quot;scannerId\&quot;,\&quot;type\&quot;:\&quot;int\&quot;}],\&quot;response\&quot;:\&quot;null\&quot;,\&quot;errors\&quot;:[\&quot;AIOError\&quot;,\&quot;AIllegalArgument\&quot;]},\&quot;scannerGetRows\&quot;:{\&quot;request\&quot;:[{\&quot;name\&quot;:\&quot;scannerId\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;numberOfRows\&quot;,\&quot;type\&quot;:\&quot;int\&quot;}],\&quot;response\&quot;:{\&quot;type\&quot;:\&quot;array\&quot;,\&quot;items\&quot;:\&quot;AResult\&quot;},\&quot;errors\&quot;:[\&quot;AIOError\&quot;,\&quot;AIllegalArgument\&quot;]}}}&quot;);
  org.apache.avro.util.Utf8 getHBaseVersion()
    throws org.apache.avro.ipc.AvroRemoteException, org.apache.hadoop.hbase.avro.generated.AIOError;
  org.apache.hadoop.hbase.avro.generated.AClusterStatus getClusterStatus()
    throws org.apache.avro.ipc.AvroRemoteException, org.apache.hadoop.hbase.avro.generated.AIOError;
  org.apache.avro.generic.GenericArray&lt;org.apache.hadoop.hbase.avro.generated.ATableDescriptor&gt; listTables()
    throws org.apache.avro.ipc.AvroRemoteException, org.apache.hadoop.hbase.avro.generated.AIOError;
  org.apache.hadoop.hbase.avro.generated.ATableDescriptor describeTable(java.nio.ByteBuffer table)
    throws org.apache.avro.ipc.AvroRemoteException, org.apache.hadoop.hbase.avro.generated.AIOError;
  boolean isTableEnabled(java.nio.ByteBuffer table)
    throws org.apache.avro.ipc.AvroRemoteException, org.apache.hadoop.hbase.avro.generated.AIOError;
  boolean tableExists(java.nio.ByteBuffer table)
    throws org.apache.avro.ipc.AvroRemoteException, org.apache.hadoop.hbase.avro.generated.AIOError;
  org.apache.hadoop.hbase.avro.generated.AFamilyDescriptor describeFamily(java.nio.ByteBuffer table, java.nio.ByteBuffer family)
    throws org.apache.avro.ipc.AvroRemoteException, org.apache.hadoop.hbase.avro.generated.AIOError;
  java.lang.Void createTable(org.apache.hadoop.hbase.avro.generated.ATableDescriptor table)
    throws org.apache.avro.ipc.AvroRemoteException, org.apache.hadoop.hbase.avro.generated.AIOError, org.apache.hadoop.hbase.avro.generated.AIllegalArgument, org.apache.hadoop.hbase.avro.generated.ATableExists, org.apache.hadoop.hbase.avro.generated.AMasterNotRunning;
  java.lang.Void deleteTable(java.nio.ByteBuffer table)
    throws org.apache.avro.ipc.AvroRemoteException, org.apache.hadoop.hbase.avro.generated.AIOError;
  java.lang.Void modifyTable(java.nio.ByteBuffer table, org.apache.hadoop.hbase.avro.generated.ATableDescriptor tableDescriptor)
    throws org.apache.avro.ipc.AvroRemoteException, org.apache.hadoop.hbase.avro.generated.AIOError;
  java.lang.Void enableTable(java.nio.ByteBuffer table)
    throws org.apache.avro.ipc.AvroRemoteException, org.apache.hadoop.hbase.avro.generated.AIOError;
  java.lang.Void disableTable(java.nio.ByteBuffer table)
    throws org.apache.avro.ipc.AvroRemoteException, org.apache.hadoop.hbase.avro.generated.AIOError;
  java.lang.Void flush(java.nio.ByteBuffer table)
    throws org.apache.avro.ipc.AvroRemoteException, org.apache.hadoop.hbase.avro.generated.AIOError;
  java.lang.Void split(java.nio.ByteBuffer table)
    throws org.apache.avro.ipc.AvroRemoteException, org.apache.hadoop.hbase.avro.generated.AIOError;
  java.lang.Void addFamily(java.nio.ByteBuffer table, org.apache.hadoop.hbase.avro.generated.AFamilyDescriptor family)
    throws org.apache.avro.ipc.AvroRemoteException, org.apache.hadoop.hbase.avro.generated.AIOError;
  java.lang.Void deleteFamily(java.nio.ByteBuffer table, java.nio.ByteBuffer family)
    throws org.apache.avro.ipc.AvroRemoteException, org.apache.hadoop.hbase.avro.generated.AIOError;
  java.lang.Void modifyFamily(java.nio.ByteBuffer table, java.nio.ByteBuffer familyName, org.apache.hadoop.hbase.avro.generated.AFamilyDescriptor familyDescriptor)
    throws org.apache.avro.ipc.AvroRemoteException, org.apache.hadoop.hbase.avro.generated.AIOError;
  org.apache.hadoop.hbase.avro.generated.AResult get(java.nio.ByteBuffer table, org.apache.hadoop.hbase.avro.generated.AGet get)
    throws org.apache.avro.ipc.AvroRemoteException, org.apache.hadoop.hbase.avro.generated.AIOError;
  boolean exists(java.nio.ByteBuffer table, org.apache.hadoop.hbase.avro.generated.AGet get)
    throws org.apache.avro.ipc.AvroRemoteException, org.apache.hadoop.hbase.avro.generated.AIOError;
  java.lang.Void put(java.nio.ByteBuffer table, org.apache.hadoop.hbase.avro.generated.APut put)
    throws org.apache.avro.ipc.AvroRemoteException, org.apache.hadoop.hbase.avro.generated.AIOError;
  java.lang.Void delete(java.nio.ByteBuffer table, org.apache.hadoop.hbase.avro.generated.ADelete delete)
    throws org.apache.avro.ipc.AvroRemoteException, org.apache.hadoop.hbase.avro.generated.AIOError;
  long incrementColumnValue(java.nio.ByteBuffer table, java.nio.ByteBuffer row, java.nio.ByteBuffer family, java.nio.ByteBuffer qualifier, long amount, boolean writeToWAL)
    throws org.apache.avro.ipc.AvroRemoteException, org.apache.hadoop.hbase.avro.generated.AIOError;
  int scannerOpen(java.nio.ByteBuffer table, org.apache.hadoop.hbase.avro.generated.AScan scan)

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/avro/generated/IOError.java
 =======================================================================
package org.apache.hadoop.hbase.avro.generated;

@SuppressWarnings(&quot;all&quot;)
public class IOError extends org.apache.avro.specific.SpecificExceptionBase implements org.apache.avro.specific.SpecificRecord {
  public static final org.apache.avro.Schema SCHEMA$ = org.apache.avro.Schema.parse(&quot;{\&quot;type\&quot;:\&quot;error\&quot;,\&quot;name\&quot;:\&quot;IOError\&quot;,\&quot;namespace\&quot;:\&quot;org.apache.hadoop.hbase.avro.generated\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;message\&quot;,\&quot;type\&quot;:\&quot;string\&quot;}]}&quot;);
  public org.apache.avro.util.Utf8 message;
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return message;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
  @SuppressWarnings(value=&quot;unchecked&quot;)
  public void put(int field$, java.lang.Object value$) {
    switch (field$) {
    case 0: message = (org.apache.avro.util.Utf8)value$; break;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
}

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/avro/generated/AServerInfo.java
 =======================================================================
package org.apache.hadoop.hbase.avro.generated;

@SuppressWarnings(&quot;all&quot;)
public class AServerInfo extends org.apache.avro.specific.SpecificRecordBase implements org.apache.avro.specific.SpecificRecord {
  public static final org.apache.avro.Schema SCHEMA$ = org.apache.avro.Schema.parse(&quot;{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;AServerInfo\&quot;,\&quot;namespace\&quot;:\&quot;org.apache.hadoop.hbase.avro.generated\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;infoPort\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;load\&quot;,\&quot;type\&quot;:{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;AServerLoad\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;load\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;maxHeapMB\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;memStoreSizeInMB\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;numberOfRegions\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;numberOfRequests\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;regionsLoad\&quot;,\&quot;type\&quot;:{\&quot;type\&quot;:\&quot;array\&quot;,\&quot;items\&quot;:{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;ARegionLoad\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;memStoreSizeMB\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;name\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;storefileIndexSizeMB\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;storefiles\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;storefileSizeMB\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;stores\&quot;,\&quot;type\&quot;:\&quot;int\&quot;}]}}},{\&quot;name\&quot;:\&quot;storefileIndexSizeInMB\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;storefiles\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;storefileSizeInMB\&quot;,\&quot;type\&quot;:\&quot;int\&quot;},{\&quot;name\&quot;:\&quot;usedHeapMB\&quot;,\&quot;type\&quot;:\&quot;int\&quot;}]}},{\&quot;name\&quot;:\&quot;serverAddress\&quot;,\&quot;type\&quot;:{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;AServerAddress\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;hostname\&quot;,\&quot;type\&quot;:\&quot;string\&quot;},{\&quot;name\&quot;:\&quot;inetSocketAddress\&quot;,\&quot;type\&quot;:\&quot;string\&quot;},{\&quot;name\&quot;:\&quot;port\&quot;,\&quot;type\&quot;:\&quot;int\&quot;}]}},{\&quot;name\&quot;:\&quot;serverName\&quot;,\&quot;type\&quot;:\&quot;string\&quot;},{\&quot;name\&quot;:\&quot;startCode\&quot;,\&quot;type\&quot;:\&quot;long\&quot;}]}&quot;);
  public int infoPort;
  public org.apache.hadoop.hbase.avro.generated.AServerLoad load;
  public org.apache.hadoop.hbase.avro.generated.AServerAddress serverAddress;
  public org.apache.avro.util.Utf8 serverName;
  public long startCode;
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return infoPort;
    case 1: return load;
    case 2: return serverAddress;
    case 3: return serverName;
    case 4: return startCode;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
  @SuppressWarnings(value=&quot;unchecked&quot;)
  public void put(int field$, java.lang.Object value$) {
    switch (field$) {
    case 0: infoPort = (java.lang.Integer)value$; break;
    case 1: load = (org.apache.hadoop.hbase.avro.generated.AServerLoad)value$; break;
    case 2: serverAddress = (org.apache.hadoop.hbase.avro.generated.AServerAddress)value$; break;
    case 3: serverName = (org.apache.avro.util.Utf8)value$; break;
    case 4: startCode = (java.lang.Long)value$; break;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
}

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/avro/generated/AColumnValue.java
 =======================================================================
package org.apache.hadoop.hbase.avro.generated;

@SuppressWarnings(&quot;all&quot;)
public class AColumnValue extends org.apache.avro.specific.SpecificRecordBase implements org.apache.avro.specific.SpecificRecord {
  public static final org.apache.avro.Schema SCHEMA$ = org.apache.avro.Schema.parse(&quot;{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;AColumnValue\&quot;,\&quot;namespace\&quot;:\&quot;org.apache.hadoop.hbase.avro.generated\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;family\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;qualifier\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;value\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;timestamp\&quot;,\&quot;type\&quot;:[\&quot;long\&quot;,\&quot;null\&quot;]}]}&quot;);
  public java.nio.ByteBuffer family;
  public java.nio.ByteBuffer qualifier;
  public java.nio.ByteBuffer value;
  public java.lang.Long timestamp;
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return family;
    case 1: return qualifier;
    case 2: return value;
    case 3: return timestamp;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
  @SuppressWarnings(value=&quot;unchecked&quot;)
  public void put(int field$, java.lang.Object value$) {
    switch (field$) {
    case 0: family = (java.nio.ByteBuffer)value$; break;
    case 1: qualifier = (java.nio.ByteBuffer)value$; break;
    case 2: value = (java.nio.ByteBuffer)value$; break;
    case 3: timestamp = (java.lang.Long)value$; break;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
}

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/avro/generated/AIllegalArgument.java
 =======================================================================
package org.apache.hadoop.hbase.avro.generated;

@SuppressWarnings(&quot;all&quot;)
public class AIllegalArgument extends org.apache.avro.specific.SpecificExceptionBase implements org.apache.avro.specific.SpecificRecord {
  public static final org.apache.avro.Schema SCHEMA$ = org.apache.avro.Schema.parse(&quot;{\&quot;type\&quot;:\&quot;error\&quot;,\&quot;name\&quot;:\&quot;AIllegalArgument\&quot;,\&quot;namespace\&quot;:\&quot;org.apache.hadoop.hbase.avro.generated\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;message\&quot;,\&quot;type\&quot;:\&quot;string\&quot;}]}&quot;);
  public org.apache.avro.util.Utf8 message;
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return message;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
  @SuppressWarnings(value=&quot;unchecked&quot;)
  public void put(int field$, java.lang.Object value$) {
    switch (field$) {
    case 0: message = (org.apache.avro.util.Utf8)value$; break;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
}

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/avro/generated/ADelete.java
 =======================================================================
package org.apache.hadoop.hbase.avro.generated;

@SuppressWarnings(&quot;all&quot;)
public class ADelete extends org.apache.avro.specific.SpecificRecordBase implements org.apache.avro.specific.SpecificRecord {
  public static final org.apache.avro.Schema SCHEMA$ = org.apache.avro.Schema.parse(&quot;{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;ADelete\&quot;,\&quot;namespace\&quot;:\&quot;org.apache.hadoop.hbase.avro.generated\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;row\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;columns\&quot;,\&quot;type\&quot;:[{\&quot;type\&quot;:\&quot;array\&quot;,\&quot;items\&quot;:{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;AColumn\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;family\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;qualifier\&quot;,\&quot;type\&quot;:[\&quot;bytes\&quot;,\&quot;null\&quot;]}]}},\&quot;null\&quot;]}]}&quot;);
  public java.nio.ByteBuffer row;
  public org.apache.avro.generic.GenericArray&lt;org.apache.hadoop.hbase.avro.generated.AColumn&gt; columns;
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return row;
    case 1: return columns;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
  @SuppressWarnings(value=&quot;unchecked&quot;)
  public void put(int field$, java.lang.Object value$) {
    switch (field$) {
    case 0: row = (java.nio.ByteBuffer)value$; break;
    case 1: columns = (org.apache.avro.generic.GenericArray&lt;org.apache.hadoop.hbase.avro.generated.AColumn&gt;)value$; break;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
}

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/avro/generated/AMasterNotRunning.java
 =======================================================================
package org.apache.hadoop.hbase.avro.generated;

@SuppressWarnings(&quot;all&quot;)
public class AMasterNotRunning extends org.apache.avro.specific.SpecificExceptionBase implements org.apache.avro.specific.SpecificRecord {
  public static final org.apache.avro.Schema SCHEMA$ = org.apache.avro.Schema.parse(&quot;{\&quot;type\&quot;:\&quot;error\&quot;,\&quot;name\&quot;:\&quot;AMasterNotRunning\&quot;,\&quot;namespace\&quot;:\&quot;org.apache.hadoop.hbase.avro.generated\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;message\&quot;,\&quot;type\&quot;:\&quot;string\&quot;}]}&quot;);
  public org.apache.avro.util.Utf8 message;
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return message;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
  @SuppressWarnings(value=&quot;unchecked&quot;)
  public void put(int field$, java.lang.Object value$) {
    switch (field$) {
    case 0: message = (org.apache.avro.util.Utf8)value$; break;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
}

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/avro/generated/TCell.java
 =======================================================================
package org.apache.hadoop.hbase.avro.generated;

@SuppressWarnings(&quot;all&quot;)
public class TCell extends org.apache.avro.specific.SpecificRecordBase implements org.apache.avro.specific.SpecificRecord {
  public static final org.apache.avro.Schema SCHEMA$ = org.apache.avro.Schema.parse(&quot;{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;TCell\&quot;,\&quot;namespace\&quot;:\&quot;org.apache.hadoop.hbase.avro.generated\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;value\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;timestamp\&quot;,\&quot;type\&quot;:\&quot;long\&quot;}]}&quot;);
  public java.nio.ByteBuffer value;
  public long timestamp;
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return value;
    case 1: return timestamp;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
  @SuppressWarnings(value=&quot;unchecked&quot;)
  public void put(int field$, java.lang.Object value$) {
    switch (field$) {
    case 0: value = (java.nio.ByteBuffer)value$; break;
    case 1: timestamp = (java.lang.Long)value$; break;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
}

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/avro/generated/AAlreadyExists.java
 =======================================================================
package org.apache.hadoop.hbase.avro.generated;

@SuppressWarnings(&quot;all&quot;)
public class AAlreadyExists extends org.apache.avro.specific.SpecificExceptionBase implements org.apache.avro.specific.SpecificRecord {
  public static final org.apache.avro.Schema SCHEMA$ = org.apache.avro.Schema.parse(&quot;{\&quot;type\&quot;:\&quot;error\&quot;,\&quot;name\&quot;:\&quot;AAlreadyExists\&quot;,\&quot;namespace\&quot;:\&quot;org.apache.hadoop.hbase.avro.generated\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;message\&quot;,\&quot;type\&quot;:\&quot;string\&quot;}]}&quot;);
  public org.apache.avro.util.Utf8 message;
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return message;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
  @SuppressWarnings(value=&quot;unchecked&quot;)
  public void put(int field$, java.lang.Object value$) {
    switch (field$) {
    case 0: message = (org.apache.avro.util.Utf8)value$; break;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
}

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/avro/generated/AScan.java
 =======================================================================
package org.apache.hadoop.hbase.avro.generated;

@SuppressWarnings(&quot;all&quot;)
public class AScan extends org.apache.avro.specific.SpecificRecordBase implements org.apache.avro.specific.SpecificRecord {
  public static final org.apache.avro.Schema SCHEMA$ = org.apache.avro.Schema.parse(&quot;{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;AScan\&quot;,\&quot;namespace\&quot;:\&quot;org.apache.hadoop.hbase.avro.generated\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;startRow\&quot;,\&quot;type\&quot;:[\&quot;bytes\&quot;,\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;stopRow\&quot;,\&quot;type\&quot;:[\&quot;bytes\&quot;,\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;columns\&quot;,\&quot;type\&quot;:[{\&quot;type\&quot;:\&quot;array\&quot;,\&quot;items\&quot;:{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;AColumn\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;family\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;qualifier\&quot;,\&quot;type\&quot;:[\&quot;bytes\&quot;,\&quot;null\&quot;]}]}},\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;timestamp\&quot;,\&quot;type\&quot;:[\&quot;long\&quot;,\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;timerange\&quot;,\&quot;type\&quot;:[{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;ATimeRange\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;minStamp\&quot;,\&quot;type\&quot;:\&quot;long\&quot;},{\&quot;name\&quot;:\&quot;maxStamp\&quot;,\&quot;type\&quot;:\&quot;long\&quot;}]},\&quot;null\&quot;]},{\&quot;name\&quot;:\&quot;maxVersions\&quot;,\&quot;type\&quot;:[\&quot;int\&quot;,\&quot;null\&quot;]}]}&quot;);
  public java.nio.ByteBuffer startRow;
  public java.nio.ByteBuffer stopRow;
  public org.apache.avro.generic.GenericArray&lt;org.apache.hadoop.hbase.avro.generated.AColumn&gt; columns;
  public java.lang.Long timestamp;
  public org.apache.hadoop.hbase.avro.generated.ATimeRange timerange;
  public java.lang.Integer maxVersions;
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return startRow;
    case 1: return stopRow;
    case 2: return columns;
    case 3: return timestamp;
    case 4: return timerange;
    case 5: return maxVersions;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
  @SuppressWarnings(value=&quot;unchecked&quot;)
  public void put(int field$, java.lang.Object value$) {
    switch (field$) {
    case 0: startRow = (java.nio.ByteBuffer)value$; break;
    case 1: stopRow = (java.nio.ByteBuffer)value$; break;
    case 2: columns = (org.apache.avro.generic.GenericArray&lt;org.apache.hadoop.hbase.avro.generated.AColumn&gt;)value$; break;
    case 3: timestamp = (java.lang.Long)value$; break;
    case 4: timerange = (org.apache.hadoop.hbase.avro.generated.ATimeRange)value$; break;
    case 5: maxVersions = (java.lang.Integer)value$; break;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
}

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/avro/generated/AResultEntry.java
 =======================================================================
package org.apache.hadoop.hbase.avro.generated;

@SuppressWarnings(&quot;all&quot;)
public class AResultEntry extends org.apache.avro.specific.SpecificRecordBase implements org.apache.avro.specific.SpecificRecord {
  public static final org.apache.avro.Schema SCHEMA$ = org.apache.avro.Schema.parse(&quot;{\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;AResultEntry\&quot;,\&quot;namespace\&quot;:\&quot;org.apache.hadoop.hbase.avro.generated\&quot;,\&quot;fields\&quot;:[{\&quot;name\&quot;:\&quot;family\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;qualifier\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;value\&quot;,\&quot;type\&quot;:\&quot;bytes\&quot;},{\&quot;name\&quot;:\&quot;timestamp\&quot;,\&quot;type\&quot;:\&quot;long\&quot;}]}&quot;);
  public java.nio.ByteBuffer family;
  public java.nio.ByteBuffer qualifier;
  public java.nio.ByteBuffer value;
  public long timestamp;
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return family;
    case 1: return qualifier;
    case 2: return value;
    case 3: return timestamp;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
  @SuppressWarnings(value=&quot;unchecked&quot;)
  public void put(int field$, java.lang.Object value$) {
    switch (field$) {
    case 0: family = (java.nio.ByteBuffer)value$; break;
    case 1: qualifier = (java.nio.ByteBuffer)value$; break;
    case 2: value = (java.nio.ByteBuffer)value$; break;
    case 3: timestamp = (java.lang.Long)value$; break;
    default: throw new org.apache.avro.AvroRuntimeException(&quot;Bad index&quot;);
    }
  }
}

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/avro/hbase.avpr
 =======================================================================
{
  &quot;protocol&quot; : &quot;HBase&quot;,
  &quot;namespace&quot; : &quot;org.apache.hadoop.hbase.avro.generated&quot;,
  &quot;types&quot; : [ {
    &quot;type&quot; : &quot;record&quot;,
    &quot;name&quot; : &quot;AServerAddress&quot;,
    &quot;fields&quot; : [ {
      &quot;name&quot; : &quot;hostname&quot;,
      &quot;type&quot; : &quot;string&quot;
    }, {
      &quot;name&quot; : &quot;inetSocketAddress&quot;,
      &quot;type&quot; : &quot;string&quot;
    }, {
      &quot;name&quot; : &quot;port&quot;,
      &quot;type&quot; : &quot;int&quot;
    } ]
  }, {
    &quot;type&quot; : &quot;record&quot;,
    &quot;name&quot; : &quot;ARegionLoad&quot;,
    &quot;fields&quot; : [ {
      &quot;name&quot; : &quot;memStoreSizeMB&quot;,
      &quot;type&quot; : &quot;int&quot;
    }, {
      &quot;name&quot; : &quot;name&quot;,
      &quot;type&quot; : &quot;bytes&quot;
    }, {
      &quot;name&quot; : &quot;storefileIndexSizeMB&quot;,
      &quot;type&quot; : &quot;int&quot;
    }, {
      &quot;name&quot; : &quot;storefiles&quot;,
      &quot;type&quot; : &quot;int&quot;
    }, {
      &quot;name&quot; : &quot;storefileSizeMB&quot;,
      &quot;type&quot; : &quot;int&quot;
    }, {
      &quot;name&quot; : &quot;stores&quot;,
      &quot;type&quot; : &quot;int&quot;
    } ]
  }, {
    &quot;type&quot; : &quot;record&quot;,
    &quot;name&quot; : &quot;AServerLoad&quot;,
    &quot;fields&quot; : [ {
      &quot;name&quot; : &quot;load&quot;,
      &quot;type&quot; : &quot;int&quot;
    }, {
      &quot;name&quot; : &quot;maxHeapMB&quot;,
      &quot;type&quot; : &quot;int&quot;
    }, {
      &quot;name&quot; : &quot;memStoreSizeInMB&quot;,
      &quot;type&quot; : &quot;int&quot;

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/filter/TimestampsFilter.java
 =======================================================================
package org.apache.hadoop.hbase.filter;

import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;
import java.util.List;
import java.util.TreeSet;

import org.apache.hadoop.hbase.KeyValue;

/**
 * Filter that returns only cells whose timestamp (version) is
 * in the specified list of timestamps (versions).
 * &lt;p&gt;
 * Note: Use of this filter overrides any time range/time stamp
 * options specified using {@link Get#setTimeRange(long, long)},
 * {@link Scan#setTimeRange(long, long)}, {@link Get#setTimeStamp(long)},
 * or {@link Scan#setTimeStamp(long)}.
 */
public class TimestampsFilter extends FilterBase {

  TreeSet&lt;Long&gt; timestamps;

  // Used during scans to hint the scan to stop early
  // once the timestamps fall below the minTimeStamp.
  long minTimeStamp = Long.MAX_VALUE;

  /**
   * Used during deserialization. Do not use otherwise.
   */
  public TimestampsFilter() {
    super();
  }

  /**
   * Constructor for filter that retains only those
   * cells whose timestamp (version) is in the specified
   * list of timestamps.
   *
   * @param timestamps
   */
  public TimestampsFilter(List&lt;Long&gt; timestamps) {
    this.timestamps = new TreeSet&lt;Long&gt;(timestamps);
    init();
  }

  private void init() {
    if (this.timestamps.size() &gt; 0) {
      minTimeStamp = this.timestamps.first();
    }

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/executor/HBaseExecutorService.java
 =======================================================================

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/executor/HBaseEventHandler.java
 =======================================================================

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/executor/RegionTransitionEventData.java
 =======================================================================

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/regionserver/RSZookeeperUpdater.java
 =======================================================================

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/regionserver/wal/LogEntryVisitor.java
 =======================================================================
package org.apache.hadoop.hbase.regionserver.wal;

import org.apache.hadoop.hbase.HRegionInfo;

public interface LogEntryVisitor {

  /**
   *
   * @param info
   * @param logKey
   * @param logEdit
   */
  public void visitLogEntryBeforeWrite(HRegionInfo info, HLogKey logKey,
                                       WALEdit logEdit);
}

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/master/ZKUnassignedWatcher.java
 =======================================================================

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/master/handler/MasterOpenRegionHandler.java
 =======================================================================

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/master/handler/MasterCloseRegionHandler.java
 =======================================================================

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/ColumnSchemaMessage.java
 =======================================================================
// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: ColumnSchemaMessage.proto

package org.apache.hadoop.hbase.rest.protobuf.generated;

public final class ColumnSchemaMessage {
  private ColumnSchemaMessage() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
  }
  public static final class ColumnSchema extends
      com.google.protobuf.GeneratedMessage {
    // Use ColumnSchema.newBuilder() to construct.
    private ColumnSchema() {
      initFields();
    }
    private ColumnSchema(boolean noInit) {}
    
    private static final ColumnSchema defaultInstance;
    public static ColumnSchema getDefaultInstance() {
      return defaultInstance;
    }
    
    public ColumnSchema getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.rest.protobuf.generated.ColumnSchemaMessage.internal_static_org_apache_hadoop_hbase_rest_protobuf_generated_ColumnSchema_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.rest.protobuf.generated.ColumnSchemaMessage.internal_static_org_apache_hadoop_hbase_rest_protobuf_generated_ColumnSchema_fieldAccessorTable;
    }
    
    public static final class Attribute extends
        com.google.protobuf.GeneratedMessage {
      // Use Attribute.newBuilder() to construct.
      private Attribute() {
        initFields();
      }
      private Attribute(boolean noInit) {}
      
      private static final Attribute defaultInstance;
      public static Attribute getDefaultInstance() {
        return defaultInstance;
      }
      

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/CellSetMessage.java
 =======================================================================
// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: CellSetMessage.proto

package org.apache.hadoop.hbase.rest.protobuf.generated;

public final class CellSetMessage {
  private CellSetMessage() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
  }
  public static final class CellSet extends
      com.google.protobuf.GeneratedMessage {
    // Use CellSet.newBuilder() to construct.
    private CellSet() {
      initFields();
    }
    private CellSet(boolean noInit) {}
    
    private static final CellSet defaultInstance;
    public static CellSet getDefaultInstance() {
      return defaultInstance;
    }
    
    public CellSet getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.rest.protobuf.generated.CellSetMessage.internal_static_org_apache_hadoop_hbase_rest_protobuf_generated_CellSet_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.rest.protobuf.generated.CellSetMessage.internal_static_org_apache_hadoop_hbase_rest_protobuf_generated_CellSet_fieldAccessorTable;
    }
    
    public static final class Row extends
        com.google.protobuf.GeneratedMessage {
      // Use Row.newBuilder() to construct.
      private Row() {
        initFields();
      }
      private Row(boolean noInit) {}
      
      private static final Row defaultInstance;
      public static Row getDefaultInstance() {
        return defaultInstance;
      }
      

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/VersionMessage.java
 =======================================================================
// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: VersionMessage.proto

package org.apache.hadoop.hbase.rest.protobuf.generated;

public final class VersionMessage {
  private VersionMessage() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
  }
  public static final class Version extends
      com.google.protobuf.GeneratedMessage {
    // Use Version.newBuilder() to construct.
    private Version() {
      initFields();
    }
    private Version(boolean noInit) {}
    
    private static final Version defaultInstance;
    public static Version getDefaultInstance() {
      return defaultInstance;
    }
    
    public Version getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.rest.protobuf.generated.VersionMessage.internal_static_org_apache_hadoop_hbase_rest_protobuf_generated_Version_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.rest.protobuf.generated.VersionMessage.internal_static_org_apache_hadoop_hbase_rest_protobuf_generated_Version_fieldAccessorTable;
    }
    
    // optional string restVersion = 1;
    public static final int RESTVERSION_FIELD_NUMBER = 1;
    private boolean hasRestVersion;
    private java.lang.String restVersion_ = &quot;&quot;;
    public boolean hasRestVersion() { return hasRestVersion; }
    public java.lang.String getRestVersion() { return restVersion_; }
    
    // optional string jvmVersion = 2;
    public static final int JVMVERSION_FIELD_NUMBER = 2;
    private boolean hasJvmVersion;
    private java.lang.String jvmVersion_ = &quot;&quot;;
    public boolean hasJvmVersion() { return hasJvmVersion; }
    public java.lang.String getJvmVersion() { return jvmVersion_; }

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/TableInfoMessage.java
 =======================================================================
// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: TableInfoMessage.proto

package org.apache.hadoop.hbase.rest.protobuf.generated;

public final class TableInfoMessage {
  private TableInfoMessage() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
  }
  public static final class TableInfo extends
      com.google.protobuf.GeneratedMessage {
    // Use TableInfo.newBuilder() to construct.
    private TableInfo() {
      initFields();
    }
    private TableInfo(boolean noInit) {}
    
    private static final TableInfo defaultInstance;
    public static TableInfo getDefaultInstance() {
      return defaultInstance;
    }
    
    public TableInfo getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.rest.protobuf.generated.TableInfoMessage.internal_static_org_apache_hadoop_hbase_rest_protobuf_generated_TableInfo_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.rest.protobuf.generated.TableInfoMessage.internal_static_org_apache_hadoop_hbase_rest_protobuf_generated_TableInfo_fieldAccessorTable;
    }
    
    public static final class Region extends
        com.google.protobuf.GeneratedMessage {
      // Use Region.newBuilder() to construct.
      private Region() {
        initFields();
      }
      private Region(boolean noInit) {}
      
      private static final Region defaultInstance;
      public static Region getDefaultInstance() {
        return defaultInstance;
      }
      

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/ScannerMessage.java
 =======================================================================
// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: ScannerMessage.proto

package org.apache.hadoop.hbase.rest.protobuf.generated;

public final class ScannerMessage {
  private ScannerMessage() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
  }
  public static final class Scanner extends
      com.google.protobuf.GeneratedMessage {
    // Use Scanner.newBuilder() to construct.
    private Scanner() {
      initFields();
    }
    private Scanner(boolean noInit) {}
    
    private static final Scanner defaultInstance;
    public static Scanner getDefaultInstance() {
      return defaultInstance;
    }
    
    public Scanner getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.rest.protobuf.generated.ScannerMessage.internal_static_org_apache_hadoop_hbase_rest_protobuf_generated_Scanner_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.rest.protobuf.generated.ScannerMessage.internal_static_org_apache_hadoop_hbase_rest_protobuf_generated_Scanner_fieldAccessorTable;
    }
    
    // optional bytes startRow = 1;
    public static final int STARTROW_FIELD_NUMBER = 1;
    private boolean hasStartRow;
    private com.google.protobuf.ByteString startRow_ = com.google.protobuf.ByteString.EMPTY;
    public boolean hasStartRow() { return hasStartRow; }
    public com.google.protobuf.ByteString getStartRow() { return startRow_; }
    
    // optional bytes endRow = 2;
    public static final int ENDROW_FIELD_NUMBER = 2;
    private boolean hasEndRow;
    private com.google.protobuf.ByteString endRow_ = com.google.protobuf.ByteString.EMPTY;
    public boolean hasEndRow() { return hasEndRow; }
    public com.google.protobuf.ByteString getEndRow() { return endRow_; }

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/TableListMessage.java
 =======================================================================
// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: TableListMessage.proto

package org.apache.hadoop.hbase.rest.protobuf.generated;

public final class TableListMessage {
  private TableListMessage() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
  }
  public static final class TableList extends
      com.google.protobuf.GeneratedMessage {
    // Use TableList.newBuilder() to construct.
    private TableList() {
      initFields();
    }
    private TableList(boolean noInit) {}
    
    private static final TableList defaultInstance;
    public static TableList getDefaultInstance() {
      return defaultInstance;
    }
    
    public TableList getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.rest.protobuf.generated.TableListMessage.internal_static_org_apache_hadoop_hbase_rest_protobuf_generated_TableList_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.rest.protobuf.generated.TableListMessage.internal_static_org_apache_hadoop_hbase_rest_protobuf_generated_TableList_fieldAccessorTable;
    }
    
    // repeated string name = 1;
    public static final int NAME_FIELD_NUMBER = 1;
    private java.util.List&lt;java.lang.String&gt; name_ =
      java.util.Collections.emptyList();
    public java.util.List&lt;java.lang.String&gt; getNameList() {
      return name_;
    }
    public int getNameCount() { return name_.size(); }
    public java.lang.String getName(int index) {
      return name_.get(index);
    }
    
    private void initFields() {

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/TableSchemaMessage.java
 =======================================================================
// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: TableSchemaMessage.proto

package org.apache.hadoop.hbase.rest.protobuf.generated;

public final class TableSchemaMessage {
  private TableSchemaMessage() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
  }
  public static final class TableSchema extends
      com.google.protobuf.GeneratedMessage {
    // Use TableSchema.newBuilder() to construct.
    private TableSchema() {
      initFields();
    }
    private TableSchema(boolean noInit) {}
    
    private static final TableSchema defaultInstance;
    public static TableSchema getDefaultInstance() {
      return defaultInstance;
    }
    
    public TableSchema getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.rest.protobuf.generated.TableSchemaMessage.internal_static_org_apache_hadoop_hbase_rest_protobuf_generated_TableSchema_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.rest.protobuf.generated.TableSchemaMessage.internal_static_org_apache_hadoop_hbase_rest_protobuf_generated_TableSchema_fieldAccessorTable;
    }
    
    public static final class Attribute extends
        com.google.protobuf.GeneratedMessage {
      // Use Attribute.newBuilder() to construct.
      private Attribute() {
        initFields();
      }
      private Attribute(boolean noInit) {}
      
      private static final Attribute defaultInstance;
      public static Attribute getDefaultInstance() {
        return defaultInstance;
      }
      

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/StorageClusterStatusMessage.java
 =======================================================================
// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: StorageClusterStatusMessage.proto

package org.apache.hadoop.hbase.rest.protobuf.generated;

public final class StorageClusterStatusMessage {
  private StorageClusterStatusMessage() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
  }
  public static final class StorageClusterStatus extends
      com.google.protobuf.GeneratedMessage {
    // Use StorageClusterStatus.newBuilder() to construct.
    private StorageClusterStatus() {
      initFields();
    }
    private StorageClusterStatus(boolean noInit) {}
    
    private static final StorageClusterStatus defaultInstance;
    public static StorageClusterStatus getDefaultInstance() {
      return defaultInstance;
    }
    
    public StorageClusterStatus getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.rest.protobuf.generated.StorageClusterStatusMessage.internal_static_org_apache_hadoop_hbase_rest_protobuf_generated_StorageClusterStatus_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.rest.protobuf.generated.StorageClusterStatusMessage.internal_static_org_apache_hadoop_hbase_rest_protobuf_generated_StorageClusterStatus_fieldAccessorTable;
    }
    
    public static final class Region extends
        com.google.protobuf.GeneratedMessage {
      // Use Region.newBuilder() to construct.
      private Region() {
        initFields();
      }
      private Region(boolean noInit) {}
      
      private static final Region defaultInstance;
      public static Region getDefaultInstance() {
        return defaultInstance;
      }
      

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/CellMessage.java
 =======================================================================
// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: CellMessage.proto

package org.apache.hadoop.hbase.rest.protobuf.generated;

public final class CellMessage {
  private CellMessage() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
  }
  public static final class Cell extends
      com.google.protobuf.GeneratedMessage {
    // Use Cell.newBuilder() to construct.
    private Cell() {
      initFields();
    }
    private Cell(boolean noInit) {}
    
    private static final Cell defaultInstance;
    public static Cell getDefaultInstance() {
      return defaultInstance;
    }
    
    public Cell getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.rest.protobuf.generated.CellMessage.internal_static_org_apache_hadoop_hbase_rest_protobuf_generated_Cell_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.rest.protobuf.generated.CellMessage.internal_static_org_apache_hadoop_hbase_rest_protobuf_generated_Cell_fieldAccessorTable;
    }
    
    // optional bytes row = 1;
    public static final int ROW_FIELD_NUMBER = 1;
    private boolean hasRow;
    private com.google.protobuf.ByteString row_ = com.google.protobuf.ByteString.EMPTY;
    public boolean hasRow() { return hasRow; }
    public com.google.protobuf.ByteString getRow() { return row_; }
    
    // optional bytes column = 2;
    public static final int COLUMN_FIELD_NUMBER = 2;
    private boolean hasColumn;
    private com.google.protobuf.ByteString column_ = com.google.protobuf.ByteString.EMPTY;
    public boolean hasColumn() { return hasColumn; }
    public com.google.protobuf.ByteString getColumn() { return column_; }

 =======================================================================
 ==src/main/javadoc/org/apache/hadoop/hbase/thrift/doc-files/style.css
 =======================================================================
/* Auto-generated CSS for generated Thrift docs */
body { font-family: Tahoma, sans-serif; }
pre { background-color: #dddddd; padding: 6px; }
h3,h4 { padding-top: 0px; margin-top: 0px; }
div.definition { border: 1px solid gray; margin: 10px; padding: 10px; }
div.extends { margin: -0.5em 0 1em 5em }
table { border: 1px solid grey; border-collapse: collapse; }
td { border: 1px solid grey; padding: 1px 6px; vertical-align: top; }
th { border: 1px solid black; background-color: #bbbbbb;
     text-align: left; padding: 1px 6px; }

 =======================================================================
 ==src/main/javadoc/org/apache/hadoop/hbase/thrift/doc-files/Hbase.html
 =======================================================================
&lt;html&gt;&lt;head&gt;
&lt;link href=&quot;style.css&quot; rel=&quot;stylesheet&quot; type=&quot;text/css&quot;/&gt;
&lt;title&gt;Thrift module: Hbase&lt;/title&gt;&lt;/head&gt;&lt;body&gt;
&lt;h1&gt;Thrift module: Hbase&lt;/h1&gt;
&lt;table&gt;&lt;tr&gt;&lt;th&gt;Module&lt;/th&gt;&lt;th&gt;Services&lt;/th&gt;&lt;th&gt;Data types&lt;/th&gt;&lt;th&gt;Constants&lt;/th&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Hbase&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;Hbase.html#Svc_Hbase&quot;&gt;Hbase&lt;/a&gt;&lt;br/&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_atomicIncrement&quot;&gt;atomicIncrement&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_compact&quot;&gt;compact&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_createTable&quot;&gt;createTable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_deleteAll&quot;&gt;deleteAll&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_deleteAllRow&quot;&gt;deleteAllRow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_deleteAllRowTs&quot;&gt;deleteAllRowTs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_deleteAllTs&quot;&gt;deleteAllTs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_deleteTable&quot;&gt;deleteTable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_disableTable&quot;&gt;disableTable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_enableTable&quot;&gt;enableTable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_get&quot;&gt;get&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_getColumnDescriptors&quot;&gt;getColumnDescriptors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_getRow&quot;&gt;getRow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_getRowTs&quot;&gt;getRowTs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_getRowWithColumns&quot;&gt;getRowWithColumns&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_getRowWithColumnsTs&quot;&gt;getRowWithColumnsTs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_getTableNames&quot;&gt;getTableNames&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_getTableRegions&quot;&gt;getTableRegions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_getVer&quot;&gt;getVer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_getVerTs&quot;&gt;getVerTs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_isTableEnabled&quot;&gt;isTableEnabled&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_majorCompact&quot;&gt;majorCompact&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_mutateRow&quot;&gt;mutateRow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_mutateRowTs&quot;&gt;mutateRowTs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_mutateRows&quot;&gt;mutateRows&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_mutateRowsTs&quot;&gt;mutateRowsTs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_scannerClose&quot;&gt;scannerClose&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_scannerGet&quot;&gt;scannerGet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_scannerGetList&quot;&gt;scannerGetList&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_scannerOpen&quot;&gt;scannerOpen&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_scannerOpenTs&quot;&gt;scannerOpenTs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_scannerOpenWithPrefix&quot;&gt;scannerOpenWithPrefix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_scannerOpenWithStop&quot;&gt;scannerOpenWithStop&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_scannerOpenWithStopTs&quot;&gt;scannerOpenWithStopTs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;Hbase.html#Struct_AlreadyExists&quot;&gt;AlreadyExists&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;Hbase.html#Struct_BatchMutation&quot;&gt;BatchMutation&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;Hbase.html#Typedef_Bytes&quot;&gt;Bytes&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;Hbase.html#Struct_ColumnDescriptor&quot;&gt;ColumnDescriptor&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;Hbase.html#Struct_IOError&quot;&gt;IOError&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;Hbase.html#Struct_IllegalArgument&quot;&gt;IllegalArgument&lt;/a&gt;&lt;br/&gt;

 =======================================================================
 ==src/main/javadoc/org/apache/hadoop/hbase/thrift/doc-files/index.html
 =======================================================================
&lt;html&gt;&lt;head&gt;
&lt;link href=&quot;style.css&quot; rel=&quot;stylesheet&quot; type=&quot;text/css&quot;/&gt;
&lt;title&gt;All Thrift declarations&lt;/title&gt;&lt;/head&gt;&lt;body&gt;
&lt;h1&gt;All Thrift declarations&lt;/h1&gt;
&lt;table&gt;&lt;tr&gt;&lt;th&gt;Module&lt;/th&gt;&lt;th&gt;Services&lt;/th&gt;&lt;th&gt;Data types&lt;/th&gt;&lt;th&gt;Constants&lt;/th&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Hbase&lt;/td&gt;&lt;td&gt;&lt;a href=&quot;Hbase.html#Svc_Hbase&quot;&gt;Hbase&lt;/a&gt;&lt;br/&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_atomicIncrement&quot;&gt;atomicIncrement&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_compact&quot;&gt;compact&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_createTable&quot;&gt;createTable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_deleteAll&quot;&gt;deleteAll&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_deleteAllRow&quot;&gt;deleteAllRow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_deleteAllRowTs&quot;&gt;deleteAllRowTs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_deleteAllTs&quot;&gt;deleteAllTs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_deleteTable&quot;&gt;deleteTable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_disableTable&quot;&gt;disableTable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_enableTable&quot;&gt;enableTable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_get&quot;&gt;get&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_getColumnDescriptors&quot;&gt;getColumnDescriptors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_getRow&quot;&gt;getRow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_getRowTs&quot;&gt;getRowTs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_getRowWithColumns&quot;&gt;getRowWithColumns&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_getRowWithColumnsTs&quot;&gt;getRowWithColumnsTs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_getTableNames&quot;&gt;getTableNames&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_getTableRegions&quot;&gt;getTableRegions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_getVer&quot;&gt;getVer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_getVerTs&quot;&gt;getVerTs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_isTableEnabled&quot;&gt;isTableEnabled&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_majorCompact&quot;&gt;majorCompact&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_mutateRow&quot;&gt;mutateRow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_mutateRowTs&quot;&gt;mutateRowTs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_mutateRows&quot;&gt;mutateRows&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_mutateRowsTs&quot;&gt;mutateRowsTs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_scannerClose&quot;&gt;scannerClose&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_scannerGet&quot;&gt;scannerGet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_scannerGetList&quot;&gt;scannerGetList&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_scannerOpen&quot;&gt;scannerOpen&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_scannerOpenTs&quot;&gt;scannerOpenTs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_scannerOpenWithPrefix&quot;&gt;scannerOpenWithPrefix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_scannerOpenWithStop&quot;&gt;scannerOpenWithStop&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;Hbase.html#Fn_Hbase_scannerOpenWithStopTs&quot;&gt;scannerOpenWithStopTs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;Hbase.html#Struct_AlreadyExists&quot;&gt;AlreadyExists&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;Hbase.html#Struct_BatchMutation&quot;&gt;BatchMutation&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;Hbase.html#Typedef_Bytes&quot;&gt;Bytes&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;Hbase.html#Struct_ColumnDescriptor&quot;&gt;ColumnDescriptor&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;Hbase.html#Struct_IOError&quot;&gt;IOError&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;Hbase.html#Struct_IllegalArgument&quot;&gt;IllegalArgument&lt;/a&gt;&lt;br/&gt;

 =======================================================================
 ==src/docbkx/sample_article.xml
 =======================================================================
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;article version=&quot;5.0&quot; xmlns=&quot;http://docbook.org/ns/docbook&quot;
         xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;
         xmlns:xi=&quot;http://www.w3.org/2001/XInclude&quot;
         xmlns:svg=&quot;http://www.w3.org/2000/svg&quot;
         xmlns:m=&quot;http://www.w3.org/1998/Math/MathML&quot;
         xmlns:html=&quot;http://www.w3.org/1999/xhtml&quot;
         xmlns:db=&quot;http://docbook.org/ns/docbook&quot;&gt;
  &lt;info&gt;
    &lt;title&gt;Wah-wah
&lt;?eval ${project.version}?&gt;
    &lt;/title&gt;


  &lt;/info&gt;

  &lt;section xml:id=&quot;wahwah&quot;&gt;
    &lt;title&gt;Wah-Wah changed my life&lt;/title&gt;

    &lt;para&gt;I was born very young...&lt;/para&gt;

    &lt;para&gt;This is a sample docbook article.&lt;/para&gt;
    &lt;para&gt;
    &lt;?eval ${project.version}?&gt;
    &lt;/para&gt;

    &lt;section xml:id=&quot;then&quot;&gt;
      &lt;title&gt;Then&lt;/title&gt;

      &lt;para&gt;&lt;/para&gt;
    &lt;/section&gt;

    &lt;section xml:id=&quot;and&quot;&gt;
      &lt;title&gt;And&lt;/title&gt;

      &lt;para&gt;&lt;/para&gt;
    &lt;/section&gt;

    &lt;section xml:id=&quot;later&quot;&gt;
      &lt;title&gt;Later&lt;/title&gt;

      &lt;para&gt;&lt;/para&gt;
    &lt;/section&gt;
  &lt;/section&gt;

  &lt;section xml:id=&quot;good_books&quot;&gt;
    &lt;title&gt;Good books&lt;/title&gt;

    &lt;para&gt;&lt;/para&gt;
  &lt;/section&gt;

 =======================================================================
 ==src/docbkx/book.xml
 =======================================================================
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;book version=&quot;5.0&quot; xmlns=&quot;http://docbook.org/ns/docbook&quot;
      xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot;
      xmlns:xi=&quot;http://www.w3.org/2001/XInclude&quot;
      xmlns:svg=&quot;http://www.w3.org/2000/svg&quot;
      xmlns:m=&quot;http://www.w3.org/1998/Math/MathML&quot;
      xmlns:html=&quot;http://www.w3.org/1999/xhtml&quot;
      xmlns:db=&quot;http://docbook.org/ns/docbook&quot;&gt;
  &lt;info&gt;
    &lt;title&gt;HBase Book
&lt;?eval ${project.version}?&gt;

    &lt;/title&gt;
  &lt;/info&gt;

  &lt;chapter xml:id=&quot;getting_started&quot;&gt;
    &lt;title &gt;Getting Started&lt;/title&gt;

    &lt;section&gt;
      &lt;title&gt;Requirements&lt;/title&gt;

      &lt;para&gt;First...&lt;/para&gt;
    &lt;/section&gt;
  &lt;/chapter&gt;

  &lt;chapter xml:id=&quot;datamodel&quot;&gt;
    &lt;title&gt;Data Model&lt;/title&gt;

    &lt;para&gt;&lt;/para&gt;
  &lt;/chapter&gt;

  &lt;chapter xml:id=&quot;implementation&quot;&gt;
    &lt;title&gt;Implementation&lt;/title&gt;

    &lt;para&gt;&lt;/para&gt;
  &lt;/chapter&gt;

  &lt;chapter xml:id=&quot;mapreduce&quot;&gt;
    &lt;title&gt;MapReduce&lt;/title&gt;

    &lt;para&gt;&lt;/para&gt;
  &lt;/chapter&gt;

  &lt;chapter xml:id=&quot;schema&quot;&gt;
    &lt;title&gt;Schema Design&lt;/title&gt;

    &lt;para&gt;&lt;/para&gt;
  &lt;/chapter&gt;

  &lt;chapter xml:id=&quot;shell&quot;&gt;

 =======================================================================
 ==src/assembly/bin.xml
 =======================================================================
&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;assembly xmlns=&quot;http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.1&quot;
          xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
          xsi:schemaLocation=&quot;http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.1 http://maven.apache.org/xsd/assembly-1.1.1.xsd&quot;&gt;
  &lt;id&gt;bin&lt;/id&gt;
  &lt;formats&gt;
    &lt;format&gt;tar.gz&lt;/format&gt;
  &lt;/formats&gt;
  &lt;fileSets&gt;
    &lt;fileSet&gt;
      &lt;includes&gt;
        &lt;include&gt;${basedir}/*.txt&lt;/include&gt;
      &lt;/includes&gt;
    &lt;/fileSet&gt;
    &lt;fileSet&gt;
      &lt;directory&gt;conf&lt;/directory&gt;
    &lt;/fileSet&gt;
    &lt;fileSet&gt;
      &lt;directory&gt;bin&lt;/directory&gt;
      &lt;fileMode&gt;755&lt;/fileMode&gt;
    &lt;/fileSet&gt;
    &lt;fileSet&gt;
      &lt;directory&gt;src/main/ruby&lt;/directory&gt;
      &lt;outputDirectory&gt;lib/ruby&lt;/outputDirectory&gt;
    &lt;/fileSet&gt;
    &lt;fileSet&gt;
      &lt;directory&gt;target&lt;/directory&gt;
      &lt;outputDirectory&gt;/&lt;/outputDirectory&gt;
      &lt;includes&gt;
          &lt;include&gt;hbase-${project.version}.jar&lt;/include&gt;
          &lt;include&gt;hbase-${project.version}-tests.jar&lt;/include&gt;
          &lt;include&gt;hbase-${project.version}-sources.jar&lt;/include&gt;
      &lt;/includes&gt;
    &lt;/fileSet&gt;
    &lt;fileSet&gt;
      &lt;directory&gt;target/hbase-webapps&lt;/directory&gt;
      &lt;outputDirectory&gt;hbase-webapps&lt;/outputDirectory&gt;
    &lt;/fileSet&gt;
    &lt;fileSet&gt;
      &lt;directory&gt;target/site&lt;/directory&gt;
      &lt;outputDirectory&gt;docs&lt;/outputDirectory&gt;
    &lt;/fileSet&gt;
    &lt;fileSet&gt;
      &lt;directory&gt;src/main/resources/&lt;/directory&gt;
      &lt;outputDirectory&gt;conf&lt;/outputDirectory&gt;
      &lt;includes&gt;
        &lt;include&gt;hbase-default.xml&lt;/include&gt;
      &lt;/includes&gt;
    &lt;/fileSet&gt;
  &lt;/fileSets&gt;

 =======================================================================
 ==src/assembly/src.xml
 =======================================================================
&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;assembly xmlns=&quot;http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.1&quot;
          xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
          xsi:schemaLocation=&quot;http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.1 http://maven.apache.org/xsd/assembly-1.1.1.xsd&quot;&gt;
  &lt;id&gt;src&lt;/id&gt;
  &lt;formats&gt;
    &lt;format&gt;tar.gz&lt;/format&gt;
  &lt;/formats&gt;
  &lt;fileSets&gt;
    &lt;fileSet&gt;
      &lt;includes&gt;
        &lt;include&gt;${basedir}/*.txt&lt;/include&gt;
      &lt;/includes&gt;
    &lt;/fileSet&gt;
    &lt;fileSet&gt;
      &lt;includes&gt;
        &lt;include&gt;pom.xml&lt;/include&gt;
      &lt;/includes&gt;
    &lt;/fileSet&gt;
    &lt;fileSet&gt;
      &lt;directory&gt;src&lt;/directory&gt;
    &lt;/fileSet&gt;
    &lt;fileSet&gt;
      &lt;directory&gt;conf&lt;/directory&gt;
    &lt;/fileSet&gt;
    &lt;fileSet&gt;
      &lt;directory&gt;docs&lt;/directory&gt;
    &lt;/fileSet&gt;
    &lt;fileSet&gt;
      &lt;directory&gt;cloudera&lt;/directory&gt;
    &lt;/fileSet&gt;
    &lt;fileSet&gt;
      &lt;directory&gt;bin&lt;/directory&gt;
      &lt;fileMode&gt;755&lt;/fileMode&gt;
    &lt;/fileSet&gt;
  &lt;/fileSets&gt;
&lt;/assembly&gt;

 =======================================================================
 ==src/examples/mapreduce/index-builder-setup.rb
 =======================================================================
# Set up sample data for IndexBuilder example
create &quot;people&quot;, &quot;attributes&quot;
create &quot;people-email&quot;, &quot;INDEX&quot;
create &quot;people-phone&quot;, &quot;INDEX&quot;
create &quot;people-name&quot;, &quot;INDEX&quot;

[[&quot;1&quot;, &quot;jenny&quot;, &quot;jenny@example.com&quot;, &quot;867-5309&quot;],
 [&quot;2&quot;, &quot;alice&quot;, &quot;alice@example.com&quot;, &quot;555-1234&quot;],
 [&quot;3&quot;, &quot;kevin&quot;, &quot;kevinpet@example.com&quot;, &quot;555-1212&quot;]].each do |fields|
  (id, name, email, phone) = *fields
  put &quot;people&quot;, id, &quot;attributes:name&quot;, name
  put &quot;people&quot;, id, &quot;attributes:email&quot;, email
  put &quot;people&quot;, id, &quot;attributes:phone&quot;, phone
end
  

 =======================================================================
 ==cloudera/hbase.1
 =======================================================================
.\&quot; Process this file with
.\&quot; groff -man -Tascii hbase.1
.\&quot;
.TH hbase 1 &quot;October 2010 &quot; Linux &quot;User Manuals&quot;

.SH NAME
HBase \- HBase is the Hadoop database.

.SH SYNOPSIS

.B hbase
\fICOMMAND\fR

.SH DESCRIPTION

HBase is the Hadoop database. Use it when you need random, realtime
read/write access to your Big Data. This project's goal is the hosting
of very large tables -- billions of rows X millions of columns -- atop
clusters of commodity hardware.

HBase is an open-source, distributed, versioned, column-oriented store
modeled after Google's Bigtable: A Distributed Storage System for
Structured Data by Chang et al. Just as Bigtable leverages the
distributed data storage provided by the Google File System, HBase
provides Bigtable-like capabilities on top of Hadoop.

For more information about HBase, see http://hbase.apache.org.

\fICOMMAND\fR may be one of the following:
  shell            run the HBase shell
  shell-tests      run the HBase shell tests
  zkcli            run the ZooKeeper shell
  master           run an HBase HMaster node
  regionserver     run an HBase HRegionServer node
  zookeeper        run a Zookeeper server
  rest             run an HBase REST server
  thrift           run an HBase Thrift server
  avro             run an HBase Avro server
  migrate          upgrade an hbase.rootdir
  hbck             run the hbase 'fsck' tool
 or
  CLASSNAME        run the class named CLASSNAME

Most commands print help when invoked w/o parameters or with --help.

.SH ENVIRONMENT

.IP JAVA_HOME
The java implementation to use.  Overrides JAVA_HOME.


 =======================================================================
 ==cloudera/apply-patches
 =======================================================================
#!/bin/sh -x

set -e

if [ $# != 2 ]; then
  echo usage: $0 '&lt;target-dir&gt; &lt;patch-dir&gt;'
  exit 1
fi

TARGET_DIR=`readlink -f $1`
PATCH_DIR=`readlink -f $2`

cd $TARGET_DIR

# We have to git init, or else git apply will search upwards and find
# some other git repository (even though this build is taking place
# inside a gitignored build/ dir)
# even though we never commit to this &quot;repository&quot;, this serves to
# anchor the repository root at the source dir root.
git init-db
for PATCH in `ls -1 $PATCH_DIR/* | sort` ; do
    git apply --whitespace=nowarn $PATCH
done

 =======================================================================
 ==cloudera/do-release-build
 =======================================================================
#!/bin/bash
# Copyright (c) 2009 Cloudera, inc
#
# Performs a release build

set -ex

# Do the build
BIN_DIR=$(readlink -f $(dirname $0))
RELEASE_DIR=$BIN_DIR/..

cd $RELEASE_DIR

mvn -DskipTests clean
mvn -DskipTests -Dhbase.version=${FULL_VERSION} site install assembly:assembly 
mkdir -p build
for x in target/*.tar.gz ; do
  tar -C build -xzf $x
done

(cd build &amp;&amp; tar -czf hbase-${FULL_VERSION}.tar.gz hbase-${FULL_VERSION})

 =======================================================================
 ==cloudera/patches/0021-HBASE-3008-Memstore.updateColumnValue-passes-wrong-f.patch
 =======================================================================
From 405eef0025bc6894c117b5d542706e18b64aa4fe Mon Sep 17 00:00:00 2001
From: Todd Lipcon &lt;todd@cloudera.com&gt;
Date: Thu, 7 Oct 2010 13:29:27 -0700
Subject: [PATCH 21/30] HBASE-3008  Memstore.updateColumnValue passes wrong flag to heapSizeChange (Causes memstore size to go negative)

Author: Ryan Rawson
---
 .../java/org/apache/hadoop/hbase/KeyValue.java     |    2 +-
 .../hadoop/hbase/regionserver/TestHRegion.java     |   16 ++++++
 .../hadoop/hbase/regionserver/TestStore.java       |   58 ++++++++++++++++++++
 3 files changed, 75 insertions(+), 1 deletions(-)

diff --git a/src/main/java/org/apache/hadoop/hbase/KeyValue.java b/src/main/java/org/apache/hadoop/hbase/KeyValue.java
index bb26f27..ffbcb15 100644
--- a/src/main/java/org/apache/hadoop/hbase/KeyValue.java
+++ b/src/main/java/org/apache/hadoop/hbase/KeyValue.java
@@ -1907,7 +1907,7 @@ public class KeyValue implements Writable, HeapSize {
     return ClassSize.align(ClassSize.OBJECT + (2 * ClassSize.REFERENCE) +
         ClassSize.align(ClassSize.ARRAY + length) +
         (3 * Bytes.SIZEOF_INT) +
-        ClassSize.align(ClassSize.ARRAY + (rowCache == null ? 0 : rowCache.length)) +
+        ClassSize.align(ClassSize.ARRAY) +
         (2 * Bytes.SIZEOF_LONG));
   }
 
diff --git a/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java b/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java
index d0b84cc..139252f 100644
--- a/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java
+++ b/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java
@@ -1958,6 +1958,22 @@ public class TestHRegion extends HBaseTestCase {
     assertICV(row, fam1, qual1, value+amount);
   }
 
+  public void testIncrementColumnValue_heapSize() throws IOException {
+    EnvironmentEdgeManagerTestHelper.injectEdge(new IncrementingEnvironmentEdge());
+
+    initHRegion(tableName, getName(), fam1);
+
+    long byAmount = 1L;
+    long size;
+
+    for( int i = 0; i &lt; 1000 ; i++) {
+      region.incrementColumnValue(row, fam1, qual1, byAmount, true);
+
+      size = region.memstoreSize.get();
+      assertTrue(&quot;memstore size: &quot; + size, size &gt;= 0);
+    }
+  }
+
   public void testIncrementColumnValue_UpdatingInPlace_Negative()

 =======================================================================
 ==cloudera/patches/0016-CLOUDERA-BUILD.-HBase-running-on-secure-hadoop-tempo.patch
 =======================================================================
From 74542880d740e9be24b103f1d5f5c6489d01911c Mon Sep 17 00:00:00 2001
From: Todd Lipcon &lt;todd@cloudera.com&gt;
Date: Mon, 30 Aug 2010 16:25:56 -0700
Subject: [PATCH 16/30] CLOUDERA-BUILD. HBase running on secure hadoop, temporary patch.

This is not upstreamed, since it currently is very difficult to do this
without reflection or a shim layer. This will be upstreamed with the
larger project of HBase security later this year.

Author: Todd Lipcon
---
 .../org/apache/hadoop/hbase/ipc/HBaseClient.java   |    4 +-
 .../java/org/apache/hadoop/hbase/ipc/HBaseRPC.java |    3 +-
 .../org/apache/hadoop/hbase/ipc/HBaseServer.java   |   24 ++-
 .../org/apache/hadoop/hbase/master/HMaster.java    |   45 +++++-
 .../hadoop/hbase/regionserver/HRegionServer.java   |   59 ++++++-
 .../apache/hadoop/hbase/util/JVMClusterUtil.java   |   60 ++++++-
 .../apache/hadoop/hbase/HBaseTestingUtility.java   |   12 +-
 .../org/apache/hadoop/hbase/MiniHBaseCluster.java  |   37 ++---
 .../hadoop/hbase/regionserver/TestStore.java       |   82 +++++----
 .../hbase/regionserver/wal/TestWALReplay.java      |  180 +++++++++++---------
 10 files changed, 330 insertions(+), 176 deletions(-)

diff --git a/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java b/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java
index 2b5eeb6..dbd4803 100644
--- a/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java
+++ b/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java
@@ -25,7 +25,6 @@ import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.io.DataOutputBuffer;
 import org.apache.hadoop.io.IOUtils;
-import org.apache.hadoop.io.ObjectWritable;
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.io.WritableUtils;
 import org.apache.hadoop.ipc.RemoteException;
@@ -385,8 +384,7 @@ public class HBaseClient {
       out.write(HBaseServer.CURRENT_VERSION);
       //When there are more fields we can have ConnectionHeader Writable.
       DataOutputBuffer buf = new DataOutputBuffer();
-      ObjectWritable.writeObject(buf, remoteId.getTicket(),
-                                 UserGroupInformation.class, conf);
+      WritableUtils.writeString(buf, remoteId.getTicket().getUserName());
       int bufLen = buf.getLength();
       out.writeInt(bufLen);
       out.write(buf.getData(), 0, bufLen);
diff --git a/src/main/java/org/apache/hadoop/hbase/ipc/HBaseRPC.java b/src/main/java/org/apache/hadoop/hbase/ipc/HBaseRPC.java
index 2d90d4e..58e9d0d 100644
--- a/src/main/java/org/apache/hadoop/hbase/ipc/HBaseRPC.java
+++ b/src/main/java/org/apache/hadoop/hbase/ipc/HBaseRPC.java
@@ -381,7 +381,8 @@ public class HBaseRPC {

 =======================================================================
 ==cloudera/patches/0009-CLOUDERA-BUILD.-rsync-all-of-lib-into-target-directo.patch
 =======================================================================
From cdeff4c4e3e5c21b55b04cd36c17804f88eba88e Mon Sep 17 00:00:00 2001
From: todd &lt;todd@ubuntu64-build01.(none)&gt;
Date: Sun, 27 Jun 2010 23:24:58 -0700
Subject: [PATCH 09/30] CLOUDERA-BUILD. rsync all of lib into target directory

Otherwise shell doesn't work, since we don't get .rb files

CLOUDERA-BUILD. Replace rsync with cp.
---
 cloudera/install_hbase.sh |    5 +----
 1 files changed, 1 insertions(+), 4 deletions(-)

diff --git a/cloudera/install_hbase.sh b/cloudera/install_hbase.sh
index 74090d4..6e9f257 100755
--- a/cloudera/install_hbase.sh
+++ b/cloudera/install_hbase.sh
@@ -93,10 +93,7 @@ install -d -m 0755 $PREFIX/$DOC_DIR
 install -d -m 0755 $PREFIX/$BIN_DIR
 install -d -m 0755 $PREFIX/$ETC_DIR
 
-for i in `find lib/*.jar -type f `
-        do echo &quot;Copying $i&quot;
-        cp $i ${PREFIX}/${LIB_DIR}/lib #don't copy directories by default
-done
+cp -ra lib/* ${PREFIX}/${LIB_DIR}/lib/
 cp hbase*.jar $PREFIX/$LIB_DIR
 cp -a docs/* $PREFIX/$DOC_DIR
 cp *.txt $PREFIX/$DOC_DIR/
-- 
1.7.0.4


 =======================================================================
 ==cloudera/patches/0005-Re-enable-log-split-test.patch
 =======================================================================
From 4377d51ef77152d798c7c7739a6e64c52911286c Mon Sep 17 00:00:00 2001
From: Todd Lipcon &lt;todd@cloudera.com&gt;
Date: Thu, 3 Jun 2010 20:32:45 -0700
Subject: [PATCH 05/30] Re-enable log split test

---
 .../hbase/regionserver/wal/TestHLogSplit.java      |    4 ++--
 1 files changed, 2 insertions(+), 2 deletions(-)

diff --git a/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java b/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java
index 9cbebb0..d7bdc44 100644
--- a/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java
+++ b/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java
@@ -404,7 +404,7 @@ public class TestHLogSplit {
       // hadoop 0.21 throws FNFE whereas hadoop 0.20 returns null
     }
   }
-/* DISABLED for now.  TODO: HBASE-2645 
+
   @Test
   public void testLogCannotBeWrittenOnceParsed() throws IOException {
     AtomicLong counter = new AtomicLong(0);
@@ -433,7 +433,7 @@ public class TestHLogSplit {
       stop.set(true);
     }
   }
-*/
+
 
   @Test
   public void testSplitWillNotTouchLogsIfNewHLogGetsCreatedAfterSplitStarted()
-- 
1.7.0.4


 =======================================================================
 ==cloudera/patches/0011-SequenceFileLogWriter-doesn-t-need-to-actually-call-.patch
 =======================================================================
From 6647194e4e48aba590a9fba9d3c699ee05510b16 Mon Sep 17 00:00:00 2001
From: Todd Lipcon &lt;todd@cloudera.com&gt;
Date: Wed, 8 Sep 2010 13:45:41 -0700
Subject: [PATCH 11/30] SequenceFileLogWriter doesn't need to actually call sync()

---
 .../regionserver/wal/SequenceFileLogWriter.java    |    2 +-
 1 files changed, 1 insertions(+), 1 deletions(-)

diff --git a/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java b/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java
index ea59695..7f1b6ce 100644
--- a/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java
+++ b/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java
@@ -139,4 +139,4 @@ public class SequenceFileLogWriter implements HLog.Writer {
   public OutputStream getDFSCOutputStream() {
     return this.dfsClient_out;
   }
-}
\ No newline at end of file
+}
-- 
1.7.0.4


 =======================================================================
 ==cloudera/patches/0014-HBASE-3000.-Add-hbase-classpath-command.patch
 =======================================================================
From 677ce02c0b4af1e361942e58bd8a34902cf363ca Mon Sep 17 00:00:00 2001
From: Todd Lipcon &lt;todd@cloudera.com&gt;
Date: Wed, 15 Sep 2010 01:37:37 -0400
Subject: [PATCH 14/30] HBASE-3000. Add hbase classpath command

---
 bin/hbase |    3 +++
 1 files changed, 3 insertions(+), 0 deletions(-)

diff --git a/bin/hbase b/bin/hbase
index c783e80..ec92f7b 100755
--- a/bin/hbase
+++ b/bin/hbase
@@ -262,6 +262,9 @@ elif [ &quot;$COMMAND&quot; = &quot;zookeeper&quot; ] ; then
   fi
 elif [ &quot;$COMMAND&quot; = &quot;zkcli&quot; ] ; then
   CLASS='org.apache.zookeeper.ZooKeeperMain'
+elif [ &quot;$COMMAND&quot; = &quot;classpath&quot; ] ; then
+  echo $CLASSPATH
+  exit 0
 else
   CLASS=$COMMAND
 fi
-- 
1.7.0.4


 =======================================================================
 ==cloudera/patches/0029-CLOUDERA-BUILD.-Publish-to-Cloudera-maven-repo.-Use-.patch
 =======================================================================
From 5e9777a0c3ce2f2c7a5bb172a4967a655b91d91e Mon Sep 17 00:00:00 2001
From: Eli Collins &lt;eli@cloudera.com&gt;
Date: Mon, 11 Oct 2010 09:43:26 -0700
Subject: [PATCH 29/30] CLOUDERA-BUILD. Publish to Cloudera maven repo. Use Cloudera ZK.

---
 pom.xml |   14 +++++++-------
 1 files changed, 7 insertions(+), 7 deletions(-)

diff --git a/pom.xml b/pom.xml
index 92523f4..c499985 100644
--- a/pom.xml
+++ b/pom.xml
@@ -2,7 +2,7 @@
 &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
          xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;&gt;
   &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
-  &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;
+  &lt;groupId&gt;com.cloudera.hbase&lt;/groupId&gt;
   &lt;artifactId&gt;hbase&lt;/artifactId&gt;
   &lt;packaging&gt;jar&lt;/packaging&gt;
   &lt;version&gt;${hbase.version}&lt;/version&gt;
@@ -496,7 +496,7 @@
     &lt;junit.version&gt;4.8.1&lt;/junit.version&gt;
     &lt;mockito-all.version&gt;1.8.4&lt;/mockito-all.version&gt;
     &lt;log4j.version&gt;1.2.15&lt;/log4j.version&gt;
-    &lt;zookeeper.version&gt;3.3.1&lt;/zookeeper.version&gt;
+    &lt;zookeeper.version&gt;3.3.1-10&lt;/zookeeper.version&gt;
 
     &lt;commons-httpclient.version&gt;3.1&lt;/commons-httpclient.version&gt;
     &lt;commons-lang.version&gt;2.5&lt;/commons-lang.version&gt;
@@ -568,7 +568,7 @@
         &lt;scope&gt;test&lt;/scope&gt;
       &lt;/dependency&gt;
       &lt;dependency&gt;
-        &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
+        &lt;groupId&gt;com.cloudera.zookeeper&lt;/groupId&gt;
         &lt;artifactId&gt;zookeeper&lt;/artifactId&gt;
         &lt;version&gt;${zookeeper.version}&lt;/version&gt;
       &lt;/dependency&gt;
@@ -658,7 +658,7 @@
     &lt;/dependency&gt;
 
     &lt;dependency&gt;
-      &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
+      &lt;groupId&gt;com.cloudera.zookeeper&lt;/groupId&gt;
       &lt;artifactId&gt;zookeeper&lt;/artifactId&gt;
     &lt;/dependency&gt;
 
@@ -826,9 +826,9 @@

 =======================================================================
 ==cloudera/patches/0015-HBASE-3001.-TableMapReduceUtil-should-always-add-dep.patch
 =======================================================================
From 5476f351a90bd028de924ad021a7f0924b20e103 Mon Sep 17 00:00:00 2001
From: Todd Lipcon &lt;todd@cloudera.com&gt;
Date: Wed, 15 Sep 2010 01:37:45 -0400
Subject: [PATCH 15/30] HBASE-3001. TableMapReduceUtil should always add dependency jars

Author: Todd Lipcon and Michael Stack
---
 .../hadoop/hbase/mapred/TableMapReduceUtil.java    |   28 ++++
 .../hadoop/hbase/mapreduce/TableMapReduceUtil.java |   36 +++---
 .../hadoop/hbase/mapreduce/package-info.java       |  152 +++++++++-----------
 3 files changed, 116 insertions(+), 100 deletions(-)

diff --git a/src/main/java/org/apache/hadoop/hbase/mapred/TableMapReduceUtil.java b/src/main/java/org/apache/hadoop/hbase/mapred/TableMapReduceUtil.java
index 41748fe..749e314 100644
--- a/src/main/java/org/apache/hadoop/hbase/mapred/TableMapReduceUtil.java
+++ b/src/main/java/org/apache/hadoop/hbase/mapred/TableMapReduceUtil.java
@@ -29,6 +29,10 @@ import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.io.WritableComparable;
 import org.apache.hadoop.mapred.FileInputFormat;
 import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.InputFormat;
+import org.apache.hadoop.mapred.OutputFormat;
+import org.apache.hadoop.mapred.TextInputFormat;
+import org.apache.hadoop.mapred.TextOutputFormat;
 
 /**
  * Utility for {@link TableMap} and {@link TableReduce}
@@ -59,6 +63,11 @@ public class TableMapReduceUtil {
     job.setMapperClass(mapper);
     FileInputFormat.addInputPaths(job, table);
     job.set(TableInputFormat.COLUMN_LIST, columns);
+    try {
+      addDependencyJars(job);
+    } catch (IOException ioe) {
+      throw new RuntimeException(ioe);
+    }
   }
 
   /**
@@ -105,6 +114,7 @@ public class TableMapReduceUtil {
     } else if (partitioner != null) {
       job.setPartitionerClass(partitioner);
     }
+    addDependencyJars(job);
   }
 
   /**
@@ -181,4 +191,22 @@ public class TableMapReduceUtil {
   public static void setScannerCaching(JobConf job, int batchSize) {
     job.setInt(&quot;hbase.client.scanner.caching&quot;, batchSize);

 =======================================================================
 ==cloudera/patches/0006-HBASE-2773.-Check-for-null-values-in-meta-in-test-ut.patch
 =======================================================================
From 89b8bdcb7eb2837c07a03783f2065be9f69a478d Mon Sep 17 00:00:00 2001
From: Todd Lipcon &lt;todd@lipcon.org&gt;
Date: Sun, 20 Jun 2010 20:25:13 -0700
Subject: [PATCH 06/30] HBASE-2773. Check for null values in meta in test util

---
 .../apache/hadoop/hbase/HBaseTestingUtility.java   |    6 ++++--
 1 files changed, 4 insertions(+), 2 deletions(-)

diff --git a/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java b/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
index fadee21..b0f132a 100644
--- a/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
+++ b/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
@@ -662,8 +662,10 @@ public class HBaseTestingUtility {
     List&lt;byte[]&gt; rows = new ArrayList&lt;byte[]&gt;();
     ResultScanner s = t.getScanner(new Scan());
     for (Result result : s) {
-      HRegionInfo info = Writables.getHRegionInfo(
-          result.getValue(HConstants.CATALOG_FAMILY, HConstants.REGIONINFO_QUALIFIER));
+      byte[] value = result.getValue(
+          HConstants.CATALOG_FAMILY, HConstants.REGIONINFO_QUALIFIER);
+      if (value == null) continue;
+      HRegionInfo info = Writables.getHRegionInfo(value);
       HTableDescriptor desc = info.getTableDesc();
       if (Bytes.compareTo(desc.getName(), tableName) == 0) {
         LOG.info(&quot;getMetaTableRows: row -&gt; &quot; +
-- 
1.7.0.4


 =======================================================================
 ==cloudera/patches/0010-HBASE-2467.-Concurrent-flushers-in-HLog-sync-using-H.patch
 =======================================================================
From cec5868f8de0080107540234f3005f1f033d43a4 Mon Sep 17 00:00:00 2001
From: Todd Lipcon &lt;todd@cloudera.com&gt;
Date: Tue, 7 Sep 2010 21:41:25 -0700
Subject: [PATCH 10/30] HBASE-2467. Concurrent flushers in HLog sync using HDFS-895 (JD's v3 patch)

---
 .../org/apache/hadoop/hbase/HTableDescriptor.java  |    2 +-
 .../apache/hadoop/hbase/regionserver/wal/HLog.java |  169 ++++++--------------
 .../hadoop/hbase/regionserver/wal/TestHLog.java    |    2 +-
 3 files changed, 53 insertions(+), 120 deletions(-)

diff --git a/src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java b/src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java
index 0d57270..3861799 100644
--- a/src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java
+++ b/src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java
@@ -95,7 +95,7 @@ public class HTableDescriptor implements WritableComparable&lt;HTableDescriptor&gt; {
 
   public static final long DEFAULT_MAX_FILESIZE = 1024*1024*256L;
 
-  public static final boolean DEFAULT_DEFERRED_LOG_FLUSH = true;
+  public static final boolean DEFAULT_DEFERRED_LOG_FLUSH = false;
 
   private volatile Boolean meta = null;
   private volatile Boolean root = null;
diff --git a/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java b/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java
index 9a62084..b7fc22f 100644
--- a/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java
+++ b/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java
@@ -141,10 +141,10 @@ public class HLog implements Syncable {
   private final long blocksize;
   private final int flushlogentries;
   private final String prefix;
-  private final AtomicInteger unflushedEntries = new AtomicInteger(0);
   private final Path oldLogDir;
   private final List&lt;LogActionsListener&gt; actionListeners =
       Collections.synchronizedList(new ArrayList&lt;LogActionsListener&gt;());
+  private boolean logRollRequested;
 
 
   private static Class&lt;? extends Writer&gt; logWriterClass;
@@ -212,6 +212,7 @@ public class HLog implements Syncable {
 
   // We synchronize on updateLock to prevent updates and to prevent a log roll
   // during an update
+  // locked during appends
   private final Object updateLock = new Object();
 
   private final boolean enabled;
@@ -224,7 +225,7 @@ public class HLog implements Syncable {
   private final int maxLogs;

 =======================================================================
 ==cloudera/patches/0023-CLOUDERA-BUILD.-cloudera-directory-should-get-instal.patch
 =======================================================================
From 3190f7406c74c716fe09f693589314c67b98b62b Mon Sep 17 00:00:00 2001
From: Todd Lipcon &lt;todd@cloudera.com&gt;
Date: Thu, 7 Oct 2010 19:56:33 -0700
Subject: [PATCH 23/30] CLOUDERA-BUILD. cloudera directory should get installed

---
 cloudera/install_hbase.sh |    1 +
 1 files changed, 1 insertions(+), 0 deletions(-)

diff --git a/cloudera/install_hbase.sh b/cloudera/install_hbase.sh
index dd96245..247780e 100755
--- a/cloudera/install_hbase.sh
+++ b/cloudera/install_hbase.sh
@@ -94,6 +94,7 @@ install -d -m 0755 $PREFIX/$BIN_DIR
 install -d -m 0755 $PREFIX/$ETC_DIR
 
 cp -ra lib/* ${PREFIX}/${LIB_DIR}/lib/
+cp -a cloudera ${PREFIX}/${LIB_DIR}/cloudera
 cp hbase*.jar $PREFIX/$LIB_DIR/
 
 # Make an unversioned jar symlink so that other
-- 
1.7.0.4


 =======================================================================
 ==cloudera/patches/0002-CLOUDERA-BUILD.-Add-build-infrastructure.patch
 =======================================================================
From b569811d0a92da0011375e27d2b38dc04cef3c44 Mon Sep 17 00:00:00 2001
From: newalex &lt;newalex@ubuntu64-build01.(none)&gt;
Date: Tue, 22 Jun 2010 12:26:19 -0700
Subject: [PATCH 02/30] CLOUDERA-BUILD. Add build infrastructure.

CLOUDERA-BUILD. Build should create a &quot;mixed&quot; src/bin tarball, and install from that
---
 cloudera/README.cloudera  |    9 +++
 cloudera/do-release-build |   21 +++++++
 cloudera/install_hbase.sh |  136 +++++++++++++++++++++++++++++++++++++++++++++
 3 files changed, 166 insertions(+), 0 deletions(-)
 create mode 100644 cloudera/README.cloudera
 create mode 100755 cloudera/do-release-build
 create mode 100755 cloudera/install_hbase.sh

diff --git a/cloudera/README.cloudera b/cloudera/README.cloudera
new file mode 100644
index 0000000..8626e8f
--- /dev/null
+++ b/cloudera/README.cloudera
@@ -0,0 +1,9 @@
+This build was generated by Cloudera's build system in the following manner:
+
+1) The pristine open-source release tarball was unpacked
+
+2) The patches contained within the patches/ directory next to this README
+were applied using the apply-patches script. A complete log of these changes
+is also included in CHANGES.cloudera.txt.
+
+3) The project was built by running the do-release-build script in this directory.
diff --git a/cloudera/do-release-build b/cloudera/do-release-build
new file mode 100755
index 0000000..0986c23
--- /dev/null
+++ b/cloudera/do-release-build
@@ -0,0 +1,21 @@
+#!/bin/bash
+# Copyright (c) 2009 Cloudera, inc
+#
+# Performs a release build
+
+set -ex
+
+# Do the build
+BIN_DIR=$(readlink -f $(dirname $0))
+RELEASE_DIR=$BIN_DIR/..
+
+cd $RELEASE_DIR
+
+mvn -DskipTests clean

 =======================================================================
 ==cloudera/patches/0026-CLOUDERA-BUILD.-Update-hadoop-version.patch
 =======================================================================
From 1faa3e29bfc2935a71197fe28cec4dbecf2b7692 Mon Sep 17 00:00:00 2001
From: Eli Collins &lt;eli@cloudera.com&gt;
Date: Fri, 8 Oct 2010 20:18:14 -0700
Subject: [PATCH 26/30] CLOUDERA-BUILD. Update hadoop version.

---
 pom.xml |   13 +------------
 1 files changed, 1 insertions(+), 12 deletions(-)

diff --git a/pom.xml b/pom.xml
index 4cbedb5..92523f4 100644
--- a/pom.xml
+++ b/pom.xml
@@ -186,17 +186,6 @@
       &lt;/releases&gt;
     &lt;/repository&gt;
     &lt;repository&gt;
-      &lt;id&gt;todd&lt;/id&gt;
-      &lt;name&gt;Todd Lipcon's repo for CDH snapshots&lt;/name&gt;
-      &lt;url&gt;http://people.apache.org/~todd/repo/&lt;/url&gt;
-      &lt;snapshots&gt;
-        &lt;enabled&gt;true&lt;/enabled&gt;
-      &lt;/snapshots&gt;
-      &lt;releases&gt;
-        &lt;enabled&gt;true&lt;/enabled&gt;
-      &lt;/releases&gt;
-    &lt;/repository&gt;
-    &lt;repository&gt;
       &lt;id&gt;temp-hadoop&lt;/id&gt;
       &lt;name&gt;Hadoop 0.20.1/2 packaging, thrift, zk&lt;/name&gt;
       &lt;url&gt;http://people.apache.org/~rawson/repo/&lt;/url&gt;
@@ -499,7 +488,7 @@
     &lt;compileSource&gt;1.6&lt;/compileSource&gt;
     &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
     &lt;hbase.version&gt;0.89.20100924&lt;/hbase.version&gt;
-    &lt;hadoop.version&gt;0.20.3-CDH3-SNAPSHOT&lt;/hadoop.version&gt;
+    &lt;hadoop.version&gt;0.20.2-737&lt;/hadoop.version&gt;
     &lt;commons-cli.version&gt;1.2&lt;/commons-cli.version&gt;
     &lt;commons-logging.version&gt;1.1.1&lt;/commons-logging.version&gt;
     &lt;jetty.version&gt;6.1.24&lt;/jetty.version&gt;
-- 
1.7.0.4


 =======================================================================
 ==cloudera/patches/0007-CLOUDERA-BUILD.-Include-cloudera-dir-in-src-assembly.patch
 =======================================================================
From cf4dc092807fc922103366ed222bef2745320a84 Mon Sep 17 00:00:00 2001
From: todd &lt;todd@ubuntu64-build01.(none)&gt;
Date: Sun, 27 Jun 2010 21:52:18 -0700
Subject: [PATCH 07/30] CLOUDERA-BUILD. Include cloudera/ dir in src assembly

---
 src/assembly/src.xml |    6 ++++++
 1 files changed, 6 insertions(+), 0 deletions(-)

diff --git a/src/assembly/src.xml b/src/assembly/src.xml
index ab414ac..5bb4f2f 100644
--- a/src/assembly/src.xml
+++ b/src/assembly/src.xml
@@ -24,6 +24,12 @@
       &lt;directory&gt;conf&lt;/directory&gt;
     &lt;/fileSet&gt;
     &lt;fileSet&gt;
+      &lt;directory&gt;docs&lt;/directory&gt;
+    &lt;/fileSet&gt;
+    &lt;fileSet&gt;
+      &lt;directory&gt;cloudera&lt;/directory&gt;
+    &lt;/fileSet&gt;
+    &lt;fileSet&gt;
       &lt;directory&gt;bin&lt;/directory&gt;
       &lt;fileMode&gt;755&lt;/fileMode&gt;
     &lt;/fileSet&gt;
-- 
1.7.0.4


 =======================================================================
 ==cloudera/patches/0017-HBASE-2782.-QoS-for-META-table-access.patch
 =======================================================================
From 49c5812eb3c58180b3a49791afc165d93d8351ea Mon Sep 17 00:00:00 2001
From: Todd Lipcon &lt;todd@cloudera.com&gt;
Date: Tue, 28 Sep 2010 22:59:55 -0700
Subject: [PATCH 17/30] HBASE-2782. QoS for META table access

Author: Ryan Rawson
---
 .../java/org/apache/hadoop/hbase/ipc/HBaseRPC.java |   15 ++--
 .../org/apache/hadoop/hbase/ipc/HBaseServer.java   |   93 ++++++++++++++++----
 .../org/apache/hadoop/hbase/master/HMaster.java    |    7 +-
 .../apache/hadoop/hbase/regionserver/HRegion.java  |   35 ++++----
 .../hadoop/hbase/regionserver/HRegionServer.java   |   74 +++++++++++++++-
 5 files changed, 181 insertions(+), 43 deletions(-)

diff --git a/src/main/java/org/apache/hadoop/hbase/ipc/HBaseRPC.java b/src/main/java/org/apache/hadoop/hbase/ipc/HBaseRPC.java
index 58e9d0d..9e7866b 100644
--- a/src/main/java/org/apache/hadoop/hbase/ipc/HBaseRPC.java
+++ b/src/main/java/org/apache/hadoop/hbase/ipc/HBaseRPC.java
@@ -20,12 +20,14 @@
 
 package org.apache.hadoop.hbase.ipc;
 
+import com.google.common.base.Function;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configurable;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.client.RetriesExhaustedException;
 import org.apache.hadoop.hbase.io.HbaseObjectWritable;
+import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.ipc.VersionedProtocol;
 import org.apache.hadoop.net.NetUtils;
@@ -82,8 +84,9 @@ public class HBaseRPC {
     super();
   }                                  // no public ctor
 
+
   /** A method invocation, including the method name and its parameters.*/
-  private static class Invocation implements Writable, Configurable {
+  public static class Invocation implements Writable, Configurable {
     private String methodName;
     @SuppressWarnings(&quot;unchecked&quot;)
     private Class[] parameterClasses;
@@ -497,9 +500,9 @@ public class HBaseRPC {
                                  final Class&lt;?&gt;[] ifaces,
                                  final String bindAddress, final int port,
                                  final int numHandlers,
-                                 final boolean verbose, Configuration conf)
+                                 int metaHandlerCount, final boolean verbose, Configuration conf, int highPriorityLevel)

 =======================================================================
 ==cloudera/patches/0019-CLOUDERA-BUILD.-Fix-copy-of-bin-to-be-cp-a.patch
 =======================================================================
From 427f35f2dc730c553d9be1c0d78d7b695e4b2b76 Mon Sep 17 00:00:00 2001
From: Todd Lipcon &lt;todd@cloudera.com&gt;
Date: Thu, 30 Sep 2010 22:50:02 -0700
Subject: [PATCH 19/30] CLOUDERA-BUILD. Fix copy of bin/ to be cp -a

---
 cloudera/install_hbase.sh |    2 +-
 1 files changed, 1 insertions(+), 1 deletions(-)

diff --git a/cloudera/install_hbase.sh b/cloudera/install_hbase.sh
index 6e9f257..65f7e71 100755
--- a/cloudera/install_hbase.sh
+++ b/cloudera/install_hbase.sh
@@ -100,7 +100,7 @@ cp *.txt $PREFIX/$DOC_DIR/
 cp -a hbase-webapps $PREFIX/$LIB_DIR
 
 cp -a conf $PREFIX/$ETC_DIR/conf
-cp bin/* $PREFIX/$BIN_DIR
+cp -a bin/* $PREFIX/$BIN_DIR/
 
 ln -s $ETC_DIR/conf $PREFIX/$LIB_DIR/conf
 
-- 
1.7.0.4


 =======================================================================
 ==cloudera/patches/0028-CLOUDERA-BUILD.-Fix-versionless-jar-naming-symlinks-.patch
 =======================================================================
From 80fd13dbb5d66bdfc80e6c22f6b4567f62499c49 Mon Sep 17 00:00:00 2001
From: Todd Lipcon &lt;todd@cloudera.com&gt;
Date: Sat, 9 Oct 2010 17:56:08 -0700
Subject: [PATCH 28/30] CLOUDERA-BUILD. Fix versionless jar naming symlinks for multijar case

Ref: CDH-2203
---
 cloudera/install_hbase.sh |    6 ++++--
 1 files changed, 4 insertions(+), 2 deletions(-)

diff --git a/cloudera/install_hbase.sh b/cloudera/install_hbase.sh
index 247780e..86c4baf 100755
--- a/cloudera/install_hbase.sh
+++ b/cloudera/install_hbase.sh
@@ -99,8 +99,10 @@ cp hbase*.jar $PREFIX/$LIB_DIR/
 
 # Make an unversioned jar symlink so that other
 # packages that depend on us can link in.
-for x in $PREFIX/hbase*jar ; do
-  ln -s $(basename $x) $PREFIX/$LIB_DIR/hbase.jar
+for x in $PREFIX/$LIB_DIR/hbase*jar ; do
+  JARNAME=$(basename $x)
+  VERSIONLESS_NAME=$(echo $JARNAME | sed -e 's,hbase-[0-9\+\-\.]*[0-9]\(-SNAPSHOT\)*,hbase,g')
+  ln -s $JARNAME $PREFIX/$LIB_DIR/$VERSIONLESS_NAME
 done
 cp -a docs/* $PREFIX/$DOC_DIR
 cp *.txt $PREFIX/$DOC_DIR/
-- 
1.7.0.4


 =======================================================================
 ==cloudera/patches/0025-HBASE-3096.-TestCompaction-timing-out.patch
 =======================================================================
From df24f2aa192153241a12711a95e861806db315f3 Mon Sep 17 00:00:00 2001
From: Todd Lipcon &lt;todd@cloudera.com&gt;
Date: Fri, 8 Oct 2010 15:29:23 -0700
Subject: [PATCH 25/30] HBASE-3096. TestCompaction timing out

Author: Todd Lipcon
---
 .../hadoop/hbase/regionserver/TestCompaction.java  |    6 +++---
 1 files changed, 3 insertions(+), 3 deletions(-)

diff --git a/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompaction.java b/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompaction.java
index 34b8044..9da4031 100644
--- a/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompaction.java
+++ b/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompaction.java
@@ -61,7 +61,7 @@ public class TestCompaction extends HBaseTestCase {
 
     // Set cache flush size to 1MB
     conf.setInt(&quot;hbase.hregion.memstore.flush.size&quot;, 1024*1024);
-    conf.setInt(&quot;hbase.hregion.memstore.block.multiplier&quot;, 10);
+    conf.setInt(&quot;hbase.hregion.memstore.block.multiplier&quot;, 30);
     this.cluster = null;
   }
 
@@ -94,9 +94,9 @@ public class TestCompaction extends HBaseTestCase {
    * @throws IOException
    */
   public void testMajorCompactingToNoOutput() throws IOException {
-    createStoreFile(r);
+    createSmallerStoreFile(r);
     for (int i = 0; i &lt; COMPACTION_THRESHOLD; i++) {
-      createStoreFile(r);
+      createSmallerStoreFile(r);
     }
     // Now delete everything.
     InternalScanner s = r.getScanner(new Scan());
-- 
1.7.0.4


 =======================================================================
 ==cloudera/patches/0003-CLOUDERA-BUILD.-Switch-to-CDH3b3-snapshot-in-todd-s-.patch
 =======================================================================
From 5ac1ffdf37c28be48196eb0f4226e0ade6d3e048 Mon Sep 17 00:00:00 2001
From: Todd Lipcon &lt;todd@cloudera.com&gt;
Date: Mon, 17 May 2010 17:22:34 -0700
Subject: [PATCH 03/30] CLOUDERA-BUILD. Switch to CDH3b3 snapshot in todd's repo

---
 pom.xml |   33 +++++++++++++++++++++++++++------
 1 files changed, 27 insertions(+), 6 deletions(-)

diff --git a/pom.xml b/pom.xml
index 90cafef..4cbedb5 100644
--- a/pom.xml
+++ b/pom.xml
@@ -186,6 +186,17 @@
       &lt;/releases&gt;
     &lt;/repository&gt;
     &lt;repository&gt;
+      &lt;id&gt;todd&lt;/id&gt;
+      &lt;name&gt;Todd Lipcon's repo for CDH snapshots&lt;/name&gt;
+      &lt;url&gt;http://people.apache.org/~todd/repo/&lt;/url&gt;
+      &lt;snapshots&gt;
+        &lt;enabled&gt;true&lt;/enabled&gt;
+      &lt;/snapshots&gt;
+      &lt;releases&gt;
+        &lt;enabled&gt;true&lt;/enabled&gt;
+      &lt;/releases&gt;
+    &lt;/repository&gt;
+    &lt;repository&gt;
       &lt;id&gt;temp-hadoop&lt;/id&gt;
       &lt;name&gt;Hadoop 0.20.1/2 packaging, thrift, zk&lt;/name&gt;
       &lt;url&gt;http://people.apache.org/~rawson/repo/&lt;/url&gt;
@@ -207,6 +218,17 @@
         &lt;enabled&gt;true&lt;/enabled&gt;
       &lt;/releases&gt;
     &lt;/repository&gt;
+    &lt;repository&gt;
+      &lt;id&gt;cloudera&lt;/id&gt;
+      &lt;name&gt;Repository for finding CDH3&lt;/name&gt;
+      &lt;url&gt; https://repository.cloudera.com/content/repositories/releases/&lt;/url&gt;
+      &lt;snapshots&gt;
+        &lt;enabled&gt;true&lt;/enabled&gt;
+      &lt;/snapshots&gt;
+      &lt;releases&gt;
+        &lt;enabled&gt;true&lt;/enabled&gt;
+      &lt;/releases&gt;
+    &lt;/repository&gt;
   &lt;/repositories&gt;
 
 
@@ -477,8 +499,7 @@

 =======================================================================
 ==cloudera/patches/0008-CLOUDERA-BUILD.-hbase-config.sh-should-set-HBASE_PID.patch
 =======================================================================
From e9c356f37bf195fc284e51ecd93c40b2a0b38509 Mon Sep 17 00:00:00 2001
From: todd &lt;todd@ubuntu64-build01.(none)&gt;
Date: Sun, 27 Jun 2010 22:21:29 -0700
Subject: [PATCH 08/30] CLOUDERA-BUILD. hbase-config.sh should set HBASE_PID_DIR if unset

---
 bin/hbase-config.sh |    2 ++
 1 files changed, 2 insertions(+), 0 deletions(-)

diff --git a/bin/hbase-config.sh b/bin/hbase-config.sh
index 5d13859..2415b53 100644
--- a/bin/hbase-config.sh
+++ b/bin/hbase-config.sh
@@ -71,6 +71,8 @@ done
  
 # Allow alternate hbase conf dir location.
 HBASE_CONF_DIR=&quot;${HBASE_CONF_DIR:-$HBASE_HOME/conf}&quot;
+
+HBASE_PID_DIR=&quot;${HBASE_PID_DIR:-/tmp}&quot;
 # List of hbase regions servers.
 HBASE_REGIONSERVERS=&quot;${HBASE_REGIONSERVERS:-$HBASE_CONF_DIR/regionservers}&quot;
 # List of hbase secondary masters.
-- 
1.7.0.4


 =======================================================================
 ==cloudera/patches/0018-CLOUDERA-BUILD.-Build-site-as-part-of-release-build.patch
 =======================================================================
From 4c89df680a254d41fff96a4387280a8be505a2ad Mon Sep 17 00:00:00 2001
From: Todd Lipcon &lt;todd@cloudera.com&gt;
Date: Thu, 30 Sep 2010 22:42:07 -0700
Subject: [PATCH 18/30] CLOUDERA-BUILD. Build site as part of release build

---
 cloudera/do-release-build |    2 +-
 1 files changed, 1 insertions(+), 1 deletions(-)

diff --git a/cloudera/do-release-build b/cloudera/do-release-build
index 0986c23..eacaf57 100755
--- a/cloudera/do-release-build
+++ b/cloudera/do-release-build
@@ -12,7 +12,7 @@ RELEASE_DIR=$BIN_DIR/..
 cd $RELEASE_DIR
 
 mvn -DskipTests clean
-mvn -DskipTests -Dhbase.version=${FULL_VERSION} install assembly:assembly 
+mvn -DskipTests -Dhbase.version=${FULL_VERSION} site install assembly:assembly 
 mkdir -p build
 for x in target/*.tar.gz ; do
   tar -C build -xzf $x
-- 
1.7.0.4


 =======================================================================
 ==cloudera/patches/0001-Updating-the-site-for-0.89.20100924.patch
 =======================================================================
From 692785c890809f1475229a398cc1dba501d6cab9 Mon Sep 17 00:00:00 2001
From: Jean-Daniel Cryans &lt;jdcryans@apache.org&gt;
Date: Tue, 5 Oct 2010 19:05:51 +0000
Subject: [PATCH 01/30] Updating the site for 0.89.20100924

git-svn-id: https://svn.apache.org/repos/asf/hbase/tags/0.89.20100924RC1@1004768 13f79535-47bb-0310-9956-ffa450edef68
---
 src/site/xdoc/index.xml |    4 ++++
 1 files changed, 4 insertions(+), 0 deletions(-)

diff --git a/src/site/xdoc/index.xml b/src/site/xdoc/index.xml
index 1f57ac3..df375c6 100644
--- a/src/site/xdoc/index.xml
+++ b/src/site/xdoc/index.xml
@@ -49,6 +49,10 @@ HBase includes:
 
         &lt;/section&gt;
         &lt;section name=&quot;News&quot;&gt;
+      &lt;p&gt;November 19th, &lt;a href=&quot;http://huguk.org/&quot;&gt;Hadoop HUG in London&lt;/a&gt; is all about HBase&lt;/p&gt;
+      &lt;p&gt;November 15-19th, &lt;a href=&quot;http://www.devoxx.com/display/Devoxx2K10/Home&quot;&gt;Devoxx&lt;/a&gt; features HBase Training and multiple HBase presentations&lt;/p&gt;
+      &lt;p&gt;October 12th, HBase-related presentations by core contributors and users at &lt;a href=&quot;http://www.cloudera.com/company/press-center/hadoop-world-nyc/&quot;&gt;Hadoop World 2010&lt;/a&gt;&lt;/p&gt;
+      &lt;p&gt;October 11th, &lt;a href=&quot;http://www.meetup.com/hbaseusergroup/calendar/14606174/&quot;&gt;HUG-NYC: HBase User Group NYC Edition&lt;/a&gt; (Night before Hadoop World)&lt;/p&gt;
       &lt;p&gt;June 30th, &lt;a href=&quot;http://www.meetup.com/hbaseusergroup/calendar/13562846/&quot;&gt;HBase Contributor Workshop&lt;/a&gt; (Day after Hadoop Summit)&lt;/p&gt;
       &lt;p&gt;May 10th, 2010: HBase graduates from Hadoop sub-project to Apache Top Level Project &lt;/p&gt;
       &lt;p&gt;&lt;a href=&quot;old_news.html&quot;&gt;...&lt;/a&gt;&lt;/p&gt;
-- 
1.7.0.4


 =======================================================================
 ==cloudera/patches/0022-CLOUDERA-BUILD.-Change-wrapper-scripts-to-not-be-dep.patch
 =======================================================================
From 62bec1e3c6a34bae54dbe05bf9c473ef401eb3fd Mon Sep 17 00:00:00 2001
From: Todd Lipcon &lt;todd@cloudera.com&gt;
Date: Thu, 7 Oct 2010 18:34:43 -0700
Subject: [PATCH 22/30] CLOUDERA-BUILD. Change wrapper scripts to not be dependent on setting a bunch of environment variables

Fixes issues where /usr/lib/hbase/bin/hbase would fail, not finding the hadoop jars.
---
 bin/hbase                 |   12 +++++++++++-
 bin/hbase-config.sh       |    5 +++++
 cloudera/install_hbase.sh |   31 +++++++++----------------------
 3 files changed, 25 insertions(+), 23 deletions(-)

diff --git a/bin/hbase b/bin/hbase
index ec92f7b..b36593b 100755
--- a/bin/hbase
+++ b/bin/hbase
@@ -42,7 +42,7 @@
 #
 #   MAVEN_HOME       Where mvn is installed.
 #
-bin=`dirname &quot;$0&quot;`
+bin=`dirname &quot;$BASH_SOURCE&quot;`
 bin=`cd &quot;$bin&quot;&gt;/dev/null; pwd`
 
 # This will set HBASE_HOME, etc.
@@ -175,6 +175,16 @@ if [ &quot;$HBASE_CLASSPATH&quot; != &quot;&quot; ]; then
   CLASSPATH=${CLASSPATH}:${HBASE_CLASSPATH}
 fi
 
+# And very last, add other project configuration dirs - this
+# allows someone to override the hdfs config by putting an hdfs-site
+# in hbase's conf dir, for example
+if [ -n &quot;$ZOOKEEPER_CONF_DIR&quot; ]; then
+  CLASSPATH=${CLASSPATH}:${ZOOKEEPER_CONF_DIR}
+fi
+if [ -n &quot;$HADOOP_CONF_DIR&quot; ]; then
+  CLASSPATH=${CLASSPATH}:${HADOOP_CONF_DIR}
+fi
+
 # default log directory &amp; file
 if [ &quot;$HBASE_LOG_DIR&quot; = &quot;&quot; ]; then
   HBASE_LOG_DIR=&quot;$HBASE_HOME/logs&quot;
diff --git a/bin/hbase-config.sh b/bin/hbase-config.sh
index 2415b53..32d495d 100644
--- a/bin/hbase-config.sh
+++ b/bin/hbase-config.sh
@@ -78,11 +78,16 @@ HBASE_REGIONSERVERS=&quot;${HBASE_REGIONSERVERS:-$HBASE_CONF_DIR/regionservers}&quot;
 # List of hbase secondary masters.
 HBASE_BACKUP_MASTERS=&quot;${HBASE_BACKUP_MASTERS:-$HBASE_CONF_DIR/backup-masters}&quot;
 

 =======================================================================
 ==cloudera/patches/0020-Fix-src-assembly-to-make-java-libs-644-and-not-inclu.patch
 =======================================================================
From 8acae9396f84c90c4b4513d4644fb30389c6847c Mon Sep 17 00:00:00 2001
From: Todd Lipcon &lt;todd@lipcon.org&gt;
Date: Thu, 30 Sep 2010 23:27:21 -0700
Subject: [PATCH 20/30] Fix src assembly to make java libs 644, and not include hbase lib twice

---
 src/assembly/bin.xml |    2 ++
 1 files changed, 2 insertions(+), 0 deletions(-)

diff --git a/src/assembly/bin.xml b/src/assembly/bin.xml
index 06e45a9..416c46a 100644
--- a/src/assembly/bin.xml
+++ b/src/assembly/bin.xml
@@ -52,6 +52,8 @@
       &lt;outputDirectory&gt;/lib&lt;/outputDirectory&gt;
       &lt;unpack&gt;false&lt;/unpack&gt;
       &lt;scope&gt;runtime&lt;/scope&gt;
+      &lt;fileMode&gt;644&lt;/fileMode&gt;
+      &lt;useProjectArtifact&gt;false&lt;/useProjectArtifact&gt;
     &lt;/dependencySet&gt;
   &lt;/dependencySets&gt;
 &lt;/assembly&gt;
-- 
1.7.0.4


 =======================================================================
 ==cloudera/patches/0030-CLOUDERA-BUILD.-Add-man-page.patch
 =======================================================================
From 141178fbd79e618aff9dc8c328872d929a97bb01 Mon Sep 17 00:00:00 2001
From: Eli Collins &lt;eli@cloudera.com&gt;
Date: Mon, 25 Oct 2010 21:01:42 -0700
Subject: [PATCH 30/30] CLOUDERA-BUILD. Add man page.

---
 cloudera/hbase.1          |   73 +++++++++++++++++++++++++++++++++++++++++++++
 cloudera/install_hbase.sh |    3 ++
 2 files changed, 76 insertions(+), 0 deletions(-)
 create mode 100644 cloudera/hbase.1

diff --git a/cloudera/hbase.1 b/cloudera/hbase.1
new file mode 100644
index 0000000..c5e4a00
--- /dev/null
+++ b/cloudera/hbase.1
@@ -0,0 +1,73 @@
+.\&quot; Process this file with
+.\&quot; groff -man -Tascii hbase.1
+.\&quot;
+.TH hbase 1 &quot;October 2010 &quot; Linux &quot;User Manuals&quot;
+
+.SH NAME
+HBase \- HBase is the Hadoop database.
+
+.SH SYNOPSIS
+
+.B hbase
+\fICOMMAND\fR
+
+.SH DESCRIPTION
+
+HBase is the Hadoop database. Use it when you need random, realtime
+read/write access to your Big Data. This project's goal is the hosting
+of very large tables -- billions of rows X millions of columns -- atop
+clusters of commodity hardware.
+
+HBase is an open-source, distributed, versioned, column-oriented store
+modeled after Google's Bigtable: A Distributed Storage System for
+Structured Data by Chang et al. Just as Bigtable leverages the
+distributed data storage provided by the Google File System, HBase
+provides Bigtable-like capabilities on top of Hadoop.
+
+For more information about HBase, see http://hbase.apache.org.
+
+\fICOMMAND\fR may be one of the following:
+  shell            run the HBase shell
+  shell-tests      run the HBase shell tests
+  zkcli            run the ZooKeeper shell
+  master           run an HBase HMaster node

 =======================================================================
 ==cloudera/patches/0027-HBASE-3101.-bin-assembly-should-include-tests-and-so.patch
 =======================================================================
From baa476fc14c80e18e91ccd683c6aa045a6689fd3 Mon Sep 17 00:00:00 2001
From: Todd Lipcon &lt;todd@cloudera.com&gt;
Date: Sat, 9 Oct 2010 17:36:53 -0700
Subject: [PATCH 27/30] HBASE-3101. bin assembly should include tests and sources jar

Author: Todd Lipcon
Reason: sqoop build depends on -tests jar
Ref: CDH-2203
---
 src/assembly/bin.xml |    3 ++-
 1 files changed, 2 insertions(+), 1 deletions(-)

diff --git a/src/assembly/bin.xml b/src/assembly/bin.xml
index 416c46a..4eeab88 100644
--- a/src/assembly/bin.xml
+++ b/src/assembly/bin.xml
@@ -28,7 +28,8 @@
       &lt;outputDirectory&gt;/&lt;/outputDirectory&gt;
       &lt;includes&gt;
           &lt;include&gt;hbase-${project.version}.jar&lt;/include&gt;
-          &lt;include&gt;hbase-${project.version}-test.jar&lt;/include&gt;
+          &lt;include&gt;hbase-${project.version}-tests.jar&lt;/include&gt;
+          &lt;include&gt;hbase-${project.version}-sources.jar&lt;/include&gt;
       &lt;/includes&gt;
     &lt;/fileSet&gt;
     &lt;fileSet&gt;
-- 
1.7.0.4


 =======================================================================
 ==cloudera/build.properties
 =======================================================================

# Autogenerated build properties
version=0.89.20100924+30
git.hash=141178fbd79e618aff9dc8c328872d929a97bb01
cloudera.hash=141178fbd79e618aff9dc8c328872d929a97bb01
cloudera.base-branch=cdh-base-0.89.20100924
cloudera.build-branch=cdh-0.89.20100924


 =======================================================================
 ==cloudera/CHANGES.cloudera.txt
 =======================================================================
commit 141178fbd79e618aff9dc8c328872d929a97bb01
Author: Eli Collins &lt;eli@cloudera.com&gt;
Date:   Mon Oct 25 21:01:42 2010 -0700

    CLOUDERA-BUILD. Add man page.

commit 5e9777a0c3ce2f2c7a5bb172a4967a655b91d91e
Author: Eli Collins &lt;eli@cloudera.com&gt;
Date:   Mon Oct 11 09:43:26 2010 -0700

    CLOUDERA-BUILD. Publish to Cloudera maven repo. Use Cloudera ZK.

commit 80fd13dbb5d66bdfc80e6c22f6b4567f62499c49
Author: Todd Lipcon &lt;todd@cloudera.com&gt;
Date:   Sat Oct 9 17:56:08 2010 -0700

    CLOUDERA-BUILD. Fix versionless jar naming symlinks for multijar case
    
    Ref: CDH-2203

commit baa476fc14c80e18e91ccd683c6aa045a6689fd3
Author: Todd Lipcon &lt;todd@cloudera.com&gt;
Date:   Sat Oct 9 17:36:53 2010 -0700

    HBASE-3101. bin assembly should include tests and sources jar
    
    Author: Todd Lipcon
    Reason: sqoop build depends on -tests jar
    Ref: CDH-2203

commit 1faa3e29bfc2935a71197fe28cec4dbecf2b7692
Author: Eli Collins &lt;eli@cloudera.com&gt;
Date:   Fri Oct 8 20:18:14 2010 -0700

    CLOUDERA-BUILD. Update hadoop version.

commit df24f2aa192153241a12711a95e861806db315f3
Author: Todd Lipcon &lt;todd@cloudera.com&gt;
Date:   Fri Oct 8 15:29:23 2010 -0700

    HBASE-3096. TestCompaction timing out
    
    Author: Todd Lipcon

commit c5361373f24d793ed11cf5935649b2d0bc920658
Author: Todd Lipcon &lt;todd@cloudera.com&gt;
Date:   Thu Oct 7 22:05:20 2010 -0700

    HBASE-2799. &quot;Append not enabled&quot; warning should not show if hbase root dir isn't on DFS
    

 =======================================================================
 ==cloudera/README.cloudera
 =======================================================================
This build was generated by Cloudera's build system in the following manner:

1) The pristine open-source release tarball was unpacked

2) The patches contained within the patches/ directory next to this README
were applied using the apply-patches script. A complete log of these changes
is also included in CHANGES.cloudera.txt.

3) The project was built by running the do-release-build script in this directory.

 =======================================================================
 ==cloudera/install_hbase.sh
 =======================================================================
#!/bin/sh
# Copyright 2009 Cloudera, inc.
set -ex

usage() {
  echo &quot;
usage: $0 &lt;options&gt;
  Required not-so-options:
     --cloudera-source-dir=DIR   path to cloudera distribution files
     --build-dir=DIR             path to hbase dist.dir
     --prefix=PREFIX             path to install into

  Optional options:
     --doc-dir=DIR               path to install docs into [/usr/share/doc/hbase]
     --lib-dir=DIR               path to install hbase home [/usr/lib/hbase]
     --installed-lib-dir=DIR     path where lib-dir will end up on target system
     --bin-dir=DIR               path to install bins [/usr/bin]
     --examples-dir=DIR          path to install examples [doc-dir/examples]
     ... [ see source for more similar options ]
  &quot;
  exit 1
}

OPTS=$(getopt \
  -n $0 \
  -o '' \
  -l 'cloudera-source-dir:' \
  -l 'prefix:' \
  -l 'doc-dir:' \
  -l 'lib-dir:' \
  -l 'installed-lib-dir:' \
  -l 'bin-dir:' \
  -l 'examples-dir:' \
  -l 'build-dir:' -- &quot;$@&quot;)

if [ $? != 0 ] ; then
    usage
fi

eval set -- &quot;$OPTS&quot;
while true ; do
    case &quot;$1&quot; in
        --cloudera-source-dir)
        CLOUDERA_SOURCE_DIR=$2 ; shift 2
        ;;
        --prefix)
        PREFIX=$2 ; shift 2
        ;;
        --build-dir)
        BUILD_DIR=$2 ; shift 2

 =======================================================================
 ==CHANGES.txt
 =======================================================================
HBase Change Log
Release 0.89.20100924 - Fri Sep 24 13:51:36 PDT 2010
  INCOMPATIBLE CHANGES
   HBASE-1822  Remove the deprecated APIs
   HBASE-1848  Fixup shell for HBASE-1822
   HBASE-1854  Remove the Region Historian
   HBASE-1930  Put.setTimeStamp misleading (doesn't change timestamp on
               existing KeyValues, not copied in copy constructor)
               (Dave Latham via Stack)
   HBASE-1360  move up to Thrift 0.2.0 (Kay Kay and Lars Francke via Stack)
   HBASE-2212  Refactor out lucene dependencies from HBase
               (Kay Kay via Stack)
   HBASE-2219  stop using code mapping for method names in the RPC
   HBASE-1728  Column family scoping and cluster identification
   HBASE-2099  Move build to Maven (Paul Smith via Stack)
   HBASE-2260  Remove all traces of Ant and Ivy (Lars Francke via Stack)
   HBASE-2255  take trunk back to hadoop 0.20
   HBASE-2378  Bulk insert with multiple reducers broken due to improper
               ImmutableBytesWritable comparator (Todd Lipcon via Stack)
   HBASE-2392  Upgrade to ZooKeeper 3.3.0
   HBASE-2294  Enumerate ACID properties of HBase in a well defined spec
               (Todd Lipcon via Stack)
   HBASE-2541  Remove transactional contrib (Clint Morgan via Stack)
   HBASE-2542  Fold stargate contrib into core
   HBASE-2565  Remove contrib module from hbase
   HBASE-2397  Bytes.toStringBinary escapes printable chars
   HBASE-2771  Update our hadoop jar to be latest from 0.20-append branch
   HBASE-2803  Remove remaining Get code from Store.java,etc
   HBASE-2553  Revisit IncrementColumnValue implementation in 0.22

  BUG FIXES
   HBASE-1791  Timeout in IndexRecordWriter (Bradford Stephens via Andrew
               Purtell)
   HBASE-1737  Regions unbalanced when adding new node (recommit)
   HBASE-1792  [Regression] Cannot save timestamp in the future
   HBASE-1793  [Regression] HTable.get/getRow with a ts is broken
   HBASE-1698  Review documentation for o.a.h.h.mapreduce
   HBASE-1798  [Regression] Unable to delete a row in the future
   HBASE-1790  filters are not working correctly (HBASE-1710 HBASE-1807 too)
   HBASE-1779  ThriftServer logged error if getVer() result is empty
   HBASE-1778  Improve PerformanceEvaluation (Schubert Zhang via Stack)
   HBASE-1751  Fix KeyValue javadoc on getValue for client-side
   HBASE-1795  log recovery doesnt reset the max sequence id, new logfiles can
               get tossed as 'duplicates'
   HBASE-1794  recovered log files are not inserted into the storefile map
   HBASE-1824  [stargate] default timestamp should be LATEST_TIMESTAMP
   HBASE-1740  ICV has a subtle race condition only visible under high load
   HBASE-1808  [stargate] fix how columns are specified for scanners
   HBASE-1828  CompareFilters are broken from client-side
   HBASE-1836  test of indexed hbase broken

 =======================================================================
 ==conf/regionservers
 =======================================================================
localhost

 =======================================================================
 ==conf/log4j.properties
 =======================================================================
# Define some default values that can be overridden by system properties
hbase.root.logger=INFO,console
hbase.log.dir=.
hbase.log.file=hbase.log

# Define the root logger to the system property &quot;hbase.root.logger&quot;.
log4j.rootLogger=${hbase.root.logger}

# Logging Threshold
log4j.threshhold=ALL

#
# Daily Rolling File Appender
#
log4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender
log4j.appender.DRFA.File=${hbase.log.dir}/${hbase.log.file}

# Rollver at midnight
log4j.appender.DRFA.DatePattern=.yyyy-MM-dd

# 30-day backup
#log4j.appender.DRFA.MaxBackupIndex=30
log4j.appender.DRFA.layout=org.apache.log4j.PatternLayout

# Pattern format: Date LogLevel LoggerName LogMessage
log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n

# Debugging Pattern format
#log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n


#
# console
# Add &quot;console&quot; to rootlogger above if you want to use this 
#
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n

# Custom Logging levels

log4j.logger.org.apache.zookeeper=INFO
#log4j.logger.org.apache.hadoop.fs.FSNamesystem=DEBUG
log4j.logger.org.apache.hadoop.hbase=DEBUG
#log4j.logger.org.apache.hadoop.dfs=DEBUG

 =======================================================================
 ==conf/hadoop-metrics.properties
 =======================================================================
# See http://wiki.apache.org/hadoop/GangliaMetrics
# Make sure you know whether you are using ganglia 3.0 or 3.1.
# If 3.1, you will have to patch your hadoop instance with HADOOP-4675
# And, yes, this file is named hadoop-metrics.properties rather than
# hbase-metrics.properties because we're leveraging the hadoop metrics
# package and hadoop-metrics.properties is an hardcoded-name, at least
# for the moment.
#
# See also http://hadoop.apache.org/hbase/docs/current/metrics.html

# Configuration of the &quot;hbase&quot; context for null
hbase.class=org.apache.hadoop.metrics.spi.NullContext

# Configuration of the &quot;hbase&quot; context for file
# hbase.class=org.apache.hadoop.hbase.metrics.file.TimeStampingFileContext
# hbase.period=10
# hbase.fileName=/tmp/metrics_hbase.log

# Configuration of the &quot;hbase&quot; context for ganglia
# Pick one: Ganglia 3.0 (former) or Ganglia 3.1 (latter)
# hbase.class=org.apache.hadoop.metrics.ganglia.GangliaContext
# hbase.class=org.apache.hadoop.metrics.ganglia.GangliaContext31
# hbase.period=10
# hbase.servers=GMETADHOST_IP:8649

# Configuration of the &quot;jvm&quot; context for null
jvm.class=org.apache.hadoop.metrics.spi.NullContext

# Configuration of the &quot;jvm&quot; context for file
# jvm.class=org.apache.hadoop.hbase.metrics.file.TimeStampingFileContext
# jvm.period=10
# jvm.fileName=/tmp/metrics_jvm.log

# Configuration of the &quot;jvm&quot; context for ganglia
# Pick one: Ganglia 3.0 (former) or Ganglia 3.1 (latter)
# jvm.class=org.apache.hadoop.metrics.ganglia.GangliaContext
# jvm.class=org.apache.hadoop.metrics.ganglia.GangliaContext31
# jvm.period=10
# jvm.servers=GMETADHOST_IP:8649

# Configuration of the &quot;rpc&quot; context for null
rpc.class=org.apache.hadoop.metrics.spi.NullContext

# Configuration of the &quot;rpc&quot; context for file
# rpc.class=org.apache.hadoop.hbase.metrics.file.TimeStampingFileContext
# rpc.period=10
# rpc.fileName=/tmp/metrics_rpc.log

# Configuration of the &quot;rpc&quot; context for ganglia
# Pick one: Ganglia 3.0 (former) or Ganglia 3.1 (latter)
</pre>
</div>
</p>
</div>

      </div>
    </div>
    <div class="clear">
      <hr/>
    </div>
    <div id="footer">
      <div class="xright">Copyright &#169;                    2011
                        <a href="http://www.apache.org">Apache Software Foundation</a>.
            All Rights Reserved.      
                
      </div>
      <div class="clear">
        <hr/>
      </div>
    </div>
  </body>
</html>
